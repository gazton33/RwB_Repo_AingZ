

## Imported snippet – 2025-07-03 14:39:25

Image generation
================
Learn how to generate or edit images.
Overview
--------
The OpenAI API lets you generate and edit images from text prompts, using the GPT Image or DALL·E models. You can access image generation capabilities through two APIs:

### Image API
The [Image API](/docs/api-reference/images) provides three endpoints, each with distinct capabilities:
\* \*\*Generations\*\*: [Generate images](#generate-images) from scratch based on a text prompt
\* \*\*Edits\*\*: [Modify existing images](#edit-images) using a new prompt, either partially or entirely
\* \*\*Variations\*\*: [Generate variations](#image-variations) of an existing image (available with DALL·E 2 only)
This API supports `gpt-image-1` as well as `dall-e-2` and `dall-e-3`.

### Responses API
The [Responses API](/docs/api-reference/responses/create#responses-create-tools) allows you to generate images as part of conversations or multi-step flows. It supports image generation as a [built-in tool](/docs/guides/tools?api-mode=responses), and accepts image inputs and outputs within context.
Compared to the Image API, it adds:
\* \*\*Multi-turn editing\*\*: Iteratively make high fidelity edits to images with prompting
\* \*\*Streaming\*\*: Display partial images as the final output is being generated to improve perceived latency
\* \*\*Flexible inputs\*\*: Accept image [File](/docs/api-reference/files) IDs as input images, not just bytes
The image generation tool in responses only supports `gpt-image-1`. For a list of mainline models that support calling this tool, refer to the [supported models](#supported-models) below.

### Choosing the right API
\* If you only need to generate or edit a single image from one prompt, the Image API is your best choice.
\* If you want to build conversational, editable image experiences with GPT Image or display partial images during generation, go with the Responses API.
Both APIs let you [customize output](#customize-image-output) — adjust quality, size, format, compression, and enable transparent backgrounds.

### Model comparison
Our latest and most advanced model for image generation is `gpt-image-1`, a natively multimodal language model.
We recommend this model for its high-quality image generation and ability to use world knowledge in image creation. However, you can also use specialized image generation models—DALL·E 2 and DALL·E 3—with the Image API.
|Model|Endpoints|Use case|
|---|---|---|
|DALL·E 2|Image API: Generations, Edits, Variations|Lower cost, concurrent requests, inpainting (image editing with a mask)|
|DALL·E 3|Image API: Generations only|Higher image quality than DALL·E 2, support for larger resolutions|
|GPT Image|Image API: Generations, Edits – Responses API support coming soon|Superior instruction following, text rendering, detailed editing, real-world knowledge|
This guide focuses on GPT Image, but you can also switch to the docs for [DALL·E 2](/docs/guides/image-generation?image-generation-model=dall-e-2) and [DALL·E 3](/docs/guides/image-generation?image-generation-model=dall-e-3).
To ensure this model is used responsibly, you may need to complete the [API Organization Verification](https://help.openai.com/en/articles/10910291-api-organization-verification) from your [developer console](https://platform.openai.com/settings/organization/general) before using `gpt-image-1`.
![a vet with a baby otter](https://cdn.openai.com/API/docs/images/otter.png)
Generate Images
---------------
You can use the [image generation endpoint](/docs/api-reference/images/create) to create images based on text prompts, or the [image generation tool](/docs/guides/tools?api-mode=responses) in the Responses API to generate images as part of a conversation.
To learn more about customizing the output (size, quality, format, transparency), refer to the [customize image output](#customize-image-output) section below.
You can set the `n` parameter to generate multiple images at once in a single request (by default, the API returns a single image).
Responses API
Generate an image
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input: "Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools: [{type: "image\_generation"}],
});
// Save the image to a file
const imageData = response.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("otter.png", Buffer.from(imageBase64, "base64"));
}
```
```python
from openai import OpenAI
import base64
client = OpenAI()
response = client.responses.create(
model="gpt-4.1-mini",
input="Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools=[{"type": "image\_generation"}],
)

# Save the image to a file
image\_data = [
output.result
for output in response.output
if output.type == "image\_generation\_call"
]
if image\_data:
image\_base64 = image\_data[0]
with open("otter.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```
Image API
Generate an image
```javascript
import OpenAI from "openai";
import fs from "fs";
const openai = new OpenAI();
const prompt = `
A children's book drawing of a veterinarian using a stethoscope to
listen to the heartbeat of a baby otter.
`;
const result = await openai.images.generate({
model: "gpt-image-1",
prompt,
});
// Save the image to a file
const image\_base64 = result.data[0].b64\_json;
const image\_bytes = Buffer.from(image\_base64, "base64");
fs.writeFileSync("otter.png", image\_bytes);
```
```python
from openai import OpenAI
import base64
client = OpenAI()
prompt = """
A children's book drawing of a veterinarian using a stethoscope to
listen to the heartbeat of a baby otter.
"""
result = client.images.generate(
model="gpt-image-1",
prompt=prompt
)
image\_base64 = result.data[0].b64\_json
image\_bytes = base64.b64decode(image\_base64)

# Save the image to a file
with open("otter.png", "wb") as f:
f.write(image\_bytes)
```
```bash
curl -X POST "https://api.openai.com/v1/images/generations" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-type: application/json" \
-d '{
"model": "gpt-image-1",
"prompt": "A childrens book drawing of a veterinarian using a stethoscope to listen to the heartbeat of a baby otter."
}' | jq -r '.data[0].b64\_json' | base64 --decode > otter.png
```

### Multi-turn image generation
With the Responses API, you can build multi-turn conversations involving image generation either by providing image generation calls outputs within context (you can also just use the image ID), or by using the [`previous\_response\_id` parameter](/docs/guides/conversation-state?api-mode=responses#openai-apis-for-conversation-state). This makes it easy to iterate on images across multiple turns—refining prompts, applying new instructions, and evolving the visual output as the conversation progresses.
Using previous response ID
Multi-turn image generation
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input:
"Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools: [{ type: "image\_generation" }],
});
const imageData = response.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("cat\_and\_otter.png", Buffer.from(imageBase64, "base64"));
}
// Follow up
const response\_fwup = await openai.responses.create({
model: "gpt-4.1-mini",
previous\_response\_id: response.id,
input: "Now make it look realistic",
tools: [{ type: "image\_generation" }],
});
const imageData\_fwup = response\_fwup.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData\_fwup.length > 0) {
const imageBase64 = imageData\_fwup[0];
const fs = await import("fs");
fs.writeFileSync(
"cat\_and\_otter\_realistic.png",
Buffer.from(imageBase64, "base64")
);
}
```
```python
from openai import OpenAI
import base64
client = OpenAI()
response = client.responses.create(
model="gpt-4.1-mini",
input="Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools=[{"type": "image\_generation"}],
)
image\_data = [
output.result
for output in response.output
if output.type == "image\_generation\_call"
]
if image\_data:
image\_base64 = image\_data[0]
with open("cat\_and\_otter.png", "wb") as f:
f.write(base64.b64decode(image\_base64))

# Follow up
response\_fwup = client.responses.create(
model="gpt-4.1-mini",
previous\_response\_id=response.id,
input="Now make it look realistic",
tools=[{"type": "image\_generation"}],
)
image\_data\_fwup = [
output.result
for output in response\_fwup.output
if output.type == "image\_generation\_call"
]
if image\_data\_fwup:
image\_base64 = image\_data\_fwup[0]
with open("cat\_and\_otter\_realistic.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```
Using image ID
Multi-turn image generation
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input:
"Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools: [{ type: "image\_generation" }],
});
const imageGenerationCalls = response.output.filter(
(output) => output.type === "image\_generation\_call"
);
const imageData = imageGenerationCalls.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("cat\_and\_otter.png", Buffer.from(imageBase64, "base64"));
}
// Follow up
const response\_fwup = await openai.responses.create({
model: "gpt-4.1-mini",
input: [
{
role: "user",
content: [{ type: "input\_text", text: "Now make it look realistic" }],
},
{
type: "image\_generation\_call",
id: imageGenerationCalls[0].id,
},
],
tools: [{ type: "image\_generation" }],
});
const imageData\_fwup = response\_fwup.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData\_fwup.length > 0) {
const imageBase64 = imageData\_fwup[0];
const fs = await import("fs");
fs.writeFileSync(
"cat\_and\_otter\_realistic.png",
Buffer.from(imageBase64, "base64")
);
}
```
```python
import openai
import base64
response = openai.responses.create(
model="gpt-4.1-mini",
input="Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools=[{"type": "image\_generation"}],
)
image\_generation\_calls = [
output
for output in response.output
if output.type == "image\_generation\_call"
]
image\_data = [output.result for output in image\_generation\_calls]
if image\_data:
image\_base64 = image\_data[0]
with open("cat\_and\_otter.png", "wb") as f:
f.write(base64.b64decode(image\_base64))

# Follow up
response\_fwup = openai.responses.create(
model="gpt-4.1-mini",
input=[
{
"role": "user",
"content": [{"type": "input\_text", "text": "Now make it look realistic"}],
},
{
"type": "image\_generation\_call",
"id": image\_generation\_calls[0].id,
},
],
tools=[{"type": "image\_generation"}],
)
image\_data\_fwup = [
output.result
for output in response\_fwup.output
if output.type == "image\_generation\_call"
]
if image\_data\_fwup:
image\_base64 = image\_data\_fwup[0]
with open("cat\_and\_otter\_realistic.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```

#### Result
|"Generate an image of gray tabby cat hugging an otter with an orange scarf"||
|"Now make it look realistic"||

### Streaming
The Responses API also supports streaming image generation. This allows you to stream partial images as they are generated, providing a more interactive experience.
You can adjust the `partial\_images` parameter to receive 1-3 partial images.
Stream an image
```javascript
import OpenAI from "openai";
import fs from "fs";
const openai = new OpenAI();
const stream = await openai.responses.create({
model: "gpt-4.1",
input:
"Draw a gorgeous image of a river made of white owl feathers, snaking its way through a serene winter landscape",
stream: true,
tools: [{ type: "image\_generation", partial\_images: 2 }],
});
for await (const event of stream) {
if (event.type === "response.image\_generation\_call.partial\_image") {
const idx = event.partial\_image\_index;
const imageBase64 = event.partial\_image\_b64;
const imageBuffer = Buffer.from(imageBase64, "base64");
fs.writeFileSync(`river${idx}.png`, imageBuffer);
}
}
```
```python
from openai import OpenAI
import base64
client = OpenAI()
stream = client.responses.create(
model="gpt-4.1",
input="Draw a gorgeous image of a river made of white owl feathers, snaking its way through a serene winter landscape",
stream=True,
tools=[{"type": "image\_generation", "partial\_images": 2}],
)
for event in stream:
if event.type == "response.image\_generation\_call.partial\_image":
idx = event.partial\_image\_index
image\_base64 = event.partial\_image\_b64
image\_bytes = base64.b64decode(image\_base64)
with open(f"river{idx}.png", "wb") as f:
f.write(image\_bytes)
```

#### Result
|Partial 1|Partial 2|Final image|
|---|---|---|
||||
Prompt: Draw a gorgeous image of a river made of white owl feathers, snaking its way through a serene winter landscape

### Revised prompt
When using the image generation tool in the Responses API, the mainline model (e.g. `gpt-4.1`) will automatically revise your prompt for improved performance.
You can access the revised prompt in the `revised\_prompt` field of the image generation call:
```json
{
"id": "ig\_123",
"type": "image\_generation\_call",
"status": "completed",
"revised\_prompt": "A gray tabby cat hugging an otter. The otter is wearing an orange scarf. Both animals are cute and friendly, depicted in a warm, heartwarming style.",
"result": "..."
}
```
Edit Images
-----------
The [image edits](/docs/api-reference/images/createEdit) endpoint lets you:
\* Edit existing images
\* Generate new images using other images as a reference
\* Edit parts of an image by uploading an image and mask indicating which areas should be replaced (a process known as \*\*inpainting\*\*)

### Create a new image using image references
You can use one or more images as a reference to generate a new image.
In this example, we'll use 4 input images to generate a new image of a gift basket containing the items in the reference images.
[![Body Lotion](https://cdn.openai.com/API/docs/images/body-lotion.png)](https://cdn.openai.com/API/docs/images/body-lotion.png)[![Soap](https://cdn.openai.com/API/docs/images/soap.png)](https://cdn.openai.com/API/docs/images/soap.png)[![Incense Kit](https://cdn.openai.com/API/docs/images/incense-kit.png)](https://cdn.openai.com/API/docs/images/incense-kit.png)[![Bath Bomb](https://cdn.openai.com/API/docs/images/bath-bomb.png)](https://cdn.openai.com/API/docs/images/bath-bomb.png)
![Bath Gift Set](https://cdn.openai.com/API/docs/images/bath-set-result.png)
Responses API
With the Responses API, you can provide input images in 2 different ways:
\* By providing an image as a Base64-encoded data URL
\* By providing a file ID (created with the [Files API](/docs/api-reference/files))
We're actively working on supporting fully qualified URLs to image files as input as well.
Create a File
Edit an image
```python
from openai import OpenAI
client = OpenAI()
def create\_file(file\_path):
with open(file\_path, "rb") as file\_content:
result = client.files.create(
file=file\_content,
purpose="vision",
)
return result.id
```
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
async function createFile(filePath) {
const fileContent = fs.createReadStream(filePath);
const result = await openai.files.create({
file: fileContent,
purpose: "vision",
});
return result.id;
}
```
Create a base64 encoded image
Edit an image
```python
def encode\_image(file\_path):
with open(file\_path, "rb") as f:
base64\_image = base64.b64encode(f.read()).decode("utf-8")
return base64\_image
```
```javascript
function encodeImage(filePath) {
const base64Image = fs.readFileSync(filePath, "base64");
return base64Image;
}
```
Edit an image
```python
from openai import OpenAI
import base64
client = OpenAI()
prompt = """Generate a photorealistic image of a gift basket on a white background
labeled 'Relax & Unwind' with a ribbon and handwriting-like font,
containing all the items in the reference pictures."""
base64\_image1 = encode\_image("body-lotion.png")
base64\_image2 = encode\_image("soap.png")
file\_id1 = create\_file("body-lotion.png")
file\_id2 = create\_file("incense-kit.png")
response = client.responses.create(
model="gpt-4.1",
input=[
{
"role": "user",
"content": [
{"type": "input\_text", "text": prompt},
{
"type": "input\_image",
"image\_url": f"data:image/jpeg;base64,{base64\_image1}",
},
{
"type": "input\_image",
"image\_url": f"data:image/jpeg;base64,{base64\_image2}",
},
{
"type": "input\_image",
"file\_id": file\_id1,
},
{
"type": "input\_image",
"file\_id": file\_id2,
}
],
}
],
tools=[{"type": "image\_generation"}],
)
image\_generation\_calls = [
output
for output in response.output
if output.type == "image\_generation\_call"
]
image\_data = [output.result for output in image\_generation\_calls]
if image\_data:
image\_base64 = image\_data[0]
with open("gift-basket.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
else:
print(response.output.content)
```
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const prompt = `Generate a photorealistic image of a gift basket on a white background
labeled 'Relax & Unwind' with a ribbon and handwriting-like font,
containing all the items in the reference pictures.`;
const base64Image1 = encodeImage("body-lotion.png");
const base64Image2 = encodeImage("soap.png");
const fileId1 = await createFile("body-lotion.png");
const fileId2 = await createFile("incense-kit.png");
const response = await openai.responses.create({
model: "gpt-4.1",
input: [
{
role: "user",
content: [
{ type: "input\_text", text: prompt },
{
type: "input\_image",
image\_url: `data:image/jpeg;base64,${base64Image1}`,
},
{
type: "input\_image",
image\_url: `data:image/jpeg;base64,${base64Image2}`,
},
{
type: "input\_image",
file\_id: fileId1,
},
{
type: "input\_image",
file\_id: fileId2,
},
],
},
],
tools: [{ type: "image\_generation" }],
});
const imageData = response.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("gift-basket.png", Buffer.from(imageBase64, "base64"));
} else {
console.log(response.output.content);
}
```
Image API
Edit an image
```python
import base64
from openai import OpenAI
client = OpenAI()
prompt = """
Generate a photorealistic image of a gift basket on a white background
labeled 'Relax & Unwind' with a ribbon and handwriting-like font,
containing all the items in the reference pictures.
"""
result = client.images.edit(
model="gpt-image-1",
image=[
open("body-lotion.png", "rb"),
open("bath-bomb.png", "rb"),
open("incense-kit.png", "rb"),
open("soap.png", "rb"),
],
prompt=prompt
)
image\_base64 = result.data[0].b64\_json
image\_bytes = base64.b64decode(image\_base64)

# Save the image to a file
with open("gift-basket.png", "wb") as f:
f.write(image\_bytes)
```
```javascript
import fs from "fs";
import OpenAI, { toFile } from "openai";
const client = new OpenAI();
const prompt = `
Generate a photorealistic image of a gift basket on a white background
labeled 'Relax & Unwind' with a ribbon and handwriting-like font,
containing all the items in the reference pictures.
`;
const imageFiles = [
"bath-bomb.png",
"body-lotion.png",
"incense-kit.png",
"soap.png",
];
const images = await Promise.all(
imageFiles.map(async (file) =>
await toFile(fs.createReadStream(file), null, {
type: "image/png",
})
),
);
const response = await client.images.edit({
model: "gpt-image-1",
image: images,
prompt,
});
// Save the image to a file
const image\_base64 = response.data[0].b64\_json;
const image\_bytes = Buffer.from(image\_base64, "base64");
fs.writeFileSync("basket.png", image\_bytes);
```
```bash
curl -s -D >(grep -i x-request-id >&2) \
-o >(jq -r '.data[0].b64\_json' | base64 --decode > gift-basket.png) \
-X POST "https://api.openai.com/v1/images/edits" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-F "model=gpt-image-1" \
-F "image[]=@body-lotion.png" \
-F "image[]=@bath-bomb.png" \
-F "image[]=@incense-kit.png" \
-F "image[]=@soap.png" \
-F 'prompt=Generate a photorealistic image of a gift basket on a white background labeled "Relax & Unwind" with a ribbon and handwriting-like font, containing all the items in the reference pictures'
```

### Edit an image using a mask (inpainting)
You can provide a mask to indicate which part of the image should be edited.
When using a mask with GPT Image, additional instructions are sent to the model to help guide the editing process accordingly.
Unlike with DALL·E 2, masking with GPT Image is entirely prompt-based. This means the model uses the mask as guidance, but may not follow its exact shape with complete precision.
If you provide multiple input images, the mask will be applied to the first image.
Responses API
Edit an image with a mask
```python
from openai import OpenAI
client = OpenAI()
fileId = create\_file("sunlit\_lounge.png")
maskId = create\_file("mask.png")
response = client.responses.create(
model="gpt-4o",
input=[
{
"role": "user",
"content": [
{
"type": "input\_text",
"text": "generate an image of the same sunlit indoor lounge area with a pool but the pool should contain a flamingo",
},
{
"type": "input\_image",
"file\_id": fileId,
}
],
},
],
tools=[
{
"type": "image\_generation",
"quality": "high",
"input\_image\_mask": {
"file\_id": maskId,
},
},
],
)
image\_data = [
output.result
for output in response.output
if output.type == "image\_generation\_call"
]
if image\_data:
image\_base64 = image\_data[0]
with open("lounge.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const fileId = await createFile("sunlit\_lounge.png");
const maskId = await createFile("mask.png");
const response = await openai.responses.create({
model: "gpt-4o",
input: [
{
role: "user",
content: [
{
type: "input\_text",
text: "generate an image of the same sunlit indoor lounge area with a pool but the pool should contain a flamingo",
},
{
type: "input\_image",
file\_id: fileId,
}
],
},
],
tools: [
{
type: "image\_generation",
quality: "high",
input\_image\_mask: {
file\_id: maskId,
},
},
],
});
const imageData = response.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("lounge.png", Buffer.from(imageBase64, "base64"));
}
```
Image API
Edit an image with a mask
```python
from openai import OpenAI
client = OpenAI()
result = client.images.edit(
model="gpt-image-1",
image=open("sunlit\_lounge.png", "rb"),
mask=open("mask.png", "rb"),
prompt="A sunlit indoor lounge area with a pool containing a flamingo"
)
image\_base64 = result.data[0].b64\_json
image\_bytes = base64.b64decode(image\_base64)

# Save the image to a file
with open("composition.png", "wb") as f:
f.write(image\_bytes)
```
```javascript
import fs from "fs";
import OpenAI, { toFile } from "openai";
const client = new OpenAI();
const rsp = await client.images.edit({
model: "gpt-image-1",
image: await toFile(fs.createReadStream("sunlit\_lounge.png"), null, {
type: "image/png",
}),
mask: await toFile(fs.createReadStream("mask.png"), null, {
type: "image/png",
}),
prompt: "A sunlit indoor lounge area with a pool containing a flamingo",
});
// Save the image to a file
const image\_base64 = rsp.data[0].b64\_json;
const image\_bytes = Buffer.from(image\_base64, "base64");
fs.writeFileSync("lounge.png", image\_bytes);
```
```bash
curl -s -D >(grep -i x-request-id >&2) \
-o >(jq -r '.data[0].b64\_json' | base64 --decode > lounge.png) \
-X POST "https://api.openai.com/v1/images/edits" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-F "model=gpt-image-1" \
-F "mask=@mask.png" \
-F "image[]=@sunlit\_lounge.png" \
-F 'prompt=A sunlit indoor lounge area with a pool containing a flamingo'
```
|Image|Mask|Output|
|---|---|---|
||||
Prompt: a sunlit indoor lounge area with a pool containing a flamingo

#### Mask requirements
The image to edit and mask must be of the same format and size (less than 50MB in size).
The mask image must also contain an alpha channel. If you're using an image editing tool to create the mask, make sure to save the mask with an alpha channel.
Add an alpha channel to a black and white mask
You can modify a black and white image programmatically to add an alpha channel.
Add an alpha channel to a black and white mask
```python
from PIL import Image
from io import BytesIO

# 1. Load your black & white mask as a grayscale image
mask = Image.open(img\_path\_mask).convert("L")

# 2. Convert it to RGBA so it has space for an alpha channel
mask\_rgba = mask.convert("RGBA")

# 3. Then use the mask itself to fill that alpha channel
mask\_rgba.putalpha(mask)

# 4. Convert the mask into bytes
buf = BytesIO()
mask\_rgba.save(buf, format="PNG")
mask\_bytes = buf.getvalue()

# 5. Save the resulting file
img\_path\_mask\_alpha = "mask\_alpha.png"
with open(img\_path\_mask\_alpha, "wb") as f:
f.write(mask\_bytes)
```
Customize Image Output
----------------------
You can configure the following output options:
\* \*\*Size\*\*: Image dimensions (e.g., `1024x1024`, `1024x1536`)
\* \*\*Quality\*\*: Rendering quality (e.g. `low`, `medium`, `high`)
\* \*\*Format\*\*: File output format
\* \*\*Compression\*\*: Compression level (0-100%) for JPEG and WebP formats
\* \*\*Background\*\*: Transparent or opaque
`size`, `quality`, and `background` support the `auto` option, where the model will automatically select the best option based on the prompt.

### Size and quality options
Square images with standard quality are the fastest to generate. The default size is 1024x1024 pixels.
|Available sizes|1024x1024 (square)1536x1024 (landscape)1024x1536 (portrait)auto (default)|
|Quality options|lowmediumhighauto (default)|

### Output format
The Image API returns base64-encoded image data. The default format is `png`, but you can also request `jpeg` or `webp`.
If using `jpeg` or `webp`, you can also specify the `output\_compression` parameter to control the compression level (0-100%). For example, `output\_compression=50` will compress the image by 50%.
Using `jpeg` is faster than `png`, so you should prioritize this format if latency is a concern.

### Transparency
The `gpt-image-1` model supports transparent backgrounds. To enable transparency, set the `background` parameter to `transparent`.
It is only supported with the `png` and `webp` output formats.
Transparency works best when setting the quality to `medium` or `high`.
Responses API
Generate an image with a transparent background
```python
import openai
import base64
response = openai.responses.create(
model="gpt-4.1-mini",
input="Draw a 2D pixel art style sprite sheet of a tabby gray cat",
tools=[
{
"type": "image\_generation",
"background": "transparent",
"quality": "high",
}
],
)
image\_data = [
output.result
for output in response.output
if output.type == "image\_generation\_call"
]
if image\_data:
image\_base64 = image\_data[0]
with open("sprite.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```
```javascript
import fs from "fs";
import OpenAI from "openai";
const client = new OpenAI();
const response = await client.responses.create({
model: "gpt-4.1-mini",
input: "Draw a 2D pixel art style sprite sheet of a tabby gray cat",
tools: [
{
type: "image\_generation",
background: "transparent",
quality: "high",
},
],
});
const imageData = response.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const imageBuffer = Buffer.from(imageBase64, "base64");
fs.writeFileSync("sprite.png", imageBuffer);
}
```
Image API
Generate an image with a transparent background
```javascript
import OpenAI from "openai";
import fs from "fs";
const openai = new OpenAI();
const result = await openai.images.generate({
model: "gpt-image-1",
prompt: "Draw a 2D pixel art style sprite sheet of a tabby gray cat",
size: "1024x1024",
background: "transparent",
quality: "high",
});
// Save the image to a file
const image\_base64 = result.data[0].b64\_json;
const image\_bytes = Buffer.from(image\_base64, "base64");
fs.writeFileSync("sprite.png", image\_bytes);
```
```python
from openai import OpenAI
import base64
client = OpenAI()
result = client.images.generate(
model="gpt-image-1",
prompt="Draw a 2D pixel art style sprite sheet of a tabby gray cat",
size="1024x1024",
background="transparent",
quality="high",
)
image\_base64 = result.json()["data"][0]["b64\_json"]
image\_bytes = base64.b64decode(image\_base64)

# Save the image to a file
with open("sprite.png", "wb") as f:
f.write(image\_bytes)
```
```bash
curl -X POST "https://api.openai.com/v1/images" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-type: application/json" \
-d '{
"prompt": "Draw a 2D pixel art style sprite sheet of a tabby gray cat",
"quality": "high",
"size": "1024x1024",
"background": "transparent"
}' | jq -r 'data[0].b64\_json' | base64 --decode > sprite.png
```
Limitations
-----------
The GPT Image 1 model is a powerful and versatile image generation model, but it still has some limitations to be aware of:
\* \*\*Latency:\*\* Complex prompts may take up to 2 minutes to process.
\* \*\*Text Rendering:\*\* Although significantly improved over the DALL·E series, the model can still struggle with precise text placement and clarity.
\* \*\*Consistency:\*\* While capable of producing consistent imagery, the model may occasionally struggle to maintain visual consistency for recurring characters or brand elements across multiple generations.
\* \*\*Composition Control:\*\* Despite improved instruction following, the model may have difficulty placing elements precisely in structured or layout-sensitive compositions.

### Content Moderation
All prompts and generated images are filtered in accordance with our [content policy](https://labs.openai.com/policies/content-policy).
For image generation using `gpt-image-1`, you can control moderation strictness with the `moderation` parameter. This parameter supports two values:
\* `auto` (default): Standard filtering that seeks to limit creating certain categories of potentially age-inappropriate content.
\* `low`: Less restrictive filtering.

### Supported models
When using image generation in the Responses API, the models that support calling this tool are:
\* `gpt-4o`
\* `gpt-4o-mini`
\* `gpt-4.1`
\* `gpt-4.1-mini`
\* `gpt-4.1-nano`
\* `o3`
Cost and latency
----------------
This model generates images by first producing specialized image tokens. Both latency and eventual cost are proportional to the number of tokens required to render an image—larger image sizes and higher quality settings result in more tokens.
The number of tokens generated depends on image dimensions and quality:
|Quality|Square (1024×1024)|Portrait (1024×1536)|Landscape (1536×1024)|
|---|---|---|---|
|Low|272 tokens|408 tokens|400 tokens|
|Medium|1056 tokens|1584 tokens|1568 tokens|
|High|4160 tokens|6240 tokens|6208 tokens|
Note that you will also need to account for [input tokens](/docs/guides/images-vision#gpt-image-1): text tokens for the prompt and image tokens for the input images if editing images.
So the final cost is the sum of:
\* input text tokens
\* input image tokens if using the edits endpoint
\* image output tokens
Refer to our [pricing page](/pricing#image-generation) for more information about price per text and image tokens.

### Partial images cost
If you want to [stream image generation](#streaming) with the Responses API using the `partial\_images` parameter, each partial image will incur an additional 100 image output tokens.
Was this page useful?


## Imported snippet – 2025-07-03 14:39:30

Text to speech
==============
Learn how to turn text into lifelike spoken audio.
The Audio API provides a [`speech`](/docs/api-reference/audio/createSpeech) endpoint based on our [GPT-4o mini TTS (text-to-speech) model](/docs/models/gpt-4o-mini-tts). It comes with 11 built-in voices and can be used to:
\* Narrate a written blog post
\* Produce spoken audio in multiple languages
\* Give realtime audio output using streaming
Here's an example of the `alloy` voice:
Our [usage policies](https://openai.com/policies/usage-policies) require you to provide a clear disclosure to end users that the TTS voice they are hearing is AI-generated and not a human voice.
Quickstart
----------
The `speech` endpoint takes three key inputs:
1. The [model](/docs/api-reference/audio/createSpeech#audio-createspeech-model) you're using
2. The [text](/docs/api-reference/audio/createSpeech#audio-createspeech-input) to be turned into audio
3. The [voice](/docs/api-reference/audio/createSpeech#audio-createspeech-voice) that will speak the output
Here's a simple request example:
Generate spoken audio from input text
```javascript
import fs from "fs";
import path from "path";
import OpenAI from "openai";
const openai = new OpenAI();
const speechFile = path.resolve("./speech.mp3");
const mp3 = await openai.audio.speech.create({
model: "gpt-4o-mini-tts",
voice: "coral",
input: "Today is a wonderful day to build something people love!",
instructions: "Speak in a cheerful and positive tone.",
});
const buffer = Buffer.from(await mp3.arrayBuffer());
await fs.promises.writeFile(speechFile, buffer);
```
```python
from pathlib import Path
from openai import OpenAI
client = OpenAI()
speech\_file\_path = Path(\_\_file\_\_).parent / "speech.mp3"
with client.audio.speech.with\_streaming\_response.create(
model="gpt-4o-mini-tts",
voice="coral",
input="Today is a wonderful day to build something people love!",
instructions="Speak in a cheerful and positive tone.",
) as response:
response.stream\_to\_file(speech\_file\_path)
```
```bash
curl https://api.openai.com/v1/audio/speech \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4o-mini-tts",
"input": "Today is a wonderful day to build something people love!",
"voice": "coral",
"instructions": "Speak in a cheerful and positive tone."
}' \
--output speech.mp3
```
By default, the endpoint outputs an MP3 of the spoken audio, but you can configure it to output any [supported format](#supported-output-formats).

### Text-to-speech models
For intelligent realtime applications, use the `gpt-4o-mini-tts` model, our newest and most reliable text-to-speech model. You can prompt the model to control aspects of speech, including:
\* Accent
\* Emotional range
\* Intonation
\* Impressions
\* Speed of speech
\* Tone
\* Whispering
Our other text-to-speech models are `tts-1` and `tts-1-hd`. The `tts-1` model provides lower latency, but at a lower quality than the `tts-1-hd` model.

### Voice options
The TTS endpoint provides 11 built‑in voices to control how speech is rendered from text. \*\*Hear and play with these voices in [OpenAI.fm](https://openai.fm), our interactive demo for trying the latest text-to-speech model in the OpenAI API\*\*. Voices are currently optimized for English.
\* `alloy`
\* `ash`
\* `ballad`
\* `coral`
\* `echo`
\* `fable`
\* `nova`
\* `onyx`
\* `sage`
\* `shimmer`
If you're using the [Realtime API](/docs/guides/realtime), note that the set of available voices is slightly different—see the [realtime conversations guide](/docs/guides/realtime-conversations#voice-options) for current realtime voices.

### Streaming realtime audio
The Speech API provides support for realtime audio streaming using [chunk transfer encoding](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Transfer-Encoding). This means the audio can be played before the full file is generated and made accessible.
Stream spoken audio from input text directly to your speakers
```javascript
import OpenAI from "openai";
import { playAudio } from "openai/helpers/audio";
const openai = new OpenAI();
const response = await openai.audio.speech.create({
model: "gpt-4o-mini-tts",
voice: "coral",
input: "Today is a wonderful day to build something people love!",
instructions: "Speak in a cheerful and positive tone.",
response\_format: "wav",
});
await playAudio(response);
```
```python
import asyncio
from openai import AsyncOpenAI
from openai.helpers import LocalAudioPlayer
openai = AsyncOpenAI()
async def main() -> None:
async with openai.audio.speech.with\_streaming\_response.create(
model="gpt-4o-mini-tts",
voice="coral",
input="Today is a wonderful day to build something people love!",
instructions="Speak in a cheerful and positive tone.",
response\_format="pcm",
) as response:
await LocalAudioPlayer().play(response)
if \_\_name\_\_ == "\_\_main\_\_":
asyncio.run(main())
```
```bash
curl https://api.openai.com/v1/audio/speech \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4o-mini-tts",
"input": "Today is a wonderful day to build something people love!",
"voice": "coral",
"instructions": "Speak in a cheerful and positive tone.",
"response\_format": "wav"
}' | ffplay -i -
```
For the fastest response times, we recommend using `wav` or `pcm` as the response format.
Supported output formats
------------------------
The default response format is `mp3`, but other formats like `opus` and `wav` are available.
\* \*\*MP3\*\*: The default response format for general use cases.
\* \*\*Opus\*\*: For internet streaming and communication, low latency.
\* \*\*AAC\*\*: For digital audio compression, preferred by YouTube, Android, iOS.
\* \*\*FLAC\*\*: For lossless audio compression, favored by audio enthusiasts for archiving.
\* \*\*WAV\*\*: Uncompressed WAV audio, suitable for low-latency applications to avoid decoding overhead.
\* \*\*PCM\*\*: Similar to WAV but contains the raw samples in 24kHz (16-bit signed, low-endian), without the header.
Supported languages
-------------------
The TTS model generally follows the Whisper model in terms of language support. Whisper [supports the following languages](https://github.com/openai/whisper#available-models-and-languages) and performs well, despite voices being optimized for English:
Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.
You can generate spoken audio in these languages by providing input text in the language of your choice.
Customization and ownership
---------------------------

### Custom voices
We do not support custom voices or creating a copy of your own voice.

### Who owns the output?
As with all outputs from our API, the person who created them owns the output. You are still required to inform end users that they are hearing audio generated by AI and not a real person talking to them.
Was this page useful?


## Imported snippet – 2025-07-03 14:39:38

Speech to text
==============
Learn how to turn audio into text.
The Audio API provides two speech to text endpoints:
\* `transcriptions`
\* `translations`
Historically, both endpoints have been backed by our open source [Whisper model](https://openai.com/blog/whisper/) (`whisper-1`). The `transcriptions` endpoint now also supports higher quality model snapshots, with limited parameter support:
\* `gpt-4o-mini-transcribe`
\* `gpt-4o-transcribe`
All endpoints can be used to:
\* Transcribe audio into whatever language the audio is in.
\* Translate and transcribe the audio into English.
File uploads are currently limited to 25 MB, and the following input file types are supported: `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `wav`, and `webm`.
Quickstart
----------

### Transcriptions
The transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. All models support the same set of input formats. On output, `whisper-1` supports a range of formats (`json`, `text`, `srt`, `verbose\_json`, `vtt`); the newer `gpt-4o-mini-transcribe` and `gpt-4o-transcribe` snapshots currently only support `json` or plain `text` responses.
Transcribe audio
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const transcription = await openai.audio.transcriptions.create({
file: fs.createReadStream("/path/to/file/audio.mp3"),
model: "gpt-4o-transcribe",
});
console.log(transcription.text);
```
```python
from openai import OpenAI
client = OpenAI()
audio\_file= open("/path/to/file/audio.mp3", "rb")
transcription = client.audio.transcriptions.create(
model="gpt-4o-transcribe",
file=audio\_file
)
print(transcription.text)
```
```bash
curl --request POST \
--url https://api.openai.com/v1/audio/transcriptions \
--header "Authorization: Bearer $OPENAI\_API\_KEY" \
--header 'Content-Type: multipart/form-data' \
--form file=@/path/to/file/audio.mp3 \
--form model=gpt-4o-transcribe
```
By default, the response type will be json with the raw text included.
{ "text": "Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. .... }
The Audio API also allows you to set additional parameters in a request. For example, if you want to set the `response\_format` as `text`, your request would look like the following:
Additional options
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const transcription = await openai.audio.transcriptions.create({
file: fs.createReadStream("/path/to/file/speech.mp3"),
model: "gpt-4o-transcribe",
response\_format: "text",
});
console.log(transcription.text);
```
```python
from openai import OpenAI
client = OpenAI()
audio\_file = open("/path/to/file/speech.mp3", "rb")
transcription = client.audio.transcriptions.create(
model="gpt-4o-transcribe",
file=audio\_file,
response\_format="text"
)
print(transcription.text)
```
```bash
curl --request POST \
--url https://api.openai.com/v1/audio/transcriptions \
--header "Authorization: Bearer $OPENAI\_API\_KEY" \
--header 'Content-Type: multipart/form-data' \
--form file=@/path/to/file/speech.mp3 \
--form model=gpt-4o-transcribe \
--form response\_format=text
```
The [API Reference](/docs/api-reference/audio) includes the full list of available parameters.
The newer `gpt-4o-mini-transcribe` and `gpt-4o-transcribe` models currently have a limited parameter surface: they only support `json` or `text` response formats. Other parameters, such as `timestamp\_granularities`, require `verbose\_json` output and are therefore only available when using `whisper-1`.

### Translations
The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text. This endpoint supports only the `whisper-1` model.
Translate audio
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const translation = await openai.audio.translations.create({
file: fs.createReadStream("/path/to/file/german.mp3"),
model: "whisper-1",
});
console.log(translation.text);
```
```python
from openai import OpenAI
client = OpenAI()
audio\_file = open("/path/to/file/german.mp3", "rb")
translation = client.audio.translations.create(
model="whisper-1",
file=audio\_file,
)
print(translation.text)
```
```bash
curl --request POST \
--url https://api.openai.com/v1/audio/translations \
--header "Authorization: Bearer $OPENAI\_API\_KEY" \
--header 'Content-Type: multipart/form-data' \
--form file=@/path/to/file/german.mp3 \
--form model=whisper-1 \
```
In this case, the inputted audio was german and the outputted text looks like:
Hello, my name is Wolfgang and I come from Germany. Where are you heading today?
We only support translation into English at this time.
Supported languages
-------------------
We currently [support the following languages](https://github.com/openai/whisper#available-models-and-languages) through both the `transcriptions` and `translations` endpoint:
Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.
While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% [word error rate](https://en.wikipedia.org/wiki/Word\_error\_rate) (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.
We support some ISO 639-1 and 639-3 language codes for GPT-4o based models. For language codes we don’t have, try prompting for specific languages (i.e., “Output in English”).
Timestamps
----------
By default, the Transcriptions API will output a transcript of the provided audio in text. The [`timestamp\_granularities[]` parameter](/docs/api-reference/audio/createTranscription#audio-createtranscription-timestamp\_granularities) enables a more structured and timestamped json output format, with timestamps at the segment, word level, or both. This enables word-level precision for transcripts and video edits, which allows for the removal of specific frames tied to individual words.
Timestamp options
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const transcription = await openai.audio.transcriptions.create({
file: fs.createReadStream("audio.mp3"),
model: "whisper-1",
response\_format: "verbose\_json",
timestamp\_granularities: ["word"]
});
console.log(transcription.words);
```
```python
from openai import OpenAI
client = OpenAI()
audio\_file = open("/path/to/file/speech.mp3", "rb")
transcription = client.audio.transcriptions.create(
file=audio\_file,
model="whisper-1",
response\_format="verbose\_json",
timestamp\_granularities=["word"]
)
print(transcription.words)
```
```bash
curl https://api.openai.com/v1/audio/transcriptions \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: multipart/form-data" \
-F file="@/path/to/file/audio.mp3" \
-F "timestamp\_granularities[]=word" \
-F model="whisper-1" \
-F response\_format="verbose\_json"
```
The `timestamp\_granularities[]` parameter is only supported for `whisper-1`.
Longer inputs
-------------
By default, the Transcriptions API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.
One way to handle this is to use the [PyDub open source Python package](https://github.com/jiaaro/pydub) to split the audio:
```python
from pydub import AudioSegment
song = AudioSegment.from\_mp3("good\_morning.mp3")

# PyDub handles time in milliseconds
ten\_minutes = 10 \* 60 \* 1000
first\_10\_minutes = song[:ten\_minutes]
first\_10\_minutes.export("good\_morning\_10.mp3", format="mp3")
```
\_OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.\_
Prompting
---------
You can use a [prompt](/docs/api-reference/audio/createTranscription#audio/createTranscription-prompt) to improve the quality of the transcripts generated by the Transcriptions API.
Prompting
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const transcription = await openai.audio.transcriptions.create({
file: fs.createReadStream("/path/to/file/speech.mp3"),
model: "gpt-4o-transcribe",
response\_format: "text",
prompt:"The following conversation is a lecture about the recent developments around OpenAI, GPT-4.5 and the future of AI.",
});
console.log(transcription.text);
```
```python
from openai import OpenAI
client = OpenAI()
audio\_file = open("/path/to/file/speech.mp3", "rb")
transcription = client.audio.transcriptions.create(
model="gpt-4o-transcribe",
file=audio\_file,
response\_format="text",
prompt="The following conversation is a lecture about the recent developments around OpenAI, GPT-4.5 and the future of AI."
)
print(transcription.text)
```
```bash
curl --request POST \
--url https://api.openai.com/v1/audio/transcriptions \
--header "Authorization: Bearer $OPENAI\_API\_KEY" \
--header 'Content-Type: multipart/form-data' \
--form file=@/path/to/file/speech.mp3 \
--form model=gpt-4o-transcribe \
--form prompt="The following conversation is a lecture about the recent developments around OpenAI, GPT-4.5 and the future of AI."
```
For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`, you can use the `prompt` parameter to improve the quality of the transcription by giving the model additional context similarly to how you would prompt other GPT-4o models.
Here are some examples of how prompting can help in different scenarios:
1. Prompts can help correct specific words or acronyms that the model misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL·E and GPT-3, which were previously written as "GDP 3" and "DALI": "The transcript is about OpenAI which makes technology like DALL·E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity."
2. To preserve the context of a file that was split into segments, prompt the model with the transcript of the preceding segment. The model uses relevant information from the previous audio, improving transcription accuracy. The `whisper-1` model only considers the final 224 tokens of the prompt and ignores anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English-only inputs, it uses the standard GPT-2 tokenizer. Find both tokenizers in the open source [Whisper Python package](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py#L361).
3. Sometimes the model skips punctuation in the transcript. To prevent this, use a simple prompt that includes punctuation: "Hello, welcome to my lecture."
4. The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, use a prompt that contains them: "Umm, let me think like, hmm... Okay, here's what I'm, like, thinking."
5. Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.
For `whisper-1`, the model tries to match the style of the prompt, so it's more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is more limited than our other language models and provides limited control over the generated text.
You can find more examples on improving your `whisper-1` transcriptions in the [improving reliability](#improving-reliability) section.
Streaming transcriptions
------------------------
There are two ways you can stream your transcription depending on your use case and whether you are trying to transcribe an already completed audio recording or handle an ongoing stream of audio and use OpenAI for turn detection.

### Streaming the transcription of a completed audio recording
If you have an already completed audio recording, either because it's an audio file or you are using your own turn detection (like push-to-talk), you can use our Transcription API with `stream=True` to receive a stream of [transcript events](/docs/api-reference/audio/transcript-text-delta-event) as soon as the model is done transcribing that part of the audio.
Stream transcriptions
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const stream = await openai.audio.transcriptions.create({
file: fs.createReadStream("/path/to/file/speech.mp3"),
model: "gpt-4o-mini-transcribe",
response\_format: "text",
stream: true,
});
for await (const event of stream) {
console.log(event);
}
```
```python
from openai import OpenAI
client = OpenAI()
audio\_file = open("/path/to/file/speech.mp3", "rb")
stream = client.audio.transcriptions.create(
model="gpt-4o-mini-transcribe",
file=audio\_file,
response\_format="text",
stream=True
)
for event in stream:
print(event)
```
```bash
curl --request POST \
--url https://api.openai.com/v1/audio/transcriptions \
--header "Authorization: Bearer $OPENAI\_API\_KEY" \
--header 'Content-Type: multipart/form-data' \
--form file=@example.wav \
--form model=whisper-1 \
--form stream=True
```
You will receive a stream of `transcript.text.delta` events as soon as the model is done transcribing that part of the audio, followed by a `transcript.text.done` event when the transcription is complete that includes the full transcript.
Additionally, you can use the `include[]` parameter to include `logprobs` in the response to get the log probabilities of the tokens in the transcription. These can be helpful to determine how confident the model is in the transcription of that particular part of the transcript.
Streamed transcription is not supported in `whisper-1`.

### Streaming the transcription of an ongoing audio recording
In the Realtime API, you can stream the transcription of an ongoing audio recording. To start a streaming session with the Realtime API, create a WebSocket connection with the following URL:
```text
wss://api.openai.com/v1/realtime?intent=transcription
```
Below is an example payload for setting up a transcription session:
```json
{
"type": "transcription\_session.update",
"input\_audio\_format": "pcm16",
"input\_audio\_transcription": {
"model": "gpt-4o-transcribe",
"prompt": "",
"language": ""
},
"turn\_detection": {
"type": "server\_vad",
"threshold": 0.5,
"prefix\_padding\_ms": 300,
"silence\_duration\_ms": 500,
},
"input\_audio\_noise\_reduction": {
"type": "near\_field"
},
"include": [
"item.input\_audio\_transcription.logprobs"
]
}
```
To stream audio data to the API, append audio buffers:
```json
{
"type": "input\_audio\_buffer.append",
"audio": "Base64EncodedAudioData"
}
```
When in VAD mode, the API will respond with `input\_audio\_buffer.committed` every time a chunk of speech has been detected. Use `input\_audio\_buffer.committed.item\_id` and `input\_audio\_buffer.committed.previous\_item\_id` to enforce the ordering.
The API responds with transcription events indicating speech start, stop, and completed transcriptions.
The primary resource used by the streaming ASR API is the `TranscriptionSession`:
```json
{
"object": "realtime.transcription\_session",
"id": "string",
"input\_audio\_format": "pcm16",
"input\_audio\_transcription": [{
"model": "whisper-1" | "gpt-4o-transcribe" | "gpt-4o-mini-transcribe",
"prompt": "string",
"language": "string"
}],
"turn\_detection": {
"type": "server\_vad",
"threshold": "float",
"prefix\_padding\_ms": "integer",
"silence\_duration\_ms": "integer",
} | null,
"input\_audio\_noise\_reduction": {
"type": "near\_field" | "far\_field"
},
"include": ["string"]
}
```
Authenticate directly through the WebSocket connection using your API key or an ephemeral token obtained from:
```text
POST /v1/realtime/transcription\_sessions
```
This endpoint returns an ephemeral token (`client\_secret`) to securely authenticate WebSocket connections.
Improving reliability
---------------------
One of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. Here are some different techniques to improve the reliability of Whisper in these cases:
Using the prompt parameter
The first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.
Because it wasn't trained with instruction-following techniques, Whisper operates more like a base GPT model. Keep in mind that Whisper only considers the first 224 tokens of the prompt.
Prompt parameter
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const transcription = await openai.audio.transcriptions.create({
file: fs.createReadStream("/path/to/file/speech.mp3"),
model: "whisper-1",
response\_format: "text",
prompt:"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.",
});
console.log(transcription.text);
```
```python
from openai import OpenAI
client = OpenAI()
audio\_file = open("/path/to/file/speech.mp3", "rb")
transcription = client.audio.transcriptions.create(
model="whisper-1",
file=audio\_file,
response\_format="text",
prompt="ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T."
)
print(transcription.text)
```
```bash
curl --request POST \
--url https://api.openai.com/v1/audio/transcriptions \
--header "Authorization: Bearer $OPENAI\_API\_KEY" \
--header 'Content-Type: multipart/form-data' \
--form file=@/path/to/file/speech.mp3 \
--form model=whisper-1 \
--form prompt="ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T."
```
While it increases reliability, this technique is limited to 224 tokens, so your list of SKUs needs to be relatively small for this to be a scalable solution.
Post-processing with GPT-4
The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.
We start by providing instructions for GPT-4 through the `system\_prompt` variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.
Post-processing
```javascript
const systemPrompt = `
You are a helpful assistant for the company ZyntriQix. Your task is
to correct any spelling discrepancies in the transcribed text. Make
sure that the names of the following products are spelled correctly:
ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array,
OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K.,
Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as
periods, commas, and capitalization, and use only the context provided.
`;
const transcript = await transcribe(audioFile);
const completion = await openai.chat.completions.create({
model: "gpt-4.1",
temperature: temperature,
messages: [
{
role: "system",
content: systemPrompt
},
{
role: "user",
content: transcript
}
],
store: true,
});
console.log(completion.choices[0].message.content);
```
```python
system\_prompt = """
You are a helpful assistant for the company ZyntriQix. Your task is to correct
any spelling discrepancies in the transcribed text. Make sure that the names of
the following products are spelled correctly: ZyntriQix, Digique Plus,
CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal
Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary
punctuation such as periods, commas, and capitalization, and use only the
context provided.
"""
def generate\_corrected\_transcript(temperature, system\_prompt, audio\_file):
response = client.chat.completions.create(
model="gpt-4.1",
temperature=temperature,
messages=[
{
"role": "system",
"content": system\_prompt
},
{
"role": "user",
"content": transcribe(audio\_file, "")
}
]
)
return completion.choices[0].message.content
corrected\_text = generate\_corrected\_transcript(
0, system\_prompt, fake\_company\_filepath
)
```
If you try this on your own audio file, you'll see that GPT-4 corrects many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter. It's also more reliable, as GPT-4 can be instructed and guided in ways that aren't possible with Whisper due to its lack of instruction following.
Was this page useful?


## Imported snippet – 2025-07-03 14:39:45

Vector embeddings
=================
Learn how to turn text into numbers, unlocking use cases like search.
New embedding models
`text-embedding-3-small` and `text-embedding-3-large`, our newest and most performant embedding models, are now available. They feature lower costs, higher multilingual performance, and new parameters to control the overall size.
What are embeddings?
--------------------
OpenAI’s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:
\* \*\*Search\*\* (where results are ranked by relevance to a query string)
\* \*\*Clustering\*\* (where text strings are grouped by similarity)
\* \*\*Recommendations\*\* (where items with related text strings are recommended)
\* \*\*Anomaly detection\*\* (where outliers with little relatedness are identified)
\* \*\*Diversity measurement\*\* (where similarity distributions are analyzed)
\* \*\*Classification\*\* (where text strings are classified by their most similar label)
An embedding is a vector (list) of floating point numbers. The [distance](#which-distance-function-should-i-use) between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.
Visit our [pricing page](https://openai.com/api/pricing/) to learn about embeddings pricing. Requests are billed based on the number of [tokens](/tokenizer) in the [input](/docs/api-reference/embeddings/create#embeddings/create-input).
How to get embeddings
---------------------
To get an embedding, send your text string to the [embeddings API endpoint](/docs/api-reference/embeddings) along with the embedding model name (e.g., `text-embedding-3-small`):
Example: Getting embeddings
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const embedding = await openai.embeddings.create({
model: "text-embedding-3-small",
input: "Your text string goes here",
encoding\_format: "float",
});
console.log(embedding);
```
```python
from openai import OpenAI
client = OpenAI()
response = client.embeddings.create(
input="Your text string goes here",
model="text-embedding-3-small"
)
print(response.data[0].embedding)
```
```bash
curl https://api.openai.com/v1/embeddings \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"input": "Your text string goes here",
"model": "text-embedding-3-small"
}'
```
The response contains the embedding vector (list of floating point numbers) along with some additional metadata. You can extract the embedding vector, save it in a vector database, and use for many different use cases.
```json
{
"object": "list",
"data": [
{
"object": "embedding",
"index": 0,
"embedding": [
-0.006929283495992422,
-0.005336422007530928,
-4.547132266452536e-05,
-0.024047505110502243
],
}
],
"model": "text-embedding-3-small",
"usage": {
"prompt\_tokens": 5,
"total\_tokens": 5
}
}
```
By default, the length of the embedding vector is `1536` for `text-embedding-3-small` or `3072` for `text-embedding-3-large`. To reduce the embedding's dimensions without losing its concept-representing properties, pass in the [dimensions parameter](/docs/api-reference/embeddings/create#embeddings-create-dimensions). Find more detail on embedding dimensions in the [embedding use case section](#use-cases).
Embedding models
----------------
OpenAI offers two powerful third-generation embedding model (denoted by `-3` in the model ID). Read the embedding v3 [announcement blog post](https://openai.com/blog/new-embedding-models-and-api-updates) for more details.
Usage is priced per input token. Below is an example of pricing pages of text per US dollar (assuming ~800 tokens per page):
|Model|~ Pages per dollar|Performance on MTEB eval|Max input|
|---|---|---|---|
|text-embedding-3-small|62,500|62.3%|8192|
|text-embedding-3-large|9,615|64.6%|8192|
|text-embedding-ada-002|12,500|61.0%|8192|
Use cases
---------
Here we show some representative use cases, using the [Amazon fine-food reviews dataset](https://www.kaggle.com/snap/amazon-fine-food-reviews).

### Obtaining the embeddings
The dataset contains a total of 568,454 food reviews left by Amazon users up to October 2012. We use a subset of the 1000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a `ProductId`, `UserId`, `Score`, review title (`Summary`) and review body (`Text`). For example:
|Product Id|User Id|Score|Summary|Text|
|---|---|---|---|---|
|B001E4KFG0|A3SGXH7AUHU8GW|5|Good Quality Dog Food|I have bought several of the Vitality canned...|
|B00813GRG4|A1D87F6ZCVE5NK|1|Not as Advertised|Product arrived labeled as Jumbo Salted Peanut...|
Below, we combine the review summary and review text into a single combined text. The model encodes this combined text and output a single vector embedding.
[Get\\_embeddings\\_from\\_dataset.ipynb](https://cookbook.openai.com/examples/get\_embeddings\_from\_dataset)
```python
from openai import OpenAI
client = OpenAI()
def get\_embedding(text, model="text-embedding-3-small"):
text = text.replace("\n", " ")
return client.embeddings.create(input = [text], model=model).data[0].embedding
df['ada\_embedding'] = df.combined.apply(lambda x: get\_embedding(x, model='text-embedding-3-small'))
df.to\_csv('output/embedded\_1k\_reviews.csv', index=False)
```
To load the data from a saved file, you can run the following:
```python
import pandas as pd
df = pd.read\_csv('output/embedded\_1k\_reviews.csv')
df['ada\_embedding'] = df.ada\_embedding.apply(eval).apply(np.array)
```
Reducing embedding dimensions
Using larger embeddings, for example storing them in a vector store for retrieval, generally costs more and consumes more compute, memory and storage than using smaller embeddings.
Both of our new embedding models were trained [with a technique](https://arxiv.org/abs/2205.13147) that allows developers to trade-off performance and cost of using embeddings. Specifically, developers can shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties by passing in the [`dimensions` API parameter](/docs/api-reference/embeddings/create#embeddings-create-dimensions). For example, on the MTEB benchmark, a `text-embedding-3-large` embedding can be shortened to a size of 256 while still outperforming an unshortened `text-embedding-ada-002` embedding with a size of 1536. You can read more about how changing the dimensions impacts performance in our [embeddings v3 launch blog post](https://openai.com/blog/new-embedding-models-and-api-updates#:~:text=Native%20support%20for%20shortening%20embeddings).
In general, using the `dimensions` parameter when creating the embedding is the suggested approach. In certain cases, you may need to change the embedding dimension after you generate it. When you change the dimension manually, you need to be sure to normalize the dimensions of the embedding as is shown below.
```python
from openai import OpenAI
import numpy as np
client = OpenAI()
def normalize\_l2(x):
x = np.array(x)
if x.ndim == 1:
norm = np.linalg.norm(x)
if norm == 0:
return x
return x / norm
else:
norm = np.linalg.norm(x, 2, axis=1, keepdims=True)
return np.where(norm == 0, x, x / norm)
response = client.embeddings.create(
model="text-embedding-3-small", input="Testing 123", encoding\_format="float"
)
cut\_dim = response.data[0].embedding[:256]
norm\_dim = normalize\_l2(cut\_dim)
print(norm\_dim)
```
Dynamically changing the dimensions enables very flexible usage. For example, when using a vector data store that only supports embeddings up to 1024 dimensions long, developers can now still use our best embedding model `text-embedding-3-large` and specify a value of 1024 for the `dimensions` API parameter, which will shorten the embedding down from 3072 dimensions, trading off some accuracy in exchange for the smaller vector size.
Question answering using embeddings-based search
[Question\\_answering\\_using\\_embeddings.ipynb](https://cookbook.openai.com/examples/question\_answering\_using\_embeddings)
There are many common cases where the model is not trained on data which contains key facts and information you want to make accessible when generating responses to a user query. One way of solving this, as shown below, is to put additional information into the context window of the model. This is effective in many use cases but leads to higher token costs. In this notebook, we explore the tradeoff between this approach and embeddings bases search.
```python
query = f"""Use the below article on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found, write "I don't know."
Article:
\"\"\"
{wikipedia\_article\_on\_curling}
\"\"\"
Question: Which athletes won the gold medal in curling at the 2022 Winter Olympics?"""
response = client.chat.completions.create(
messages=[
{'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},
{'role': 'user', 'content': query},
],
model=GPT\_MODEL,
temperature=0,
)
print(response.choices[0].message.content)
```
Text search using embeddings
[Semantic\\_text\\_search\\_using\\_embeddings.ipynb](https://cookbook.openai.com/examples/semantic\_text\_search\_using\_embeddings)
To retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents.
```python
from openai.embeddings\_utils import get\_embedding, cosine\_similarity
def search\_reviews(df, product\_description, n=3, pprint=True):
embedding = get\_embedding(product\_description, model='text-embedding-3-small')
df['similarities'] = df.ada\_embedding.apply(lambda x: cosine\_similarity(x, embedding))
res = df.sort\_values('similarities', ascending=False).head(n)
return res
res = search\_reviews(df, 'delicious beans', n=3)
```
Code search using embeddings
[Code\\_search.ipynb](https://cookbook.openai.com/examples/code\_search\_using\_embeddings)
Code search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the `text-embedding-3-small` model.
To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant.
```python
from openai.embeddings\_utils import get\_embedding, cosine\_similarity
df['code\_embedding'] = df['code'].apply(lambda x: get\_embedding(x, model='text-embedding-3-small'))
def search\_functions(df, code\_query, n=3, pprint=True, n\_lines=7):
embedding = get\_embedding(code\_query, model='text-embedding-3-small')
df['similarities'] = df.code\_embedding.apply(lambda x: cosine\_similarity(x, embedding))
res = df.sort\_values('similarities', ascending=False).head(n)
return res
res = search\_functions(df, 'Completions API tests', n=3)
```
Recommendations using embeddings
[Recommendation\\_using\\_embeddings.ipynb](https://cookbook.openai.com/examples/recommendation\_using\_embeddings)
Because shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.
Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the [AG news dataset](http://groups.di.unipi.it/~gulli/AG\_corpus\_of\_news\_articles.html) (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article.
```python
def recommendations\_from\_strings(
strings: List[str],
index\_of\_source\_string: int,
model="text-embedding-3-small",
) -> List[int]:
"""Return nearest neighbors of a given string."""

# get embeddings for all strings
embeddings = [embedding\_from\_string(string, model=model) for string in strings]

# get the embedding of the source string
query\_embedding = embeddings[index\_of\_source\_string]

# get distances between the source embedding and other embeddings (function from embeddings\_utils.py)
distances = distances\_from\_embeddings(query\_embedding, embeddings, distance\_metric="cosine")

# get indices of nearest neighbors (function from embeddings\_utils.py)
indices\_of\_nearest\_neighbors = indices\_of\_nearest\_neighbors\_from\_distances(distances)
return indices\_of\_nearest\_neighbors
```
Data visualization in 2D
[Visualizing\\_embeddings\\_in\\_2D.ipynb](https://cookbook.openai.com/examples/visualizing\_embeddings\_in\_2d)
The size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the t-SNE algorithm to transform the data into two dimensions.
We color the individual reviews based on the star rating which the reviewer has given:
\* 1-star: red
\* 2-star: dark orange
\* 3-star: gold
\* 4-star: turquoise
\* 5-star: dark green
![Amazon ratings visualized in language using t-SNE](https://cdn.openai.com/API/docs/images/embeddings-tsne.png)
The visualization seems to have produced roughly 3 clusters, one of which has mostly negative reviews.
```python
import pandas as pd
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import matplotlib
df = pd.read\_csv('output/embedded\_1k\_reviews.csv')
matrix = df.ada\_embedding.apply(eval).to\_list()

# Create a t-SNE model and transform the data
tsne = TSNE(n\_components=2, perplexity=15, random\_state=42, init='random', learning\_rate=200)
vis\_dims = tsne.fit\_transform(matrix)
colors = ["red", "darkorange", "gold", "turquiose", "darkgreen"]
x = [x for x,y in vis\_dims]
y = [y for x,y in vis\_dims]
color\_indices = df.Score.values - 1
colormap = matplotlib.colors.ListedColormap(colors)
plt.scatter(x, y, c=color\_indices, cmap=colormap, alpha=0.3)
plt.title("Amazon ratings visualized in language using t-SNE")
```
Embedding as a text feature encoder for ML algorithms
[Regression\\_using\\_embeddings.ipynb](https://cookbook.openai.com/examples/regression\_using\_embeddings)
An embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task.
We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks.
This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification.
```python
from sklearn.model\_selection import train\_test\_split
X\_train, X\_test, y\_train, y\_test = train\_test\_split(
list(df.ada\_embedding.values),
df.Score,
test\_size = 0.2,
random\_state=42
)
```

#### Regression using the embedding features
Embeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer’s star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews.
We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star.
```python
from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor(n\_estimators=100)
rfr.fit(X\_train, y\_train)
preds = rfr.predict(X\_test)
```
Classification using the embedding features
[Classification\\_using\\_embeddings.ipynb](https://cookbook.openai.com/examples/classification\_using\_embeddings)
This time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars.
After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression.
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification\_report, accuracy\_score
clf = RandomForestClassifier(n\_estimators=100)
clf.fit(X\_train, y\_train)
preds = clf.predict(X\_test)
```
Zero-shot classification
[Zero-shot\\_classification\\_with\\_embeddings.ipynb](https://cookbook.openai.com/examples/zero-shot\_classification\_with\_embeddings)
We can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity.
```python
from openai.embeddings\_utils import cosine\_similarity, get\_embedding
df= df[df.Score!=3]
df['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})
labels = ['negative', 'positive']
label\_embeddings = [get\_embedding(label, model=model) for label in labels]
def label\_score(review\_embedding, label\_embeddings):
return cosine\_similarity(review\_embedding, label\_embeddings[1]) - cosine\_similarity(review\_embedding, label\_embeddings[0])
prediction = 'positive' if label\_score('Sample Review', label\_embeddings) > 0 else 'negative'
```
Obtaining user and product embeddings for cold-start recommendation
[User\\_and\\_product\\_embeddings.ipynb](https://cookbook.openai.com/examples/user\_and\_product\_embeddings)
We can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product.
We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product.
![Boxplot grouped by Score](https://cdn.openai.com/API/docs/images/embeddings-boxplot.png)
```python
user\_embeddings = df.groupby('UserId').ada\_embedding.apply(np.mean)
prod\_embeddings = df.groupby('ProductId').ada\_embedding.apply(np.mean)
```
Clustering
[Clustering.ipynb](https://cookbook.openai.com/examples/clustering)
Clustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset.
In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews.
![Clusters identified visualized in language 2d using t-SNE](https://cdn.openai.com/API/docs/images/embeddings-cluster.png)
```python
import numpy as np
from sklearn.cluster import KMeans
matrix = np.vstack(df.ada\_embedding.values)
n\_clusters = 4
kmeans = KMeans(n\_clusters = n\_clusters, init='k-means++', random\_state=42)
kmeans.fit(matrix)
df['Cluster'] = kmeans.labels\_
```
FAQ
---

### How can I tell how many tokens a string has before I embed it?
In Python, you can split a string into tokens with OpenAI's tokenizer [`tiktoken`](https://github.com/openai/tiktoken).
Example code:
```python
import tiktoken
def num\_tokens\_from\_string(string: str, encoding\_name: str) -> int:
"""Returns the number of tokens in a text string."""
encoding = tiktoken.get\_encoding(encoding\_name)
num\_tokens = len(encoding.encode(string))
return num\_tokens
num\_tokens\_from\_string("tiktoken is great!", "cl100k\_base")
```
For third-generation embedding models like `text-embedding-3-small`, use the `cl100k\_base` encoding.
More details and example code are in the OpenAI Cookbook guide [how to count tokens with tiktoken](https://cookbook.openai.com/examples/how\_to\_count\_tokens\_with\_tiktoken).

### How can I retrieve K nearest embedding vectors quickly?
For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API [in our Cookbook](https://cookbook.openai.com/examples/vector\_databases/readme) on GitHub.

### Which distance function should I use?
We recommend [cosine similarity](https://en.wikipedia.org/wiki/Cosine\_similarity). The choice of distance function typically doesn't matter much.
OpenAI embeddings are normalized to length 1, which means that:
\* Cosine similarity can be computed slightly faster using just a dot product
\* Cosine similarity and Euclidean distance will result in the identical rankings

### Can I share my embeddings online?
Yes, customers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our [Terms of Use](https://openai.com/policies/terms-of-use).

### Do V3 embedding models know about recent events?
No, the `text-embedding-3-large` and `text-embedding-3-small` models lack knowledge of events that occurred after September 2021. This is generally not as much of a limitation as it would be for text generation models but in certain edge cases it can reduce performance.
Was this page useful?


## Imported snippet – 2025-07-03 14:39:51

Moderation
==========
Identify potentially harmful content in text and images.
Use the [moderations](/docs/api-reference/moderations) endpoint to check whether text or images are potentially harmful. If harmful content is identified, you can take corrective action, like filtering content or intervening with user accounts creating offending content. The moderation endpoint is free to use.
You can use two models for this endpoint:
\* `omni-moderation-latest`: This model and all snapshots support more categorization options and multi-modal inputs.
\* `text-moderation-latest` \*\*(Legacy)\*\*: Older model that supports only text inputs and fewer input categorizations. The newer omni-moderation models will be the best choice for new applications.
Quickstart
----------
Use the tabs below to see how you can moderate text inputs or image inputs, using our [official SDKs](/docs/libraries) and the [omni-moderation-latest model](/docs/models#moderation):
Moderate text inputs
Get classification information for a text input
```python
from openai import OpenAI
client = OpenAI()
response = client.moderations.create(
model="omni-moderation-latest",
input="...text to classify goes here...",
)
print(response)
```
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const moderation = await openai.moderations.create({
model: "omni-moderation-latest",
input: "...text to classify goes here...",
});
console.log(moderation);
```
```bash
curl https://api.openai.com/v1/moderations \
-X POST \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "omni-moderation-latest",
"input": "...text to classify goes here..."
}'
```
Moderate images and text
Get classification information for image and text input
```python
from openai import OpenAI
client = OpenAI()
response = client.moderations.create(
model="omni-moderation-latest",
input=[
{"type": "text", "text": "...text to classify goes here..."},
{
"type": "image\_url",
"image\_url": {
"url": "https://example.com/image.png",

# can also use base64 encoded image URLs

# "url": "data:image/jpeg;base64,abcdefg..."
}
},
],
)
print(response)
```
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const moderation = await openai.moderations.create({
model: "omni-moderation-latest",
input: [
{ type: "text", text: "...text to classify goes here..." },
{
type: "image\_url",
image\_url: {
url: "https://example.com/image.png"
// can also use base64 encoded image URLs
// url: "data:image/jpeg;base64,abcdefg..."
}
}
],
});
console.log(moderation);
```
```bash
curl https://api.openai.com/v1/moderations \
-X POST \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "omni-moderation-latest",
"input": [
{ "type": "text", "text": "...text to classify goes here..." },
{
"type": "image\_url",
"image\_url": {
"url": "https://example.com/image.png"
}
}
]
}'
```
Here's a full example output, where the input is an image from a single frame of a war movie. The model correctly predicts indicators of violence in the image, with a `violence` category score of greater than 0.8.
```json
{
"id": "modr-970d409ef3bef3b70c73d8232df86e7d",
"model": "omni-moderation-latest",
"results": [
{
"flagged": true,
"categories": {
"sexual": false,
"sexual/minors": false,
"harassment": false,
"harassment/threatening": false,
"hate": false,
"hate/threatening": false,
"illicit": false,
"illicit/violent": false,
"self-harm": false,
"self-harm/intent": false,
"self-harm/instructions": false,
"violence": true,
"violence/graphic": false
},
"category\_scores": {
"sexual": 2.34135824776394e-7,
"sexual/minors": 1.6346470245419304e-7,
"harassment": 0.0011643905680426018,
"harassment/threatening": 0.0022121340080906377,
"hate": 3.1999824407395835e-7,
"hate/threatening": 2.4923252458203563e-7,
"illicit": 0.0005227032493135171,
"illicit/violent": 3.682979260160596e-7,
"self-harm": 0.0011175734280627694,
"self-harm/intent": 0.0006264858507989037,
"self-harm/instructions": 7.368592981140821e-8,
"violence": 0.8599265510337075,
"violence/graphic": 0.37701736389561064
},
"category\_applied\_input\_types": {
"sexual": [
"image"
],
"sexual/minors": [],
"harassment": [],
"harassment/threatening": [],
"hate": [],
"hate/threatening": [],
"illicit": [],
"illicit/violent": [],
"self-harm": [
"image"
],
"self-harm/intent": [
"image"
],
"self-harm/instructions": [
"image"
],
"violence": [
"image"
],
"violence/graphic": [
"image"
]
}
}
]
}
```
The output has several categories in the JSON response, which tell you which (if any) categories of content are present in the inputs, and to what degree the model believes them to be present.
||
|flagged|Set to true if the model classifies the content as potentially harmful, false otherwise.|
|categories|Contains a dictionary of per-category violation flags. For each category, the value is true if the model flags the corresponding category as violated, false otherwise.|
|category\_scores|Contains a dictionary of per-category scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. The value is between 0 and 1, where higher values denote higher confidence.|
|category\_applied\_input\_types|This property contains information on which input types were flagged in the response, for each category. For example, if the both the image and text inputs to the model are flagged for "violence/graphic", the violence/graphic property will be set to ["image", "text"]. This is only available on omni models.|
We plan to continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on `category\_scores` may need recalibration over time.
Content classifications
-----------------------
The table below describes the types of content that can be detected in the moderation API, along with which models and input types are supported for each category.
Categories marked as "Text only" do not support image inputs. If you send only images (without accompanying text) to the `omni-moderation-latest` model, it will return a score of 0 for these unsupported categories.
||
|harassment|Content that expresses, incites, or promotes harassing language towards any target.|All|Text only|
|harassment/threatening|Harassment content that also includes violence or serious harm towards any target.|All|Text only|
|hate|Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harassment.|All|Text only|
|hate/threatening|Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.|All|Text only|
|illicit|Content that gives advice or instruction on how to commit illicit acts. A phrase like "how to shoplift" would fit this category.|Omni only|Text only|
|illicit/violent|The same types of content flagged by the illicit category, but also includes references to violence or procuring a weapon.|Omni only|Text only|
|self-harm|Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.|All|Text and images|
|self-harm/intent|Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.|All|Text and images|
|self-harm/instructions|Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.|All|Text and images|
|sexual|Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).|All|Text and images|
|sexual/minors|Sexual content that includes an individual who is under 18 years old.|All|Text only|
|violence|Content that depicts death, violence, or physical injury.|All|Text and images|
|violence/graphic|Content that depicts death, violence, or physical injury in graphic detail.|All|Text and images|
Was this page useful?
