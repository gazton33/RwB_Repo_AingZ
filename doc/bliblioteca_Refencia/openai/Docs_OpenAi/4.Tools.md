

## Imported snippet – 2025-07-03 11:29:25

Using tools
===========
Use tools like remote MCP servers or web search to extend the model's capabilities.
When generating model responses, you can extend model capabilities using built-in \*\*tools\*\*. These tools help models access additional context and information from the web or your files. The example below uses the [web search tool](/docs/guides/tools-web-search) to use the latest information from the web to generate a model response.
Include web search results for the model response
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const response = await client.responses.create({
model: "gpt-4.1",
tools: [ { type: "web\_search\_preview" } ],
input: "What was a positive news story from today?",
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4.1",
tools=[{"type": "web\_search\_preview"}],
input="What was a positive news story from today?"
)
print(response.output\_text)
```
```bash
curl "https://api.openai.com/v1/responses" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"tools": [{"type": "web\_search\_preview"}],
"input": "what was a positive news story from today?"
}'
```
You can include several built-in tools from the available tools list below and let the model decide which tools to use based on the conversation.
Available tools
---------------
Here's an overview of the tools available in the OpenAI platform—select one of them for further guidance on usage.
[
Function calling
Call custom code to give the model access to additional data and capabilities.
](/docs/guides/function-calling)[
Web search
Include data from the Internet in model response generation.
](/docs/guides/tools-web-search)[
Remote MCP servers
Give the model access to new capabilities via Model Context Protocol (MCP) servers.
](/docs/guides/tools-remote-mcp)[
File search
Search the contents of uploaded files for context when generating a response.
](/docs/guides/tools-file-search)[
Image Generation
Generate or edit images using GPT Image.
](/docs/guides/tools-image-generation)[
Code interpreter
Allow the model to execute code in a secure container.
](/docs/guides/tools-code-interpreter)[
Computer use
Create agentic workflows that enable a model to control a computer interface.
](/docs/guides/tools-computer-use)
Usage in the API
----------------
When making a request to generate a [model response](/docs/api-reference/responses/create), you can enable tool access by specifying configurations in the `tools` parameter. Each tool has its own unique configuration requirements—see the [Available tools](#available-tools) section for detailed instructions.
Based on the provided [prompt](/docs/guides/text), the model automatically decides whether to use a configured tool. For instance, if your prompt requests information beyond the model's training cutoff date and web search is enabled, the model will typically invoke the web search tool to retrieve relevant, up-to-date information.
You can explicitly control or guide this behavior by setting the `tool\_choice` parameter [in the API request](/docs/api-reference/responses/create).

### Function calling
In addition to built-in tools, you can define custom functions using the `tools` array. These custom functions allow the model to call your application's code, enabling access to specific data or capabilities not directly available within the model.
Learn more in the [function calling guide](/docs/guides/function-calling).
Was this page useful?


## Imported snippet – 2025-07-03 11:29:37

Remote MCP
==========
Allow models to use remote MCP servers to perform tasks.
[Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. The MCP tool in the Responses API allows developers to give the model access to tools hosted on \*\*Remote MCP servers\*\*. These are MCP servers maintained by developers and organizations across the internet that expose these tools to MCP clients, like the Responses API.
Calling a remote MCP server with the Responses API is straightforward. For example, here's how you can use the [DeepWiki](https://deepwiki.com/) MCP server to ask questions about nearly any public GitHub repository.
A Responses API request with MCP tools enabled
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"tools": [
{
"type": "mcp",
"server\_label": "deepwiki",
"server\_url": "https://mcp.deepwiki.com/mcp",
"require\_approval": "never"
}
],
"input": "What transport protocols are supported in the 2025-03-26 version of the MCP spec?"
}'
```
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const resp = await client.responses.create({
model: "gpt-4.1",
tools: [
{
type: "mcp",
server\_label: "deepwiki",
server\_url: "https://mcp.deepwiki.com/mcp",
require\_approval: "never",
},
],
input: "What transport protocols are supported in the 2025-03-26 version of the MCP spec?",
});
console.log(resp.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
resp = client.responses.create(
model="gpt-4.1",
tools=[
{
"type": "mcp",
"server\_label": "deepwiki",
"server\_url": "https://mcp.deepwiki.com/mcp",
"require\_approval": "never",
},
],
input="What transport protocols are supported in the 2025-03-26 version of the MCP spec?",
)
print(resp.output\_text)
```
It is very important that developers trust any remote MCP server they use with the Responses API. A malicious server can exfiltrate sensitive data from anything that enters the model's context. Carefully review the [Risks and Safety](#risks-and-safety) section below before using this tool.
The MCP ecosystem
-----------------
We are still in the early days of the MCP ecosystem. Some popular remote MCP servers today include [Cloudflare](https://developers.cloudflare.com/agents/guides/remote-mcp-server/), [Hubspot](https://developers.hubspot.com/mcp), [Intercom](https://developers.intercom.com/docs/guides/mcp), [Paypal](https://developer.paypal.com/tools/mcp-server/), [Pipedream](https://pipedream.com/docs/connect/mcp/openai/), [Plaid](https://plaid.com/docs/mcp/), [Shopify](https://shopify.dev/docs/apps/build/storefront-mcp), [Stripe](https://docs.stripe.com/mcp), [Square](https://developer.squareup.com/docs/mcp), [Twilio](https://github.com/twilio-labs/function-templates/tree/main/mcp-server) and [Zapier](https://zapier.com/mcp). We expect many more servers—and registries making it easy to discover these servers—to launch in the coming months. The MCP protocol itself is also early, and we expect to add many more updates to our MCP tool as the protocol evolves.
How it works
------------
The MCP tool works only in the [Responses API](/docs/api-reference/responses/create), and is available across all our new models (gpt-4o, gpt-4.1, and our reasoning models). When you're using the MCP tool, you only pay for [tokens](/docs/pricing) used when importing tool definitions or making tool calls—there are no additional fees involved.

### Step 1: Getting the list of tools from the MCP server
The first thing the Responses API does when you attach a remote MCP server to the `tools` array, is attempt to get a list of tools from the server. The Responses API supports remote MCP servers that support either the Streamable HTTP or the HTTP/SSE transport protocol.
If successful in retrieving the list of tools, a new `mcp\_list\_tools` output item will be visible in the Response object that is created for each MCP server. The `tools` property of this object will show the tools that were successfully imported.
```json
{
"id": "mcpl\_682d4379df088191886b70f4ec39f90403937d5f622d7a90",
"type": "mcp\_list\_tools",
"server\_label": "deepwiki",
"tools": [
{
"name": "read\_wiki\_structure",
"input\_schema": {
"type": "object",
"properties": {
"repoName": {
"type": "string",
"description": "GitHub repository: owner/repo (e.g. \"facebook/react\")"
}
},
"required": [
"repoName"
],
"additionalProperties": false,
"annotations": null,
"description": "",
"$schema": "http://json-schema.org/draft-07/schema#"
}
},
// ... other tools
]
}
```
As long as the `mcp\_list\_tools` item is present in the context of the model, we will not attempt to pull a refreshed list of tools from an MCP server. We recommend you keep this item in the model's context as part of every conversation or workflow execution to optimize for latency.

#### Filtering tools
Some MCP servers can have dozens of tools, and exposing many tools to the model can result in high cost and latency. If you're only interested in a subset of tools an MCP server exposes, you can use the `allowed\_tools` parameter to only import those tools.
Constrain allowed tools
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"tools": [
{
"type": "mcp",
"server\_label": "deepwiki",
"server\_url": "https://mcp.deepwiki.com/mcp",
"require\_approval": "never",
"allowed\_tools": ["ask\_question"]
}
],
"input": "What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?"
}'
```
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const resp = await client.responses.create({
model: "gpt-4.1",
tools: [{
type: "mcp",
server\_label: "deepwiki",
server\_url: "https://mcp.deepwiki.com/mcp",
require\_approval: "never",
allowed\_tools: ["ask\_question"],
}],
input: "What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?",
});
console.log(resp.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
resp = client.responses.create(
model="gpt-4.1",
tools=[{
"type": "mcp",
"server\_label": "deepwiki",
"server\_url": "https://mcp.deepwiki.com/mcp",
"require\_approval": "never",
"allowed\_tools": ["ask\_question"],
}],
input="What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?",
)
print(resp.output\_text)
```

### Step 2: Calling tools
Once the model has access to these tool definitions, it may choose to call them depending on what's in the model's context. When the model decides to call an MCP tool, we make an request to the remote MCP server to call the tool, take it's output and put that into the model's context. This creates an `mcp\_call` item which looks like this:
```json
{
"id": "mcp\_682d437d90a88191bf88cd03aae0c3e503937d5f622d7a90",
"type": "mcp\_call",
"approval\_request\_id": null,
"arguments": "{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\",\"question\":\"What transport protocols does the 2025-03-26 version of the MCP spec support?\"}",
"error": null,
"name": "ask\_question",
"output": "The 2025-03-26 version of the Model Context Protocol (MCP) specification supports two standard transport mechanisms: `stdio` and `Streamable HTTP` ...",
"server\_label": "deepwiki"
}
```
As you can see, this includes both the arguments the model decided to use for this tool call, and the `output` that the remote MCP server returned. All models can choose to make multiple (MCP) tool calls in the Responses API, and so, you may see several of these items generated in a single Response API request.
Failed tool calls will populate the error field of this item with MCP protocol errors, MCP tool execution errors, or general connectivity errors. The MCP errors are documented in the MCP spec [here](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#error-handling).

#### Approvals
By default, OpenAI will request your approval before any data is shared with a remote MCP server. Approvals help you maintain control and visibility over what data is being sent to an MCP server. We highly recommend that you carefully review (and optionally, log) all data being shared with a remote MCP server. A request for an approval to make an MCP tool call creates a `mcp\_approval\_request` item in the Response's output that looks like this:
```json
{
"id": "mcpr\_682d498e3bd4819196a0ce1664f8e77b04ad1e533afccbfa",
"type": "mcp\_approval\_request",
"arguments": "{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\",\"question\":\"What transport protocols are supported in the 2025-03-26 version of the MCP spec?\"}",
"name": "ask\_question",
"server\_label": "deepwiki"
}
```
You can then respond to this by creating a new Response object and appending an `mcp\_approval\_response` item to it.
Approving the use of tools in an API request
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"tools": [
{
"type": "mcp",
"server\_label": "deepwiki",
"server\_url": "https://mcp.deepwiki.com/mcp"
}
],
"previous\_response\_id": "resp\_682d498bdefc81918b4a6aa477bfafd904ad1e533afccbfa",
"input": [{
"type": "mcp\_approval\_response",
"approve": true,
"approval\_request\_id": "mcpr\_682d498e3bd4819196a0ce1664f8e77b04ad1e533afccbfa"
}]
}'
```
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const resp = await client.responses.create({
model: "gpt-4.1",
tools: [{
type: "mcp",
server\_label: "deepwiki",
server\_url: "https://mcp.deepwiki.com/mcp",
}],
previous\_response\_id: "resp\_682d498bdefc81918b4a6aa477bfafd904ad1e533afccbfa",
input: [{
type: "mcp\_approval\_response",
approve: true,
approval\_request\_id: "mcpr\_682d498e3bd4819196a0ce1664f8e77b04ad1e533afccbfa"
}],
});
console.log(resp.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
resp = client.responses.create(
model="gpt-4.1",
tools=[{
"type": "mcp",
"server\_label": "deepwiki",
"server\_url": "https://mcp.deepwiki.com/mcp",
}],
previous\_response\_id="resp\_682d498bdefc81918b4a6aa477bfafd904ad1e533afccbfa",
input=[{
"type": "mcp\_approval\_response",
"approve": True,
"approval\_request\_id": "mcpr\_682d498e3bd4819196a0ce1664f8e77b04ad1e533afccbfa"
}],
)
print(resp.output\_text)
```
Here we're using the `previous\_response\_id` parameter to chain this new Response, with the previous Response that generated the approval request. But you can also pass back the [outputs from one response, as inputs into another](/docs/guides/conversation-state#manually-manage-conversation-state) for maximum control over what enter's the model's context.
If and when you feel comfortable trusting a remote MCP server, you can choose to skip the approvals for reduced latency. To do this, you can set the `require\_approval` parameter of the MCP tool to an object listing just the tools you'd like to skip approvals for like shown below, or set it to the value `'never'` to skip approvals for all tools in that remote MCP server.
Never require approval for some tools
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"tools": [
{
"type": "mcp",
"server\_label": "deepwiki",
"server\_url": "https://mcp.deepwiki.com/mcp",
"require\_approval": {
"never": {
"tool\_names": ["ask\_question", "read\_wiki\_structure"]
}
}
}
],
"input": "What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?"
}'
```
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const resp = await client.responses.create({
model: "gpt-4.1",
tools: [
{
type: "mcp",
server\_label: "deepwiki",
server\_url: "https://mcp.deepwiki.com/mcp",
require\_approval: {
never: {
tool\_names: ["ask\_question", "read\_wiki\_structure"]
}
}
},
],
input: "What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?",
});
console.log(resp.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
resp = client.responses.create(
model="gpt-4.1",
tools=[
{
"type": "mcp",
"server\_label": "deepwiki",
"server\_url": "https://mcp.deepwiki.com/mcp",
"require\_approval": {
"never": {
"tool\_names": ["ask\_question", "read\_wiki\_structure"]
}
}
},
],
input="What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?",
)
print(resp.output\_text)
```
Authentication
--------------
Unlike the DeepWiki MCP server, most other MCP servers require authentication. The MCP tool in the Responses API gives you the ability to flexibly specify headers that should be included in any request made to a remote MCP server. These headers can be used to share API keys, oAuth access tokens, or any other authentication scheme the remote MCP server implements.
The most common header used by remote MCP servers is the `Authorization` header. This is what passing this header looks like:
Use Stripe MCP tool
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"input": "Create a payment link for $20",
"tools": [
{
"type": "mcp",
"server\_label": "stripe",
"server\_url": "https://mcp.stripe.com",
"headers": {
"Authorization": "Bearer $STRIPE\_API\_KEY"
}
}
]
}'
```
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const resp = await client.responses.create({
model: "gpt-4.1",
input: "Create a payment link for $20",
tools: [
{
type: "mcp",
server\_label: "stripe",
server\_url: "https://mcp.stripe.com",
headers: {
Authorization: "Bearer $STRIPE\_API\_KEY"
}
}
]
});
console.log(resp.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
resp = client.responses.create(
model="gpt-4.1",
input="Create a payment link for $20",
tools=[
{
"type": "mcp",
"server\_label": "stripe",
"server\_url": "https://mcp.stripe.com",
"headers": {
"Authorization": "Bearer $STRIPE\_API\_KEY"
}
}
]
)
print(resp.output\_text)
```
To prevent the leakage of sensitive keys, the Responses API does not store the values of \*\*any\*\* string you provide in the `headers` object. These values will also not be visible in the Response object created. Additionally, because some remote MCP servers generate authenticated URLs, we also discard the \_path\_ portion of the `server\_url` in our responses (i.e. `example.com/mcp` becomes `example.com`). Because of this, you must send the full path of the MCP `server\_url` and any relevant `headers` in every Responses API creation request you make.
Risks and safety
----------------
The MCP tool permits you to connect OpenAI to services that have not been verified by OpenAI and allows OpenAI to access, send and receive data, and take action in these services. All MCP servers are third-party services that are subject to their own terms and conditions.
If you come across a malicious MCP server, please report it to `security@openai.com`.

#### Connecting to trusted servers
Pick official servers hosted by the service providers themselves (e.g. we recommend connecting to the Stripe server hosted by Stripe themselves on mcp.stripe.com, instead of a Stripe MCP server hosted by a third party). Because there aren't too many official remote MCP servers today, you may be tempted to use a MCP server hosted by an organization that doesn't operate that server and simply proxies request to that service via your API. If you must do this, be extra careful in doing your due diligence on these "aggregators", and carefully review how they use your data.

#### Log and review data being shared with third party MCP servers.
Because MCP servers define their own tool definitions, they may request for data that you may not always be comfortable sharing with the host of that MCP server. Because of this, the MCP tool in the Responses API defaults to requiring approvals of each MCP tool call being made. When developing your application, review the type of data being shared with these MCP servers carefully and robustly. Once you gain confidence in your trust of this MCP server, you can skip these approvals for more performant execution.
We also recommend logging any data sent to MCP servers. If you're using the Responses API with `store=true`, these data are already logged via the API for 30 days unless Zero Data Retention is enabled for your organization. You may also want to log these data in your own systems and perform periodic reviews on this to ensure data is being shared per your expectations.
Malicious MCP servers may include hidden instructions (prompt injections) designed to make OpenAI models behave unexpectedly. While OpenAI has implemented built-in safeguards to help detect and block these threats, it's essential to carefully review inputs and outputs, and ensure connections are established only with trusted servers.
MCP servers may update tool behavior unexpectedly, potentially leading to unintended or malicious behavior.

#### Implications on Zero Data Retention and Data Residency
The MCP tool is compatible with Zero Data Retention and Data Residency, but it's important to note that MCP servers are third-party services, and data sent to an MCP server is subject to their data retention and data residency policies.
In other words, if you're an organization with Data Residency in Europe, OpenAI will limit inference and storage of Customer Content to take place in Europe up until the point communication or data is sent to the MCP server. It is your responsibility to ensure that the MCP server also adheres to any Zero Data Retention or Data Residency requirements you may have. Learn more about Zero Data Retention and Data Residency [here](/docs/guides/your-data).
Usage notes
-----------
||
|ResponsesChat CompletionsAssistants|Tier 1200 RPMTier 2 and 31000 RPMTier 4 and 52000 RPM|PricingZDR and data residency|
Was this page useful?


## Imported snippet – 2025-07-03 11:29:40

Web search
==========
Allow models to search the web for the latest information before generating a response.
Using the [Responses API](/docs/api-reference/responses), you can enable web search by configuring it in the `tools` array in an API request to generate content. Like any other tool, the model can choose to search the web or not based on the content of the input prompt.
Web search tool example
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const response = await client.responses.create({
model: "gpt-4.1",
tools: [ { type: "web\_search\_preview" } ],
input: "What was a positive news story from today?",
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4.1",
tools=[{"type": "web\_search\_preview"}],
input="What was a positive news story from today?"
)
print(response.output\_text)
```
```bash
curl "https://api.openai.com/v1/responses" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"tools": [{"type": "web\_search\_preview"}],
"input": "what was a positive news story from today?"
}'
```
Web search tool versions
The current default version of the web search tool is:
`web\_search\_preview`
Which points to a dated version:
`web\_search\_preview\_2025\_03\_11`
As the tool evolves, future dated snapshot versions will be documented in the [API reference](/docs/api-reference/responses/create).
You can also force the use of the `web\_search\_preview` tool by using the `tool\_choice` parameter, and setting it to `{type: "web\_search\_preview"}` - this can help ensure lower latency and more consistent results.
Output and citations
--------------------
Model responses that use the web search tool will include two parts:
\* A `web\_search\_call` output item with the ID of the search call, along with the action taken in `web\_search\_call.action`. The action is one of:
\* `search`, which represents a web search. It will usually (but not always) includes the search `query` and `domains` which were searched. Search actions are the only actions that incur cost, [as described here](docs/pricing#built-in-tools).
\* `open\_page`, which represents a page being opened. Only emitted by Deep Research models.
\* `find\_in\_page`, which represents searching within a page. Only emitted by Deep Research models.
\* A `message` output item containing:
\* The text result in `message.content[0].text`
\* Annotations `message.content[0].annotations` for the cited URLs
By default, the model's response will include inline citations for URLs found in the web search results. In addition to this, the `url\_citation` annotation object will contain the URL, title and location of the cited source.
When displaying web results or information contained in web results to end users, inline citations must be made clearly visible and clickable in your user interface.
```json
[
{
"type": "web\_search\_call",
"id": "ws\_67c9fa0502748190b7dd390736892e100be649c1a5ff9609",
"status": "completed"
},
{
"id": "msg\_67c9fa077e288190af08fdffda2e34f20be649c1a5ff9609",
"type": "message",
"status": "completed",
"role": "assistant",
"content": [
{
"type": "output\_text",
"text": "On March 6, 2025, several news...",
"annotations": [
{
"type": "url\_citation",
"start\_index": 2606,
"end\_index": 2758,
"url": "https://...",
"title": "Title..."
}
]
}
]
}
]
```
User location
-------------
To refine search results based on geography, you can specify an approximate user location using country, city, region, and/or timezone.
\* The `city` and `region` fields are free text strings, like `Minneapolis` and `Minnesota` respectively.
\* The `country` field is a two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO\_3166-1), like `US`.
\* The `timezone` field is an [IANA timezone](https://timeapi.io/documentation/iana-timezones) like `America/Chicago`.
Note that user location is not supported for deep research models using web search.
Customizing user location
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="o4-mini",
tools=[{
"type": "web\_search\_preview",
"user\_location": {
"type": "approximate",
"country": "GB",
"city": "London",
"region": "London",
}
}],
input="What are the best restaurants around Granary Square?",
)
print(response.output\_text)
```
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "o4-mini",
tools: [{
type: "web\_search\_preview",
user\_location: {
type: "approximate",
country: "GB",
city: "London",
region: "London"
}
}],
input: "What are the best restaurants around Granary Square?",
});
console.log(response.output\_text);
```
```bash
curl "https://api.openai.com/v1/responses" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "o4-mini",
"tools": [{
"type": "web\_search\_preview",
"user\_location": {
"type": "approximate",
"country": "GB",
"city": "London",
"region": "London"
}
}],
"input": "What are the best restaurants around Granary Square?"
}'
```
Search context size
-------------------
When using this tool, the `search\_context\_size` parameter controls how much context is retrieved from the web to help the tool formulate a response. The tokens used by the search tool do \*\*not\*\* affect the context window of the main model specified in the `model` parameter in your response creation request. These tokens are also \*\*not\*\* carried over from one turn to another — they're simply used to formulate the tool response and then discarded.
Choosing a context size impacts:
\* \*\*Cost\*\*: Pricing of our search tool varies based on the value of this parameter. Higher context sizes are more expensive. See tool pricing [here](/docs/pricing).
\* \*\*Quality\*\*: Higher search context sizes generally provide richer context, resulting in more accurate, comprehensive answers.
\* \*\*Latency\*\*: Higher context sizes require processing more tokens, which can slow down the tool's response time.
Available values:
\* \*\*`high`\*\*: Most comprehensive context, highest cost, slower response.
\* \*\*`medium`\*\* (default): Balanced context, cost, and latency.
\* \*\*`low`\*\*: Least context, lowest cost, fastest response, but potentially lower answer quality.
Again, tokens used by the search tool do \*\*not\*\* impact main model's token usage and are not carried over from turn to turn. Check the [pricing page](/docs/pricing) for details on costs associated with each context size.
Context size configuration is not supported for o3, o3-pro, o4-mini, and deep research models.
Customizing search context size
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="o4-mini",
tools=[{
"type": "web\_search\_preview",
"search\_context\_size": "low",
}],
input="What movie won best picture in 2025?",
)
print(response.output\_text)
```
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "o4-mini",
tools: [{
type: "web\_search\_preview",
search\_context\_size: "low",
}],
input: "What movie won best picture in 2025?",
});
console.log(response.output\_text);
```
```bash
curl "https://api.openai.com/v1/responses" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "o4-mini",
"tools": [{
"type": "web\_search\_preview",
"search\_context\_size": "low"
}],
"input": "What movie won best picture in 2025?"
}'
```
Usage notes
-----------
||
|ResponsesChat CompletionsAssistants|Same as tiered rate limits for underlying model used with the tool.|PricingZDR and data residency|

#### Limitations
\* Web search is currently not supported in the [`gpt-4.1-nano`](/docs/models/gpt-4.1-nano) model.
\* The [`gpt-4o-search-preview`](/docs/models/gpt-4o-search-preview) and [`gpt-4o-mini-search-preview`](/docs/models/gpt-4o-mini-search-preview) models used in Chat Completions only support a subset of API parameters - view their model data pages for specific information on rate limits and feature support.
\* When used as a tool in the [Responses API](/docs/api-reference/responses), web search has the same tiered rate limits as the models above.
\* Web search is limited to a context window size of 128000 (even with [`gpt-4.1`](/docs/models/gpt-4.1) and [`gpt-4.1-mini`](/docs/models/gpt-4.1-mini) models).
\* [Refer to this guide](/docs/guides/your-data) for data handling, residency, and retention information.
Was this page useful?


## Imported snippet – 2025-07-03 11:29:44

File search
===========
Allow models to search your files for relevant information before generating a response.
File search is a tool available in the [Responses API](/docs/api-reference/responses). It enables models to retrieve information in a knowledge base of previously uploaded files through semantic and keyword search. By creating vector stores and uploading files to them, you can augment the models' inherent knowledge by giving them access to these knowledge bases or `vector\_stores`.
To learn more about how vector stores and semantic search work, refer to our [retrieval guide](/docs/guides/retrieval).
This is a hosted tool managed by OpenAI, meaning you don't have to implement code on your end to handle its execution. When the model decides to use it, it will automatically call the tool, retrieve information from your files, and return an output.
How to use
----------
Prior to using file search with the Responses API, you need to have set up a knowledge base in a vector store and uploaded files to it.
Create a vector store and upload a file
Follow these steps to create a vector store and upload a file to it. You can use [this example file](https://cdn.openai.com/API/docs/deep\_research\_blog.pdf) or upload your own.

#### Upload the file to the File API
Upload a file
```python
import requests
from io import BytesIO
from openai import OpenAI
client = OpenAI()
def create\_file(client, file\_path):
if file\_path.startswith("http://") or file\_path.startswith("https://"):

# Download the file content from the URL
response = requests.get(file\_path)
file\_content = BytesIO(response.content)
file\_name = file\_path.split("/")[-1]
file\_tuple = (file\_name, file\_content)
result = client.files.create(
file=file\_tuple,
purpose="assistants"
)
else:

# Handle local file path
with open(file\_path, "rb") as file\_content:
result = client.files.create(
file=file\_content,
purpose="assistants"
)
print(result.id)
return result.id

# Replace with your own file path or URL
file\_id = create\_file(client, "https://cdn.openai.com/API/docs/deep\_research\_blog.pdf")
```
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
async function createFile(filePath) {
let result;
if (filePath.startsWith("http://") || filePath.startsWith("https://")) {
// Download the file content from the URL
const res = await fetch(filePath);
const buffer = await res.arrayBuffer();
const urlParts = filePath.split("/");
const fileName = urlParts[urlParts.length - 1];
const file = new File([buffer], fileName);
result = await openai.files.create({
file: file,
purpose: "assistants",
});
} else {
// Handle local file path
const fileContent = fs.createReadStream(filePath);
result = await openai.files.create({
file: fileContent,
purpose: "assistants",
});
}
return result.id;
}
// Replace with your own file path or URL
const fileId = await createFile(
"https://cdn.openai.com/API/docs/deep\_research\_blog.pdf"
);
console.log(fileId);
```

#### Create a vector store
Create a vector store
```python
vector\_store = client.vector\_stores.create(
name="knowledge\_base"
)
print(vector\_store.id)
```
```javascript
const vectorStore = await openai.vectorStores.create({
name: "knowledge\_base",
});
console.log(vectorStore.id);
```

#### Add the file to the vector store
Add a file to a vector store
```python
client.vector\_stores.files.create(
vector\_store\_id=vector\_store.id,
file\_id=file\_id
)
print(result)
```
```javascript
await openai.vectorStores.files.create(
vectorStore.id,
{
file\_id: fileId,
}
});
```

#### Check status
Run this code until the file is ready to be used (i.e., when the status is `completed`).
Check status
```python
result = client.vector\_stores.files.list(
vector\_store\_id=vector\_store.id
)
print(result)
```
```javascript
const result = await openai.vectorStores.files.list({
vector\_store\_id: vectorStore.id,
});
console.log(result);
```
Once your knowledge base is set up, you can include the `file\_search` tool in the list of tools available to the model, along with the list of vector stores in which to search.
File search tool
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4o-mini",
input="What is deep research by OpenAI?",
tools=[{
"type": "file\_search",
"vector\_store\_ids": [""]
}]
)
print(response)
```
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4o-mini",
input: "What is deep research by OpenAI?",
tools: [{
type: "file\_search",
vector\_store\_ids: [""],
}],
});
console.log(response);
```
When this tool is called by the model, you will receive a response with multiple outputs:
1. A `file\_search\_call` output item, which contains the id of the file search call.
2. A `message` output item, which contains the response from the model, along with the file citations.
File search response
```json
{
"output": [
{
"type": "file\_search\_call",
"id": "fs\_67c09ccea8c48191ade9367e3ba71515",
"status": "completed",
"queries": ["What is deep research?"],
"search\_results": null
},
{
"id": "msg\_67c09cd3091c819185af2be5d13d87de",
"type": "message",
"role": "assistant",
"content": [
{
"type": "output\_text",
"text": "Deep research is a sophisticated capability that allows for extensive inquiry and synthesis of information across various domains. It is designed to conduct multi-step research tasks, gather data from multiple online sources, and provide comprehensive reports similar to what a research analyst would produce. This functionality is particularly useful in fields requiring detailed and accurate information...",
"annotations": [
{
"type": "file\_citation",
"index": 992,
"file\_id": "file-2dtbBZdjtDKS8eqWxqbgDi",
"filename": "deep\_research\_blog.pdf"
},
{
"type": "file\_citation",
"index": 992,
"file\_id": "file-2dtbBZdjtDKS8eqWxqbgDi",
"filename": "deep\_research\_blog.pdf"
},
{
"type": "file\_citation",
"index": 1176,
"file\_id": "file-2dtbBZdjtDKS8eqWxqbgDi",
"filename": "deep\_research\_blog.pdf"
},
{
"type": "file\_citation",
"index": 1176,
"file\_id": "file-2dtbBZdjtDKS8eqWxqbgDi",
"filename": "deep\_research\_blog.pdf"
}
]
}
]
}
]
}
```
Retrieval customization
-----------------------

### Limiting the number of results
Using the file search tool with the Responses API, you can customize the number of results you want to retrieve from the vector stores. This can help reduce both token usage and latency, but may come at the cost of reduced answer quality.
Limit the number of results
```python
response = client.responses.create(
model="gpt-4o-mini",
input="What is deep research by OpenAI?",
tools=[{
"type": "file\_search",
"vector\_store\_ids": [""],
"max\_num\_results": 2
}]
)
print(response)
```
```javascript
const response = await openai.responses.create({
model: "gpt-4o-mini",
input: "What is deep research by OpenAI?",
tools: [{
type: "file\_search",
vector\_store\_ids: [""],
max\_num\_results: 2,
}],
});
console.log(response);
```

### Include search results in the response
While you can see annotations (references to files) in the output text, the file search call will not return search results by default.
To include search results in the response, you can use the `include` parameter when creating the response.
Include search results
```python
response = client.responses.create(
model="gpt-4o-mini",
input="What is deep research by OpenAI?",
tools=[{
"type": "file\_search",
"vector\_store\_ids": [""]
}],
include=["file\_search\_call.results"]
)
print(response)
```
```javascript
const response = await openai.responses.create({
model: "gpt-4o-mini",
input: "What is deep research by OpenAI?",
tools: [{
type: "file\_search",
vector\_store\_ids: [""],
}],
include: ["file\_search\_call.results"],
});
console.log(response);
```

### Metadata filtering
You can filter the search results based on the metadata of the files. For more details, refer to our [retrieval guide](/docs/guides/retrieval), which covers:
\* How to [set attributes on vector store files](/docs/guides/retrieval#attributes)
\* How to [define filters](/docs/guides/retrieval#attribute-filtering)
Metadata filtering
```python
response = client.responses.create(
model="gpt-4o-mini",
input="What is deep research by OpenAI?",
tools=[{
"type": "file\_search",
"vector\_store\_ids": [""],
"filters": {
"type": "eq",
"key": "type",
"value": "blog"
}
}]
)
print(response)
```
```javascript
const response = await openai.responses.create({
model: "gpt-4o-mini",
input: "What is deep research by OpenAI?",
tools: [{
type: "file\_search",
vector\_store\_ids: [""],
filters: {
type: "eq",
key: "type",
value: "blog"
}
}]
});
console.log(response);
```
Supported files
---------------
\_For `text/` MIME types, the encoding must be one of `utf-8`, `utf-16`, or `ascii`.\_
|File format|MIME type|
|---|---|
|.c|text/x-c|
|.cpp|text/x-c++|
|.cs|text/x-csharp|
|.css|text/css|
|.doc|application/msword|
|.docx|application/vnd.openxmlformats-officedocument.wordprocessingml.document|
|.go|text/x-golang|
|.html|text/html|
|.java|text/x-java|
|.js|text/javascript|
|.json|application/json|
|.md|text/markdown|
|.pdf|application/pdf|
|.php|text/x-php|
|.pptx|application/vnd.openxmlformats-officedocument.presentationml.presentation|
|.py|text/x-python|
|.py|text/x-script.python|
|.rb|text/x-ruby|
|.sh|application/x-sh|
|.tex|text/x-tex|
|.ts|application/typescript|
|.txt|text/plain|
Usage notes
-----------
||
|ResponsesChat CompletionsAssistants|Tier 1100 RPMTier 2 and 3500 RPMTier 4 and 51000 RPM|PricingZDR and data residency|
Was this page useful?


## Imported snippet – 2025-07-03 11:29:47

Image generation
================
Allow models to generate or edit images.
The image generation tool allows you to generate images using a text prompt, and optionally image inputs. It leverages the [GPT Image model](/docs/models/gpt-image-1), and automatically optimizes text inputs for improved performance.
To learn more about image generation, refer to our dedicated [image generation guide](/docs/guides/image-generation?image-generation-model=gpt-image-1&api=responses).
Usage
-----
When you include the `image\_generation` tool in your request, the model can decide when and how to generate images as part of the conversation, using your prompt and any provided image inputs.
The `image\_generation\_call` tool call result will include a base64-encoded image.
Generate an image
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input: "Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools: [{type: "image\_generation"}],
});
// Save the image to a file
const imageData = response.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("otter.png", Buffer.from(imageBase64, "base64"));
}
```
```python
from openai import OpenAI
import base64
client = OpenAI()
response = client.responses.create(
model="gpt-4.1-mini",
input="Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools=[{"type": "image\_generation"}],
)

# Save the image to a file
image\_data = [
output.result
for output in response.output
if output.type == "image\_generation\_call"
]
if image\_data:
image\_base64 = image\_data[0]
with open("otter.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```
You can [provide input images](/docs/guides/image-generation?image-generation-model=gpt-image-1#edit-images) using file IDs or base64 data.
To force the image generation tool call, you can set the parameter `tool\_choice` to `{"type": "image\_generation"}`.

### Tool options
You can configure the following output options as parameters for the [image generation tool](/docs/api-reference/responses/create#responses-create-tools):
\* Size: Image dimensions (e.g., 1024x1024, 1024x1536)
\* Quality: Rendering quality (e.g. low, medium, high)
\* Format: File output format
\* Compression: Compression level (0-100%) for JPEG and WebP formats
\* Background: Transparent or opaque
`size`, `quality`, and `background` support the `auto` option, where the model will automatically select the best option based on the prompt.
For more details on available options, refer to the [image generation guide](/docs/guides/image-generation#customize-image-output).

### Revised prompt
When using the image generation tool, the mainline model (e.g. `gpt-4.1`) will automatically revise your prompt for improved performance.
You can access the revised prompt in the `revised\_prompt` field of the image generation call:
```json
{
"id": "ig\_123",
"type": "image\_generation\_call",
"status": "completed",
"revised\_prompt": "A gray tabby cat hugging an otter. The otter is wearing an orange scarf. Both animals are cute and friendly, depicted in a warm, heartwarming style.",
"result": "..."
}
```

### Prompting tips
Image generation works best when you use terms like "draw" or "edit" in your prompt.
For example, if you want to combine images, instead of saying "combine" or "merge", you can say something like "edit the first image by adding this element from the second image".
Multi-turn editing
------------------
You can iteratively edit images by referencing previous response or image IDs. This allows you to refine images across multiple turns in a conversation.
Using previous response ID
Multi-turn image generation
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input:
"Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools: [{ type: "image\_generation" }],
});
const imageData = response.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("cat\_and\_otter.png", Buffer.from(imageBase64, "base64"));
}
// Follow up
const response\_fwup = await openai.responses.create({
model: "gpt-4.1-mini",
previous\_response\_id: response.id,
input: "Now make it look realistic",
tools: [{ type: "image\_generation" }],
});
const imageData\_fwup = response\_fwup.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData\_fwup.length > 0) {
const imageBase64 = imageData\_fwup[0];
const fs = await import("fs");
fs.writeFileSync(
"cat\_and\_otter\_realistic.png",
Buffer.from(imageBase64, "base64")
);
}
```
```python
from openai import OpenAI
import base64
client = OpenAI()
response = client.responses.create(
model="gpt-4.1-mini",
input="Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools=[{"type": "image\_generation"}],
)
image\_data = [
output.result
for output in response.output
if output.type == "image\_generation\_call"
]
if image\_data:
image\_base64 = image\_data[0]
with open("cat\_and\_otter.png", "wb") as f:
f.write(base64.b64decode(image\_base64))

# Follow up
response\_fwup = client.responses.create(
model="gpt-4.1-mini",
previous\_response\_id=response.id,
input="Now make it look realistic",
tools=[{"type": "image\_generation"}],
)
image\_data\_fwup = [
output.result
for output in response\_fwup.output
if output.type == "image\_generation\_call"
]
if image\_data\_fwup:
image\_base64 = image\_data\_fwup[0]
with open("cat\_and\_otter\_realistic.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```
Using image ID
Multi-turn image generation
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input:
"Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools: [{ type: "image\_generation" }],
});
const imageGenerationCalls = response.output.filter(
(output) => output.type === "image\_generation\_call"
);
const imageData = imageGenerationCalls.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("cat\_and\_otter.png", Buffer.from(imageBase64, "base64"));
}
// Follow up
const response\_fwup = await openai.responses.create({
model: "gpt-4.1-mini",
input: [
{
role: "user",
content: [{ type: "input\_text", text: "Now make it look realistic" }],
},
{
type: "image\_generation\_call",
id: imageGenerationCalls[0].id,
},
],
tools: [{ type: "image\_generation" }],
});
const imageData\_fwup = response\_fwup.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData\_fwup.length > 0) {
const imageBase64 = imageData\_fwup[0];
const fs = await import("fs");
fs.writeFileSync(
"cat\_and\_otter\_realistic.png",
Buffer.from(imageBase64, "base64")
);
}
```
```python
import openai
import base64
response = openai.responses.create(
model="gpt-4.1-mini",
input="Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools=[{"type": "image\_generation"}],
)
image\_generation\_calls = [
output
for output in response.output
if output.type == "image\_generation\_call"
]
image\_data = [output.result for output in image\_generation\_calls]
if image\_data:
image\_base64 = image\_data[0]
with open("cat\_and\_otter.png", "wb") as f:
f.write(base64.b64decode(image\_base64))

# Follow up
response\_fwup = openai.responses.create(
model="gpt-4.1-mini",
input=[
{
"role": "user",
"content": [{"type": "input\_text", "text": "Now make it look realistic"}],
},
{
"type": "image\_generation\_call",
"id": image\_generation\_calls[0].id,
},
],
tools=[{"type": "image\_generation"}],
)
image\_data\_fwup = [
output.result
for output in response\_fwup.output
if output.type == "image\_generation\_call"
]
if image\_data\_fwup:
image\_base64 = image\_data\_fwup[0]
with open("cat\_and\_otter\_realistic.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```
Streaming
---------
The image generation tool supports streaming partial images as the final result is being generated. This provides faster visual feedback for users and improves perceived latency.
You can set the number of partial images (1-3) with the `partial\_images` parameter.
Stream an image
```javascript
import OpenAI from "openai";
import fs from "fs";
const openai = new OpenAI();
const stream = await openai.responses.create({
model: "gpt-4.1",
input:
"Draw a gorgeous image of a river made of white owl feathers, snaking its way through a serene winter landscape",
stream: true,
tools: [{ type: "image\_generation", partial\_images: 2 }],
});
for await (const event of stream) {
if (event.type === "response.image\_generation\_call.partial\_image") {
const idx = event.partial\_image\_index;
const imageBase64 = event.partial\_image\_b64;
const imageBuffer = Buffer.from(imageBase64, "base64");
fs.writeFileSync(`river${idx}.png`, imageBuffer);
}
}
```
```python
from openai import OpenAI
import base64
client = OpenAI()
stream = client.responses.create(
model="gpt-4.1",
input="Draw a gorgeous image of a river made of white owl feathers, snaking its way through a serene winter landscape",
stream=True,
tools=[{"type": "image\_generation", "partial\_images": 2}],
)
for event in stream:
if event.type == "response.image\_generation\_call.partial\_image":
idx = event.partial\_image\_index
image\_base64 = event.partial\_image\_b64
image\_bytes = base64.b64decode(image\_base64)
with open(f"river{idx}.png", "wb") as f:
f.write(image\_bytes)
```
Supported models
----------------
The image generation tool is supported for the following models:
\* `gpt-4o`
\* `gpt-4o-mini`
\* `gpt-4.1`
\* `gpt-4.1-mini`
\* `gpt-4.1-nano`
\* `o3`
The model used for the image generation process is always `gpt-image-1`, but these models can be used as the mainline model in the Responses API as they can reliably call the image generation tool when needed.
Was this page useful?


## Imported snippet – 2025-07-03 11:29:49

Code Interpreter
================
Allow models to write and run Python to solve problems.
The Code Interpreter tool allows models to write and run Python code in a sandboxed environment to solve complex problems in domains like data analysis, coding, and math. Use it for:
\* Processing files with diverse data and formatting
\* Generating files with data and images of graphs
\* Writing and running code iteratively to solve problems—for example, a model that writes code that fails to run can keep rewriting and running that code until it succeeds
\* Boosting visual intelligence in our latest reasoning models (like [o3](/docs/models/o3) and [o4-mini](/docs/models/o4-mini)). The model can use this tool to crop, zoom, rotate, and otherwise process and transform images.
Here's an example of calling the [Responses API](/docs/api-reference/responses) with a tool call to Code Interpreter:
Use the Responses API with Code Interpreter
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"tools": [{
"type": "code\_interpreter",
"container": { "type": "auto" }
}],
"instructions": "You are a personal math tutor. When asked a math question, write and run code using the python tool to answer the question.",
"input": "I need to solve the equation 3x + 11 = 14. Can you help me?"
}'
```
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const instructions = `
You are a personal math tutor. When asked a math question,
write and run code using the python tool to answer the question.
`;
const resp = await client.responses.create({
model: "gpt-4.1",
tools: [
{
type: "code\_interpreter",
container: { type: "auto" },
},
],
instructions,
input: "I need to solve the equation 3x + 11 = 14. Can you help me?",
});
console.log(JSON.stringify(resp.output, null, 2));
```
```python
from openai import OpenAI
client = OpenAI()
instructions = """
You are a personal math tutor. When asked a math question,
write and run code using the python tool to answer the question.
"""
resp = client.responses.create(
model="gpt-4.1",
tools=[
{
"type": "code\_interpreter",
"container": {"type": "auto"}
}
],
instructions=instructions,
input="I need to solve the equation 3x + 11 = 14. Can you help me?",
)
print(resp.output)
```
While we call this tool Code Interpreter, the model knows it as the "python tool". Models usually understand prompts that refer to the code interpreter tool, however, the most explicit way to invoke this tool is to ask for "the python tool" in your prompts.
Containers
----------
The Code Interpreter tool requires a [container object](/docs/api-reference/containers/object). A container is a fully sandboxed virtual machine that the model can run Python code in. This container can contain files that you upload, or that it generates.
There are two ways to create containers:
1. Auto mode: as seen in the example above, you can do this by passing the `"container": { "type": "auto", files: ["file-1", "file-2"] }` property in the tool configuration while creating a new Response object. This automatically creates a new container, or reuses an active container that was used by a previous `code\_interpreter\_call` item in the model's context. Look for the `code\_interpreter\_call` item in the output of this API request to find the `container\_id` that was generated or used.
2. Explicit mode: here, you explicitly [create a container](/docs/api-reference/containers/createContainers) using the `v1/containers` endpoint, and assign its `id` as the `container` value in the tool configuration in the Response object. For example:
Use explicit container creation
```bash
curl https://api.openai.com/v1/containers \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"name": "My Container"
}'

# Use the returned container id in the next call:
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4.1",
"tools": [{
"type": "code\_interpreter",
"container": "cntr\_abc123"
}],
"tool\_choice": "required",
"input": "use the python tool to calculate what is 4 \* 3.82. and then find its square root and then find the square root of that result"
}'
```
```python
from openai import OpenAI
client = OpenAI()
container = client.containers.create(name="test-container")
response = client.responses.create(
model="gpt-4.1",
tools=[{
"type": "code\_interpreter",
"container": container.id
}],
tool\_choice="required",
input="use the python tool to calculate what is 4 \* 3.82. and then find its square root and then find the square root of that result"
)
print(response.output\_text)
```
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const container = await client.containers.create({ name: "test-container" });
const resp = await client.responses.create({
model: "gpt-4.1",
tools: [
{
type: "code\_interpreter",
container: container.id
}
],
tool\_choice: "required",
input: "use the python tool to calculate what is 4 \* 3.82. and then find its square root and then find the square root of that result"
});
console.log(resp.output\_text);
```
Note that containers created with the auto mode are also accessible using the [`/v1/containers`](/docs/api-reference/containers) endpoint.

### Expiration
We highly recommend you treat containers as ephemeral and store all data related to the use of this tool on your own systems. Expiration details:
\* A container expires if it is not used for 20 minutes. When this happens, using the container in `v1/responses` will fail. You'll still be able to see a snapshot of the container's metadata at its expiry, but all data associated with the container will be discarded from our systems and not recoverable. You should download any files you may need from the container while it is active.
\* You can't move a container from an expired state to an active one. Instead, create a new container and upload files again. Note that any state in the old container's memory (like python objects) will be lost.
\* Any container operation, like retrieving the container, or adding or deleting files from the container, will automatically refresh the container's `last\_active\_at` time.
Work with files
---------------
When running Code Interpreter, the model can create its own files. For example, if you ask it to construct a plot, or create a CSV, it creates these images directly on your container. When it does so, it cites these files in the `annotations` of its next message. Here's an example:
```json
{
"id": "msg\_682d514e268c8191a89c38ea318446200f2610a7ec781a4f",
"content": [
{
"annotations": [
{
"file\_id": "cfile\_682d514b2e00819184b9b07e13557f82",
"index": null,
"type": "container\_file\_citation",
"container\_id": "cntr\_682d513bb0c48191b10bd4f8b0b3312200e64562acc2e0af",
"end\_index": 0,
"filename": "cfile\_682d514b2e00819184b9b07e13557f82.png",
"start\_index": 0
}
],
"text": "Here is the histogram of the RGB channels for the uploaded image. Each curve represents the distribution of pixel intensities for the red, green, and blue channels. Peaks toward the high end of the intensity scale (right-hand side) suggest a lot of brightness and strong warm tones, matching the orange and light background in the image. If you want a different style of histogram (e.g., overall intensity, or quantized color groups), let me know!",
"type": "output\_text",
"logprobs": []
}
],
"role": "assistant",
"status": "completed",
"type": "message"
}
```
You can download these constructed files by calling the [get container file content](/docs/api-reference/container-files/retrieveContainerFileContent) method.
Any [files in the model input](/docs/guides/pdf-files) get automatically uploaded to the container. You do not have to explicitly upload it to the container.

### Uploading and downloading files
Add new files to your container using [Create container file](/docs/api-reference/container-files/createContainerFile). This endpoint accepts either a multipart upload or a JSON body with a `file\_id`. List existing container files with [List container files](/docs/api-reference/container-files/listContainerFiles) and download bytes from [Retrieve container file content](/docs/api-reference/container-files/retrieveContainerFileContent).

### Dealing with citations
Files and images generated by the model are returned as annotations on the assistant's message. `container\_file\_citation` annotations point to files created in the container. They include the `container\_id`, `file\_id`, and `filename`. You can parse these annotations to surface download links or otherwise process the files.

### Supported files
|File format|MIME type|
|---|---|
|.c|text/x-c|
|.cs|text/x-csharp|
|.cpp|text/x-c++|
|.csv|text/csv|
|.doc|application/msword|
|.docx|application/vnd.openxmlformats-officedocument.wordprocessingml.document|
|.html|text/html|
|.java|text/x-java|
|.json|application/json|
|.md|text/markdown|
|.pdf|application/pdf|
|.php|text/x-php|
|.pptx|application/vnd.openxmlformats-officedocument.presentationml.presentation|
|.py|text/x-python|
|.py|text/x-script.python|
|.rb|text/x-ruby|
|.tex|text/x-tex|
|.txt|text/plain|
|.css|text/css|
|.js|text/javascript|
|.sh|application/x-sh|
|.ts|application/typescript|
|.csv|application/csv|
|.jpeg|image/jpeg|
|.jpg|image/jpeg|
|.gif|image/gif|
|.pkl|application/octet-stream|
|.png|image/png|
|.tar|application/x-tar|
|.xlsx|application/vnd.openxmlformats-officedocument.spreadsheetml.sheet|
|.xml|application/xml or "text/xml"|
|.zip|application/zip|
Usage notes
-----------
||
|ResponsesChat CompletionsAssistants|100 RPM per org|PricingZDR and data residency|
Was this page useful?


## Imported snippet – 2025-07-03 11:29:51

Computer use
============
Build a computer-using agent that can perform tasks on your behalf.
\*\*Computer use\*\* is a practical application of our [Computer-Using Agent](https://openai.com/index/computer-using-agent/) (CUA) model, `computer-use-preview`, which combines the vision capabilities of [GPT-4o](/docs/models/gpt-4o) with advanced reasoning to simulate controlling computer interfaces and performing tasks.
Computer use is available through the [Responses API](/docs/guides/responses-vs-chat-completions). It is not available on Chat Completions.
Computer use is in beta. Because the model is still in preview and may be susceptible to exploits and inadvertent mistakes, we discourage trusting it in fully authenticated environments or for high-stakes tasks. See [limitations](#limitations) and [risk and safety best practices](#risks-and-safety) below. You must use the Computer Use tool in line with OpenAI's [Usage Policy](https://openai.com/policies/usage-policies/) and [Business Terms](https://openai.com/policies/business-terms/).
How it works
------------
The computer use tool operates in a continuous loop. It sends computer actions, like `click(x,y)` or `type(text)`, which your code executes on a computer or browser environment and then returns screenshots of the outcomes back to the model.
In this way, your code simulates the actions of a human using a computer interface, while our model uses the screenshots to understand the state of the environment and suggest next actions.
This loop lets you automate many tasks requiring clicking, typing, scrolling, and more. For example, booking a flight, searching for a product, or filling out a form.
Refer to the [integration section](#integration) below for more details on how to integrate the computer use tool, or check out our sample app repository to set up an environment and try example integrations.
[
CUA sample app
Examples of how to integrate the computer use tool in different environments
](https://github.com/openai/openai-cua-sample-app)
Setting up your environment
---------------------------
Before integrating the tool, prepare an environment that can capture screenshots and execute the recommended actions. We recommend using a sandboxed environment for safety reasons.
In this guide, we'll show you examples using either a local browsing environment or a local virtual machine, but there are more example computer environments in our sample app.
Set up a local browsing environment
If you want to try out the computer use tool with minimal setup, you can use a browser automation framework such as [Playwright](https://playwright.dev/) or [Selenium](https://www.selenium.dev/).
Running a browser automation framework locally can pose security risks. We recommend the following setup to mitigate them:
\* Use a sandboxed environment
\* Set `env` to an empty object to avoid exposing host environment variables to the browser
\* Set flags to disable extensions and the file system

#### Start a browser instance
You can start browser instances using your preferred language by installing the corresponding SDK.
For example, to start a Playwright browser instance, install the Playwright SDK:
\* Python: `pip install playwright`
\* JavaScript: `npm i playwright` then `npx playwright install`
Then run the following code:
Start a browser instance
```javascript
import { chromium } from "playwright";
const browser = await chromium.launch({
headless: false,
chromiumSandbox: true,
env: {},
args: ["--disable-extensions", "--disable-file-system"],
});
const page = await browser.newPage();
await page.setViewportSize({ width: 1024, height: 768 });
await page.goto("https://bing.com");
await page.waitForTimeout(10000);
browser.close();
```
```python
from playwright.sync\_api import sync\_playwright
with sync\_playwright() as p:
browser = p.chromium.launch(
headless=False,
chromium\_sandbox=True,
env={},
args=[
"--disable-extensions",
"--disable-file-system"
]
)
page = browser.new\_page()
page.set\_viewport\_size({"width": 1024, "height": 768})
page.goto("https://bing.com")
page.wait\_for\_timeout(10000)
```
Set up a local virtual machine
If you'd like to use the computer use tool beyond just a browser interface, you can set up a local virtual machine instead, using a tool like [Docker](https://www.docker.com/). You can then connect to this local machine to execute computer use actions.

#### Start Docker
If you don't have Docker installed, you can install it from [their website](https://www.docker.com). Once installed, make sure Docker is running on your machine.

#### Create a Dockerfile
Create a Dockerfile to define the configuration of your virtual machine.
Here is an example Dockerfile that starts an Ubuntu virtual machine with a VNC server:
Dockerfile
```json
FROM ubuntu:22.04
ENV DEBIAN\_FRONTEND=noninteractive

# 1) Install Xfce, x11vnc, Xvfb, xdotool, etc., but remove any screen lockers or power managers
RUN apt-get update && apt-get install -y xfce4 xfce4-goodies x11vnc xvfb xdotool imagemagick x11-apps sudo software-properties-common imagemagick && apt-get remove -y light-locker xfce4-screensaver xfce4-power-manager || true && apt-get clean && rm -rf /var/lib/apt/lists/\*

# 2) Add the mozillateam PPA and install Firefox ESR
RUN add-apt-repository ppa:mozillateam/ppa && apt-get update && apt-get install -y --no-install-recommends firefox-esr && update-alternatives --set x-www-browser /usr/bin/firefox-esr && apt-get clean && rm -rf /var/lib/apt/lists/\*

# 3) Create non-root user
RUN useradd -ms /bin/bash myuser && echo "myuser ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers
USER myuser
WORKDIR /home/myuser

# 4) Set x11vnc password ("secret")
RUN x11vnc -storepasswd secret /home/myuser/.vncpass

# 5) Expose port 5900 and run Xvfb, x11vnc, Xfce (no login manager)
EXPOSE 5900
CMD ["/bin/sh", "-c", " Xvfb :99 -screen 0 1280x800x24 >/dev/null 2>&1 & x11vnc -display :99 -forever -rfbauth /home/myuser/.vncpass -listen 0.0.0.0 -rfbport 5900 >/dev/null 2>&1 & export DISPLAY=:99 && startxfce4 >/dev/null 2>&1 & sleep 2 && echo 'Container running!' && tail -f /dev/null "]
```

#### Build the Docker image
Build the Docker image by running the following command in the directory containing the Dockerfile:
```bash
docker build -t cua-image .
```

#### Run the Docker container locally
Start the Docker container with the following command:
```bash
docker run --rm -it --name cua-image -p 5900:5900 -e DISPLAY=:99 cua-image
```

#### Execute commands on the container
Now that your container is running, you can execute commands on it. For example, we can define a helper function to execute commands on the container that will be used in the next steps.
Execute commands on the container
```python
def docker\_exec(cmd: str, container\_name: str, decode=True) -> str:
safe\_cmd = cmd.replace('"', '\"')
docker\_cmd = f'docker exec {container\_name} sh -c "{safe\_cmd}"'
output = subprocess.check\_output(docker\_cmd, shell=True)
if decode:
return output.decode("utf-8", errors="ignore")
return output
class VM:
def \_\_init\_\_(self, display, container\_name):
self.display = display
self.container\_name = container\_name
vm = VM(display=":99", container\_name="cua-image")
```
```javascript
async function dockerExec(cmd, containerName, decode = true) {
const safeCmd = cmd.replace(/"/g, '\"');
const dockerCmd = `docker exec ${containerName} sh -c "${safeCmd}"`;
const output = await execAsync(dockerCmd, {
encoding: decode ? "utf8" : "buffer",
});
const result = output && output.stdout ? output.stdout : output;
if (decode) {
return result.toString("utf-8");
}
return result;
}
const vm = {
display: ":99",
containerName: "cua-image",
};
```
Integrating the CUA loop
------------------------
These are the high-level steps you need to follow to integrate the computer use tool in your application:
1. \*\*Send a request to the model\*\*: Include the `computer` tool as part of the available tools, specifying the display size and environment. You can also include in the first request a screenshot of the initial state of the environment.
2. \*\*Receive a response from the model\*\*: Check if the response has any `computer\_call` items. This tool call contains a suggested action to take to progress towards the specified goal. These actions could be clicking at a given position, typing in text, scrolling, or even waiting.
3. \*\*Execute the requested action\*\*: Execute through code the corresponding action on your computer or browser environment.
4. \*\*Capture the updated state\*\*: After executing the action, capture the updated state of the environment as a screenshot.
5. \*\*Repeat\*\*: Send a new request with the updated state as a `computer\_call\_output`, and repeat this loop until the model stops requesting actions or you decide to stop.
![Computer use diagram](https://cdn.openai.com/API/docs/images/cua\_diagram.png)

### 1\. Send a request to the model
Send a request to create a Response with the `computer-use-preview` model equipped with the `computer\_use\_preview` tool. This request should include details about your environment, along with an initial input prompt.
If you want to show a summary of the reasoning performed by the model, you can include the `summary` parameter in the request. This can be helpful if you want to debug or show what's happening behind the scenes in your interface. The summary can either be `concise` or `detailed`.
Optionally, you can include a screenshot of the initial state of the environment.
To be able to use the `computer\_use\_preview` tool, you need to set the `truncation` parameter to `"auto"` (by default, truncation is disabled).
Send a CUA request
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "computer-use-preview",
tools: [
{
type: "computer\_use\_preview",
display\_width: 1024,
display\_height: 768,
environment: "browser", // other possible values: "mac", "windows", "ubuntu"
},
],
input: [
{
role: "user",
content: [
{
type: "text",
text: "Check the latest OpenAI news on bing.com.",
},
// Optional: include a screenshot of the initial state of the environment
// {
// type: "input\_image",
// image\_url: `data:image/png;base64,${screenshot\_base64}`
// }
],
},
],
reasoning: {
summary: "concise",
},
truncation: "auto",
});
console.log(JSON.stringify(response.output, null, 2));
```
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="computer-use-preview",
tools=[{
"type": "computer\_use\_preview",
"display\_width": 1024,
"display\_height": 768,
"environment": "browser" # other possible values: "mac", "windows", "ubuntu"
}],
input=[
{
"role": "user",
"content": [
{
"type": "text",
"text": "Check the latest OpenAI news on bing.com."
}

# Optional: include a screenshot of the initial state of the environment

# {

# type: "input\_image",

# image\_url: f"data:image/png;base64,{screenshot\_base64}"

# }
]
}
],
reasoning={
"summary": "concise",
},
truncation="auto"
)
print(response.output)
```

### 2\. Receive a suggested action
The model returns an output that contains either a `computer\_call` item, just text, or other tool calls, depending on the state of the conversation.
Examples of `computer\_call` items are a click, a scroll, a key press, or any other event defined in the [API reference](/docs/api-reference/computer-use). In our example, the item is a click action:
CUA suggested action
```json
"output": [
{
"type": "reasoning",
"id": "rs\_67cc...",
"summary": [
{
"type": "summary\_text",
"text": "Clicking on the browser address bar."
}
]
},
{
"type": "computer\_call",
"id": "cu\_67cc...",
"call\_id": "call\_zw3...",
"action": {
"type": "click",
"button": "left",
"x": 156,
"y": 50
},
"pending\_safety\_checks": [],
"status": "completed"
}
]
```

#### Reasoning items
The model may return a `reasoning` item in the response output for some actions. If you don't use the `previous\_response\_id` parameter as shown in [Step 5](#5-repeat) and manage the inputs array on your end, make sure to include those reasoning items along with the computer calls when sending the next request to the CUA model–or the request will fail.
The reasoning items are only compatible with the same model that produced them (in this case, `computer-use-preview`). If you implement a flow where you use several models with the same conversation history, you should filter these reasoning items out of the inputs array you send to other models.

#### Safety checks
The model may return safety checks with the `pending\_safety\_check` parameter. Refer to the section on how to [acknowledge safety checks](#acknowledge-safety-checks) below for more details.

### 3\. Execute the action in your environment
Execute the corresponding actions on your computer or browser. How you map a computer call to actions through code depends on your environment. This code shows example implementations for the most common computer actions.
Playwright
Execute the action
```javascript
async function handleModelAction(page, action) {
// Given a computer action (e.g., click, double\_click, scroll, etc.),
// execute the corresponding operation on the Playwright page.
const actionType = action.type;
try {
switch (actionType) {
case "click": {
const { x, y, button = "left" } = action;
console.log(`Action: click at (${x}, ${y}) with button '${button}'`);
await page.mouse.click(x, y, { button });
break;
}
case "scroll": {
const { x, y, scrollX, scrollY } = action;
console.log(
`Action: scroll at (${x}, ${y}) with offsets (scrollX=${scrollX}, scrollY=${scrollY})`
);
await page.mouse.move(x, y);
await page.evaluate(`window.scrollBy(${scrollX}, ${scrollY})`);
break;
}
case "keypress": {
const { keys } = action;
for (const k of keys) {
console.log(`Action: keypress '${k}'`);
// A simple mapping for common keys; expand as needed.
if (k.includes("ENTER")) {
await page.keyboard.press("Enter");
} else if (k.includes("SPACE")) {
await page.keyboard.press(" ");
} else {
await page.keyboard.press(k);
}
}
break;
}
case "type": {
const { text } = action;
console.log(`Action: type text '${text}'`);
await page.keyboard.type(text);
break;
}
case "wait": {
console.log(`Action: wait`);
await page.waitForTimeout(2000);
break;
}
case "screenshot": {
// Nothing to do as screenshot is taken at each turn
console.log(`Action: screenshot`);
break;
}
// Handle other actions here
default:
console.log("Unrecognized action:", action);
}
} catch (e) {
console.error("Error handling action", action, ":", e);
}
}
```
```python
def handle\_model\_action(page, action):
"""
Given a computer action (e.g., click, double\_click, scroll, etc.),
execute the corresponding operation on the Playwright page.
"""
action\_type = action.type
try:
match action\_type:
case "click":
x, y = action.x, action.y
button = action.button
print(f"Action: click at ({x}, {y}) with button '{button}'")

# Not handling things like middle click, etc.
if button != "left" and button != "right":
button = "left"
page.mouse.click(x, y, button=button)
case "scroll":
x, y = action.x, action.y
scroll\_x, scroll\_y = action.scroll\_x, action.scroll\_y
print(f"Action: scroll at ({x}, {y}) with offsets (scroll\_x={scroll\_x}, scroll\_y={scroll\_y})")
page.mouse.move(x, y)
page.evaluate(f"window.scrollBy({scroll\_x}, {scroll\_y})")
case "keypress":
keys = action.keys
for k in keys:
print(f"Action: keypress '{k}'")

# A simple mapping for common keys; expand as needed.
if k.lower() == "enter":
page.keyboard.press("Enter")
elif k.lower() == "space":
page.keyboard.press(" ")
else:
page.keyboard.press(k)
case "type":
text = action.text
print(f"Action: type text: {text}")
page.keyboard.type(text)
case "wait":
print(f"Action: wait")
time.sleep(2)
case "screenshot":

# Nothing to do as screenshot is taken at each turn
print(f"Action: screenshot")

# Handle other actions here
case \_:
print(f"Unrecognized action: {action}")
except Exception as e:
print(f"Error handling action {action}: {e}")
```
Docker
Execute the action
```javascript
async function handleModelAction(vm, action) {
// Given a computer action (e.g., click, double\_click, scroll, etc.),
// execute the corresponding operation on the Docker environment.
const actionType = action.type;
try {
switch (actionType) {
case "click": {
const { x, y, button = "left" } = action;
const buttonMap = { left: 1, middle: 2, right: 3 };
const b = buttonMap[button] || 1;
console.log(`Action: click at (${x}, ${y}) with button '${button}'`);
await dockerExec(
`DISPLAY=${vm.display} xdotool mousemove ${x} ${y} click ${b}`,
vm.containerName
);
break;
}
case "scroll": {
const { x, y, scrollX, scrollY } = action;
console.log(
`Action: scroll at (${x}, ${y}) with offsets (scrollX=${scrollX}, scrollY=${scrollY})`
);
await dockerExec(
`DISPLAY=${vm.display} xdotool mousemove ${x} ${y}`,
vm.containerName
);
// For vertical scrolling, use button 4 for scroll up and button 5 for scroll down.
if (scrollY !== 0) {
const button = scrollY < 0 ? 4 : 5;
const clicks = Math.abs(scrollY);
for (let i = 0; i < clicks; i++) {
await dockerExec(
`DISPLAY=${vm.display} xdotool click ${button}`,
vm.containerName
);
}
}
break;
}
case "keypress": {
const { keys } = action;
for (const k of keys) {
console.log(`Action: keypress '${k}'`);
// A simple mapping for common keys; expand as needed.
if (k.includes("ENTER")) {
await dockerExec(
`DISPLAY=${vm.display} xdotool key 'Return'`,
vm.containerName
);
} else if (k.includes("SPACE")) {
await dockerExec(
`DISPLAY=${vm.display} xdotool key 'space'`,
vm.containerName
);
} else {
await dockerExec(
`DISPLAY=${vm.display} xdotool key '${k}'`,
vm.containerName
);
}
}
break;
}
case "type": {
const { text } = action;
console.log(`Action: type text '${text}'`);
await dockerExec(
`DISPLAY=${vm.display} xdotool type '${text}'`,
vm.containerName
);
break;
}
case "wait": {
console.log(`Action: wait`);
await new Promise((resolve) => setTimeout(resolve, 2000));
break;
}
case "screenshot": {
// Nothing to do as screenshot is taken at each turn
console.log(`Action: screenshot`);
break;
}
// Handle other actions here
default:
console.log("Unrecognized action:", action);
}
} catch (e) {
console.error("Error handling action", action, ":", e);
}
}
```
```python
def handle\_model\_action(vm, action):
"""
Given a computer action (e.g., click, double\_click, scroll, etc.),
execute the corresponding operation on the Docker environment.
"""
action\_type = action.type
try:
match action\_type:
case "click":
x, y = int(action.x), int(action.y)
button\_map = {"left": 1, "middle": 2, "right": 3}
b = button\_map.get(action.button, 1)
print(f"Action: click at ({x}, {y}) with button '{action.button}'")
docker\_exec(f"DISPLAY={vm.display} xdotool mousemove {x} {y} click {b}", vm.container\_name)
case "scroll":
x, y = int(action.x), int(action.y)
scroll\_x, scroll\_y = int(action.scroll\_x), int(action.scroll\_y)
print(f"Action: scroll at ({x}, {y}) with offsets (scroll\_x={scroll\_x}, scroll\_y={scroll\_y})")
docker\_exec(f"DISPLAY={vm.display} xdotool mousemove {x} {y}", vm.container\_name)

# For vertical scrolling, use button 4 (scroll up) or button 5 (scroll down)
if scroll\_y != 0:
button = 4 if scroll\_y < 0 else 5
clicks = abs(scroll\_y)
for \_ in range(clicks):
docker\_exec(f"DISPLAY={vm.display} xdotool click {button}", vm.container\_name)
case "keypress":
keys = action.keys
for k in keys:
print(f"Action: keypress '{k}'")

# A simple mapping for common keys; expand as needed.
if k.lower() == "enter":
docker\_exec(f"DISPLAY={vm.display} xdotool key 'Return'", vm.container\_name)
elif k.lower() == "space":
docker\_exec(f"DISPLAY={vm.display} xdotool key 'space'", vm.container\_name)
else:
docker\_exec(f"DISPLAY={vm.display} xdotool key '{k}'", vm.container\_name)
case "type":
text = action.text
print(f"Action: type text: {text}")
docker\_exec(f"DISPLAY={vm.display} xdotool type '{text}'", vm.container\_name)
case "wait":
print(f"Action: wait")
time.sleep(2)
case "screenshot":

# Nothing to do as screenshot is taken at each turn
print(f"Action: screenshot")

# Handle other actions here
case \_:
print(f"Unrecognized action: {action}")
except Exception as e:
print(f"Error handling action {action}: {e}")
```

### 4\. Capture the updated screenshot
After executing the action, capture the updated state of the environment as a screenshot, which also differs depending on your environment.
Playwright
Capture and send the updated screenshot
```javascript
async function getScreenshot(page) {
// Take a full-page screenshot using Playwright and return the image bytes.
return await page.screenshot();
}
```
```python
def get\_screenshot(page):
"""
Take a full-page screenshot using Playwright and return the image bytes.
"""
return page.screenshot()
```
Docker
Capture and send the updated screenshot
```javascript
async function getScreenshot(vm) {
// Take a screenshot, returning raw bytes.
const cmd = `export DISPLAY=${vm.display} && import -window root png:-`;
const screenshotBuffer = await dockerExec(cmd, vm.containerName, false);
return screenshotBuffer;
}
```
```python
def get\_screenshot(vm):
"""
Takes a screenshot, returning raw bytes.
"""
cmd = (
f"export DISPLAY={vm.display} && "
"import -window root png:-"
)
screenshot\_bytes = docker\_exec(cmd, vm.container\_name, decode=False)
return screenshot\_bytes
```

### 5\. Repeat
Once you have the screenshot, you can send it back to the model as a `computer\_call\_output` to get the next action. Repeat these steps as long as you get a `computer\_call` item in the response.
Repeat steps in a loop
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
async function computerUseLoop(instance, response) {
/\*\*
\* Run the loop that executes computer actions until no 'computer\_call' is found.
\*/
while (true) {
const computerCalls = response.output.filter(
(item) => item.type === "computer\_call"
);
if (computerCalls.length === 0) {
console.log("No computer call found. Output from model:");
response.output.forEach((item) => {
console.log(JSON.stringify(item, null, 2));
});
break; // Exit when no computer calls are issued.
}
// We expect at most one computer call per response.
const computerCall = computerCalls[0];
const lastCallId = computerCall.call\_id;
const action = computerCall.action;
// Execute the action (function defined in step 3)
handleModelAction(instance, action);
await new Promise((resolve) => setTimeout(resolve, 1000)); // Allow time for changes to take effect.
// Take a screenshot after the action (function defined in step 4)
const screenshotBytes = await getScreenshot(instance);
const screenshotBase64 = Buffer.from(screenshotBytes).toString("base64");
// Send the screenshot back as a computer\_call\_output
response = await openai.responses.create({
model: "computer-use-preview",
previous\_response\_id: response.id,
tools: [
{
type: "computer\_use\_preview",
display\_width: 1024,
display\_height: 768,
environment: "browser",
},
],
input: [
{
call\_id: lastCallId,
type: "computer\_call\_output",
output: {
type: "input\_image",
image\_url: `data:image/png;base64,${screenshotBase64}`,
},
},
],
truncation: "auto",
});
}
return response;
}
```
```python
import time
import base64
from openai import OpenAI
client = OpenAI()
def computer\_use\_loop(instance, response):
"""
Run the loop that executes computer actions until no 'computer\_call' is found.
"""
while True:
computer\_calls = [item for item in response.output if item.type == "computer\_call"]
if not computer\_calls:
print("No computer call found. Output from model:")
for item in response.output:
print(item)
break # Exit when no computer calls are issued.

# We expect at most one computer call per response.
computer\_call = computer\_calls[0]
last\_call\_id = computer\_call.call\_id
action = computer\_call.action

# Execute the action (function defined in step 3)
handle\_model\_action(instance, action)
time.sleep(1) # Allow time for changes to take effect.

# Take a screenshot after the action (function defined in step 4)
screenshot\_bytes = get\_screenshot(instance)
screenshot\_base64 = base64.b64encode(screenshot\_bytes).decode("utf-8")

# Send the screenshot back as a computer\_call\_output
response = client.responses.create(
model="computer-use-preview",
previous\_response\_id=response.id,
tools=[
{
"type": "computer\_use\_preview",
"display\_width": 1024,
"display\_height": 768,
"environment": "browser"
}
],
input=[
{
"call\_id": last\_call\_id,
"type": "computer\_call\_output",
"output": {
"type": "input\_image",
"image\_url": f"data:image/png;base64,{screenshot\_base64}"
}
}
],
truncation="auto"
)
return response
```

#### Handling conversation history
You can use the `previous\_response\_id` parameter to link the current request to the previous response. We recommend using this method if you don't want to manage the conversation history on your side.
If you do not want to use this parameter, you should make sure to include in your inputs array all the items returned in the response output of the previous request, including reasoning items if present.

### Acknowledge safety checks
We have implemented safety checks in the API to help protect against prompt injection and model mistakes. These checks include:
\* Malicious instruction detection: we evaluate the screenshot image and check if it contains adversarial content that may change the model's behavior.
\* Irrelevant domain detection: we evaluate the `current\_url` (if provided) and check if the current domain is considered relevant given the conversation history.
\* Sensitive domain detection: we check the `current\_url` (if provided) and raise a warning when we detect the user is on a sensitive domain.
If one or multiple of the above checks is triggered, a safety check is raised when the model returns the next `computer\_call`, with the `pending\_safety\_checks` parameter.
Pending safety checks
```json
"output": [
{
"type": "reasoning",
"id": "rs\_67cb...",
"summary": [
{
"type": "summary\_text",
"text": "Exploring 'File' menu option."
}
]
},
{
"type": "computer\_call",
"id": "cu\_67cb...",
"call\_id": "call\_nEJ...",
"action": {
"type": "click",
"button": "left",
"x": 135,
"y": 193
},
"pending\_safety\_checks": [
{
"id": "cu\_sc\_67cb...",
"code": "malicious\_instructions",
"message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
}
],
"status": "completed"
}
]
```
You need to pass the safety checks back as `acknowledged\_safety\_checks` in the next request in order to proceed. In all cases where `pending\_safety\_checks` are returned, actions should be handed over to the end user to confirm model behavior and accuracy.
\* `malicious\_instructions` and `irrelevant\_domain`: end users should review model actions and confirm that the model is behaving as intended.
\* `sensitive\_domain`: ensure an end user is actively monitoring the model actions on these sites. Exact implementation of this "watch mode" may vary by application, but a potential example could be collecting user impression data on the site to make sure there is active end user engagement with the application.
Acknowledge safety checks
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="computer-use-preview",
previous\_response\_id="",
tools=[{
"type": "computer\_use\_preview",
"display\_width": 1024,
"display\_height": 768,
"environment": "browser"
}],
input=[
{
"type": "computer\_call\_output",
"call\_id": "",
"acknowledged\_safety\_checks": [
{
"id": "",
"code": "malicious\_instructions",
"message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
}
],
"output": {
"type": "computer\_screenshot",
"image\_url": ""
}
}
],
truncation="auto"
)
```
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "computer-use-preview",
previous\_response\_id: "",
tools: [{
type: "computer\_use\_preview",
display\_width: 1024,
display\_height: 768,
environment: "browser"
}],
input: [
{
"type": "computer\_call\_output",
"call\_id": "",
"acknowledged\_safety\_checks": [
{
"id": "",
"code": "malicious\_instructions",
"message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
}
],
"output": {
"type": "computer\_screenshot",
"image\_url": ""
}
}
],
truncation: "auto",
});
```

### Final code
Putting it all together, the final code should include:
1. The initialization of the environment
2. A first request to the model with the `computer` tool
3. A loop that executes the suggested action in your environment
4. A way to acknowledge safety checks and give end users a chance to confirm actions
To see end-to-end example integrations, refer to our CUA sample app repository.
[
CUA sample app
Examples of how to integrate the computer use tool in different environments
](https://github.com/openai/openai-cua-sample-app)
Limitations
-----------
We recommend using the `computer-use-preview` model for browser-based tasks. The model may be susceptible to inadvertent model mistakes, especially in non-browser environments that it is less used to.
For example, `computer-use-preview`'s performance on OSWorld is currently 38.1%, indicating that the model is not yet highly reliable for automating tasks on an OS. More details about the model and related safety work can be found in our updated [system card](https://openai.com/index/operator-system-card/).
Some other behavior limitations to be aware of:
\* The [`computer-use-preview` model](/docs/models/computer-use-preview) has constrained rate limits and feature support, described on its model detail page.
\* [Refer to this guide](/docs/guides/your-data) for data retention, residency, and handling policies.
Risks and safety
----------------
Computer use presents unique risks that differ from those in standard API features or chat interfaces, especially when interacting with the internet.
There are a number of best practices listed below that you should follow to mitigate these risks.

#### Human in the loop for high-stakes tasks
Avoid tasks that are high-stakes or require high levels of accuracy. The model may make mistakes that are challenging to reverse. As mentioned above, the model is still prone to mistakes, especially on non-browser surfaces. While we expect the model to request user confirmation before proceeding with certain higher-impact decisions, this is not fully reliable. Ensure a human is in the loop to confirm model actions with real-world consequences.

#### Beware of prompt injections
A prompt injection occurs when an AI model mistakenly follows untrusted instructions appearing in its input. For the `computer-use-preview` model, this may manifest as it seeing something in the provided screenshot, like a malicious website or email, that instructs it to do something that the user does not want, and it complies. To avoid prompt injection risk, limit computer use access to trusted, isolated environments like a sandboxed browser or container.

#### Use blocklists and allowlists
Implement a blocklist or an allowlist of websites, actions, and users. For example, if you're using the computer use tool to book tickets on a website, create an allowlist of only the websites you expect to use in that workflow.

#### Send user IDs
Send end-user IDs (optional param) to help OpenAI monitor and detect abuse.

#### Use our safety checks
The following safety checks are available to protect against prompt injection and model mistakes:
\* Malicious instruction detection
\* Irrelevant domain detection
\* Sensitive domain detection
When you receive a `pending\_safety\_check`, you should increase oversight into model actions, for example by handing over to an end user to explicitly acknowledge the desire to proceed with the task and ensure that the user is actively monitoring the agent's actions (e.g., by implementing something like a watch mode similar to [Operator](https://operator.chatgpt.com/)). Essentially, when safety checks fire, a human should come into the loop.
Read the [acknowledge safety checks](#acknowledge-safety-checks) section above for more details on how to proceed when you receive a `pending\_safety\_check`.
Where possible, it is highly recommended to pass in the optional parameter `current\_url` as part of the `computer\_call\_output`, as it can help increase the accuracy of our safety checks.
Using current URL
```json
{
"type": "computer\_call\_output",
"call\_id": "call\_7OU...",
"acknowledged\_safety\_checks": [],
"output": {
"type": "computer\_screenshot",
"image\_url": "..."
},
"current\_url": "https://openai.com"
}
```

#### Additional safety precautions
Implement additional safety precautions as best suited for your application, such as implementing guardrails that run in parallel of the computer use loop.

#### Comply with our Usage Policy
Remember, you are responsible for using our services in compliance with the [OpenAI Usage Policy](https://openai.com/policies/usage-policies/) and [Business Terms](https://openai.com/policies/business-terms/), and we encourage you to employ our safety features and tools to help ensure this compliance.
Was this page useful?
