

## Imported snippet – 2025-07-03 11:52:03

Agents
======
Learn how to build agents with the OpenAI API.
Agents represent \*\*systems that intelligently accomplish tasks\*\*, ranging from executing simple workflows to pursuing complex, open-ended objectives.
OpenAI provides a \*\*rich set of composable primitives that enable you to build agents\*\*. This guide walks through those primitives, and how they come together to form a robust agentic platform.
Overview
--------
Building agents involves assembling components across several domains—such as \*\*models, tools, knowledge and memory, audio and speech, guardrails, and orchestration\*\*—and OpenAI provides composable primitives for each.
|Domain|Description|OpenAI Primitives|
|---|---|---|
|Models|Core intelligence capable of reasoning, making decisions, and processing different modalities.|o1, o3-mini, GPT-4.5, GPT-4o, GPT-4o-mini|
|Tools|Interface to the world, interact with environment, function calling, built-in tools, etc.|Function calling, Web search, File search, Computer use|
|Knowledge and memory|Augment agents with external and persistent knowledge.|Vector stores, File search, Embeddings|
|Audio and speech|Create agents that can understand audio and respond back in natural language.|Audio generation, realtime, Audio agents|
|Guardrails|Prevent irrelevant, harmful, or undesirable behavior.|Moderation, Instruction hierarchy (Python), Instruction hierarchy (TypeScript)|
|Orchestration|Develop, deploy, monitor, and improve agents.|Python Agents SDK, TypeScript Agents SDK, Tracing, Evaluations, Fine-tuning|
|Voice agents|Create agents that can understand audio and respond back in natural language.|Realtime API, Voice support in the Python Agents SDK, Voice support in the TypeScript Agents SDK|
Models
------
|Model|Agentic Strengths|
|---|---|
|o3 and o4-mini|Best for long-term planning, hard tasks, and reasoning.|
|GPT-4.1|Best for agentic execution.|
|GPT-4.1-mini|Good balance of agentic capability and latency.|
|GPT-4.1-nano|Best for low-latency.|
Large language models (LLMs) are at the core of many agentic systems, responsible for making decisions and interacting with the world. OpenAI’s models support a wide range of capabilities:
\* \*\*High intelligence:\*\* Capable of [reasoning](/docs/guides/reasoning) and planning to tackle the most difficult tasks.
\* \*\*Tools:\*\* [Call your functions](/docs/guides/function-calling) and leverage OpenAI's [built-in tools](/docs/guides/tools).
\* \*\*Multimodality:\*\* Natively understand text, images, audio, code, and documents.
\* \*\*Low-latency:\*\* Support for [real-time audio](/docs/guides/realtime) conversations and smaller, faster models.
For detailed model comparisons, visit the [models](/docs/models) page.
Tools
-----
Tools enable agents to interact with the world. OpenAI supports [\*\*function calling\*\*](/docs/guides/function-calling) to connect with your code, and [\*\*built-in tools\*\*](/docs/guides/tools) for common tasks like web searches and data retrieval.
|Tool|Description|
|---|---|
|Function calling|Interact with developer-defined code.|
|Web search|Fetch up-to-date information from the web.|
|File search|Perform semantic search across your documents.|
|Computer use|Understand and control a computer or browser.|
|Local shell|Execute commands on a local machine.|
Knowledge and memory
--------------------
Knowledge and memory help agents store, retrieve, and utilize information beyond their initial training data. \*\*Vector stores\*\* enable agents to search your documents semantically and retrieve relevant information at runtime. Meanwhile, \*\*embeddings\*\* represent data efficiently for quick retrieval, powering dynamic knowledge solutions and long-term agent memory. You can integrate your data using OpenAI’s [vector stores](/docs/guides/retrieval#vector-stores) and [Embeddings API](/docs/guides/embeddings).
Guardrails
----------
Guardrails ensure your agents behave safely, consistently, and within your intended boundaries—critical for production deployments. Use OpenAI’s free [Moderation API](/docs/guides/moderation) to automatically filter unsafe content. Further control your agent’s behavior by leveraging the [instruction hierarchy](https://openai.github.io/openai-agents-python/guardrails/), which prioritizes developer-defined prompts and mitigates unwanted agent behaviors.
Orchestration
-------------
Building agents is a process. OpenAI provides tools to effectively build, deploy, monitor, evaluate, and improve agentic systems.
![Agent Traces UI in OpenAI Dashboard](https://cdn.openai.com/API/docs/images/orchestration.png)
|Phase|Description|OpenAI Primitives|
|---|---|---|
|Build and deploy|Rapidly build agents, enforce guardrails, and handle conversational flows using the Agents SDK.|Agents SDK Python, Agents SDK TypeScript|
|Monitor|Observe agent behavior in real-time, debug issues, and gain insights through tracing.|Tracing|
|Evaluate and improve|Measure agent performance, identify areas for improvement, and refine your agents.|EvaluationsFine-tuning|
Get started
-----------
Python
```bash
pip install openai-agents
```
[
View the documentation
Check out our documentation for more information on how to get started with the Agents SDK for Python.
](https://openai.github.io/openai-agents-python/)[
View the Python repository
The OpenAI Agents SDK for Python is open source. Check out our repository for implementation details and a collection of examples.
](https://github.com/openai/openai-agents-python)
TypeScript/JavaScript
```bash
npm install @openai/agents
```
[
View the documentation
Check out our documentation for more information on how to get started with the Agents SDK for TypeScript.
](https://openai.github.io/openai-agents-js/)[
Check out the code
The OpenAI Agents SDK for TypeScript is open source. Check out our repository for implementation details and a collection of examples.
](https://github.com/openai/openai-agents-js)
Was this page useful?


## Imported snippet – 2025-07-03 11:52:07

Voice agents
============
Learn how to build voice agents that can understand audio and respond back in natural language.
Use the OpenAI API and Agents SDK to create powerful, context-aware voice agents for applications like customer support and language tutoring. This guide helps you design and build a voice agent.
Choose the right architecture
-----------------------------
OpenAI provides two primary architectures for building voice agents:
[
![Speech-to-Speech](https://cdn.openai.com/API/docs/images/blue\_card.png)
Speech-to-Speech
Native audio handling by the model using the Realtime API
](/docs/guides/voice-agents?voice-agent-architecture=speech-to-speech)[
![Chained](https://cdn.openai.com/API/docs/images/orange\_card.png)
Chained
Transforming audio to text and back to use existing models
](/docs/guides/voice-agents?voice-agent-architecture=chained)

### Speech-to-speech (realtime) architecture
![Diagram of a speech-to-speech agent](https://cdn.openai.com/API/docs/images/diagram-speech-to-speech.png)
The multimodal speech-to-speech (S2S) architecture directly processes audio inputs and outputs, handling speech in real time in a single multimodal model, `gpt-4o-realtime-preview`. The model thinks and responds in speech. It doesn't rely on a transcript of the user's input—it hears emotion and intent, filters out noise, and responds directly in speech. Use this approach for highly interactive, low-latency, conversational use cases.
|Strengths|Best for|
|---|---|
|Low latency interactions|Interactive and unstructured conversations|
|Rich multimodal understanding (audio and text simultaneously)|Language tutoring and interactive learning experiences|
|Natural, fluid conversational flow|Conversational search and discovery|
|Enhanced user experience through vocal context understanding|Interactive customer service scenarios|

### Chained architecture
![Diagram of a chained agent architecture](https://cdn.openai.com/API/docs/images/diagram-chained-agent.png)
A chained architecture processes audio sequentially, converting audio to text, generating intelligent responses using large language models (LLMs), and synthesizing audio from text. We recommend this predictable architecture if you're new to building voice agents. Both the user input and model's response are in text, so you have a transcript and can control what happens in your application. It's also a reliable way to convert an existing LLM-based application into a voice agent.
You're chaining these models: `gpt-4o-transcribe` → `gpt-4.1` → `gpt-4o-mini-tts`
|Strengths|Best for|
|---|---|
|High control and transparency|Structured workflows focused on specific user objectives|
|Robust function calling and structured interactions|Customer support|
|Reliable, predictable responses|Sales and inbound triage|
|Support for extended conversational context|Scenarios that involve transcripts and scripted responses|
The following guide below is for building agents using our recommended \*\*speech-to-speech architecture\*\*.
To learn more about the chained architecture, see [the chained architecture guide](/docs/guides/voice-agents?voice-agent-architecture=chained).
Build a voice agent
-------------------
Use OpenAI's APIs and SDKs to create powerful, context-aware voice agents.
Building a speech-to-speech voice agent requires:
1. Establishing a connection for realtime data transfer
2. Creating a realtime session with the Realtime API
3. Using an OpenAI model with realtime audio input and output capabilities
If you are new to building voice agents, we recommend using the [Realtime Agents in the TypeScript Agents SDK](https://openai.github.io/openai-agents-js/guides/voice-agents/) to get started with your voice agents.
```bash
npm install @openai/agents
```
If you want to get an idea of what interacting with a speech-to-speech voice agent looks like, check out our [quickstart guide](https://openai.github.io/openai-agents-js/guides/voice-agents/) to get started or check out our example application below.
[
Realtime API Agents Demo
A collection of example speech-to-speech voice agents including handoffs and reasoning model validation.
](https://github.com/openai/openai-realtime-agents)

### Choose your transport method
As latency is critical in voice agent use cases, the Realtime API provides two low-latency transport methods:
1. \*\*WebRTC\*\*: A peer-to-peer protocol that allows for low-latency audio and video communication.
2. \*\*WebSocket\*\*: A common protocol for realtime data transfer.
The two transport methods for the Realtime API support largely the same capabilities, but which one is more suitable for you will depend on your use case.
WebRTC is generally the better choice if you are building client-side applications such as browser-based voice agents.
For anything where you are executing the agent server-side such as building an agent that can [answer phone calls](https://github.com/openai/openai-realtime-twilio-demo), WebSockets will be the better option.
If you are using the [OpenAI Agents SDK for TypeScript](https://openai.github.io/openai-agents-js/), we will automatically use WebRTC if you are building in the browser and WebSockets otherwise.

### Design your voice agent
Just like when designing a text-based agent, you'll want to start small and keep your agent focused on a single task.
Try to limit the number of tools your agent has access to and provide an escape hatch for the agent to deal with tasks that it is not equipped to handle.
This could be a tool that allows the agent to handoff the conversation to a human or a certain phrase that it can fall back to.
While providing tools to text-based agents is a great way to provide additional context to the agent, for voice agents you should consider giving critical information as part of the prompt as opposed to requiring the agent to call a tool first.
If you are just getting started, check out our [Realtime Playground](/playground/realtime) that provides prompt generation helpers, as well as a way to stub out your function tools including stubbed tool responses to try end to end flows.

### Precisely prompt your agent
With speech-to-speech agents, prompting is even more powerful than with text-based agents as the prompt allows you to not just control the content of the agent's response but also the way the agent speaks or help it understand audio content.
A good example of what a prompt might look like:
```text

# Personality and Tone

## Identity
// Who or what the AI represents (e.g., friendly teacher, formal advisor, helpful assistant). Be detailed and include specific details about their character or backstory.

## Task
// At a high level, what is the agent expected to do? (e.g. "you are an expert at accurately handling user returns")

## Demeanor
// Overall attitude or disposition (e.g., patient, upbeat, serious, empathetic)

## Tone
// Voice style (e.g., warm and conversational, polite and authoritative)

## Level of Enthusiasm
// Degree of energy in responses (e.g., highly enthusiastic vs. calm and measured)

## Level of Formality
// Casual vs. professional language (e.g., “Hey, great to see you!” vs. “Good afternoon, how may I assist you?”)

## Level of Emotion
// How emotionally expressive or neutral the AI should be (e.g., compassionate vs. matter-of-fact)

## Filler Words
// Helps make the agent more approachable, e.g. “um,” “uh,” "hm," etc.. Options are generally "none", "occasionally", "often", "very often"

## Pacing
// Rhythm and speed of delivery

## Other details
// Any other information that helps guide the personality or tone of the agent.

# Instructions
- If a user provides a name or phone number, or something else where you ened to know the exact spelling, always repeat it back to the user to confrm you have the right understanding before proceeding. // Always include this
- If the caller corrects any detail, acknowledge the correction in a straightforward manner and confirm the new spelling or value.
```
You do not have to be as detailed with your instructions. This is for illustrative purposes. For shorter examples, check out the prompts on [OpenAI.fm](https://openai.fm).
For use cases with common conversation flows you can encode those inside the prompt using markup language like JSON
```text

# Conversation States
[
{
"id": "1\_greeting",
"description": "Greet the caller and explain the verification process.",
"instructions": [
"Greet the caller warmly.",
"Inform them about the need to collect personal information for their record."
],
"examples": [
"Good morning, this is the front desk administrator. I will assist you in verifying your details.",
"Let us proceed with the verification. May I kindly have your first name? Please spell it out letter by letter for clarity."
],
"transitions": [{
"next\_step": "2\_get\_first\_name",
"condition": "After greeting is complete."
}]
},
{
"id": "2\_get\_first\_name",
"description": "Ask for and confirm the caller's first name.",
"instructions": [
"Request: 'Could you please provide your first name?'",
"Spell it out letter-by-letter back to the caller to confirm."
],
"examples": [
"May I have your first name, please?",
"You spelled that as J-A-N-E, is that correct?"
],
"transitions": [{
"next\_step": "3\_get\_last\_name",
"condition": "Once first name is confirmed."
}]
},
{
"id": "3\_get\_last\_name",
"description": "Ask for and confirm the caller's last name.",
"instructions": [
"Request: 'Thank you. Could you please provide your last name?'",
"Spell it out letter-by-letter back to the caller to confirm."
],
"examples": [
"And your last name, please?",
"Let me confirm: D-O-E, is that correct?"
],
"transitions": [{
"next\_step": "4\_next\_steps",
"condition": "Once last name is confirmed."
}]
},
{
"id": "4\_next\_steps",
"description": "Attempt to verify the caller's information and proceed with next steps.",
"instructions": [
"Inform the caller that you will now attempt to verify their information.",
"Call the 'authenticateUser' function with the provided details.",
"Once verification is complete, transfer the caller to the tourGuide agent for further assistance."
],
"examples": [
"Thank you for providing your details. I will now verify your information.",
"Attempting to authenticate your information now.",
"I'll transfer you to our agent who can give you an overview of our facilities. Just to help demonstrate different agent personalities, she's instructed to act a little crabby."
],
"transitions": [{
"next\_step": "transferAgents",
"condition": "Once verification is complete, transfer to tourGuide agent."
}]
}
]
```
Instead of writing this out by hand, you can also check out this [Voice Agent Metaprompter](https://chatgpt.com/g/g-678865c9fb5c81918fa28699735dd08e-voice-agent-metaprompt-gpt) or [copy the metaprompt](https://github.com/openai/openai-realtime-agents/blob/main/src/app/agentConfigs/voiceAgentMetaprompt.txt) and use it directly.

### Handle agent handoff
In order to keep your agent focused on a single task, you can provide the agent with the ability to transfer or handoff to another specialized agent. You can do this by providing the agent with a function tool to initiate the transfer. This tool should have information on when to use it for a handoff.
If you are using the [OpenAI Agents SDK for TypeScript](https://openai.github.io/openai-agents-js/), you can define any agent as a potential handoff to another agent.
```typescript
import { RealtimeAgent } from "@openai/agents/realtime";
const productSpecialist = new RealtimeAgent({
name: 'Product Specialist',
instructions: 'You are a product specialist. You are responsible for answering questions about our products.',
});
const triageAgent = new RealtimeAgent({
name: 'Triage Agent',
instructions: 'You are a customer service frontline agent. You are responsible for triaging calls to the appropriate agent.',
tools: [
productSpecialist,
]
})
```
The SDK will automatically facilitate the handoff between the agents for you.
Alternatively if you are building your own voice agent, here is an example of such a tool definition:
```js
const tool = {
type: "function",
function: {
name: "transferAgents",
description: `
Triggers a transfer of the user to a more specialized agent.
Calls escalate to a more specialized LLM agent or to a human agent, with additional context.
Only call this function if one of the available agents is appropriate. Don't transfer to your own agent type.
Let the user know you're about to transfer them before doing so.
Available Agents:
- returns\_agent
- product\_specialist\_agent
`.trim(),
parameters: {
type: "object",
properties: {
rationale\_for\_transfer: {
type: "string",
description: "The reasoning why this transfer is needed.",
},
conversation\_context: {
type: "string",
description:
"Relevant context from the conversation that will help the recipient perform the correct action.",
},
destination\_agent: {
type: "string",
description:
"The more specialized destination\_agent that should handle the user's intended request.",
enum: ["returns\_agent", "product\_specialist\_agent"],
},
},
},
},
};
```
Once the agent calls that tool you can then use the `session.update` event of the Realtime API to update the configuration of the session to use the instructions and tools available to the specialized agent.

### Extend your agent with specialized models
![Diagram showing the speech-to-speech model calling other agents as tools](https://cdn.openai.com/API/docs/diagram-speech-to-speech-agent-tools.png)
While the speech-to-speech model is useful for conversational use cases, there might be use cases where you need a specific model to handle the task like having o3 validate a return request against a detailed return policy.
In that case you can expose your text-based agent using your preferred model as a function tool call that your agent can send specific requests to.
If you are using the [OpenAI Agents SDK for TypeScript](https://openai.github.io/openai-agents-js/), you can give a `RealtimeAgent` a `tool` that will trigger the specialized agent on your server.
```typescript
import { RealtimeAgent, tool } from '@openai/agents/realtime';
import { z } from 'zod';
const supervisorAgent = tool({
name: 'supervisorAgent',
description: 'Passes a case to your supervisor for approval.',
parameters: z.object({
caseDetails: z.string(),
}),
execute: async ({ caseDetails }, details) => {
const history = details.context.history;
const response = await fetch('/request/to/your/specialized/agent', {
method: 'POST',
body: JSON.stringify({
caseDetails,
history,
}),
});
return response.text();
},
});
const returnsAgent = new RealtimeAgent({
name: 'Returns Agent',
instructions: 'You are a returns agent. You are responsible for handling return requests. Always check with your supervisor before making a decision.',
tools: [
supervisorAgent,
],
});
```
Was this page useful?


## Imported snippet – 2025-07-03 11:52:40

https://openai.github.io/openai-agents-python/


## Imported snippet – 2025-07-03 11:53:15

OpenAI Agents SDK
The OpenAI Agents SDK enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, Swarm. The Agents SDK has a very small set of primitives:
Agents, which are LLMs equipped with instructions and tools
Handoffs, which allow agents to delegate to other agents for specific tasks
Guardrails, which enable the inputs to agents to be validated
In combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in tracing that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.
Why use the Agents SDK
The SDK has two driving design principles:
Enough features to be worth using, but few enough primitives to make it quick to learn.
Works great out of the box, but you can customize exactly what happens.
Here are the main features of the SDK:
Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.
Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.
Handoffs: A powerful feature to coordinate and delegate between multiple agents.
Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.
Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.
Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.
Installation
pip install openai-agents
Hello world example
from agents import Agent, Runner
agent = Agent(name="Assistant", instructions="You are a helpful assistant")
result = Runner.run\_sync(agent, "Write a haiku about recursion in programming.")
print(result.final\_output)

# Code within the code,

# Functions calling themselves,

# Infinite loop's dance.
(If running this, ensure you set the OPENAI\_API\_KEY environment variable)
export OPENAI\_API\_KEY=sk-...


## Imported snippet – 2025-07-03 11:53:35

https://github.com/openai/openai-agents-python.git
