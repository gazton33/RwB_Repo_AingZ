

## Imported snippet – 2025-07-03 11:47:38

Text generation and prompting
=============================
Learn how to prompt a model to generate text.
With the OpenAI API, you can use a [large language model](/docs/models) to generate text from a prompt, as you might using [ChatGPT](https://chatgpt.com). Models can generate almost any kind of text response—like code, mathematical equations, structured JSON data, or human-like prose.
Here's a simple example using the [Responses API](/docs/api-reference/responses).
Generate text from a simple prompt
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const response = await client.responses.create({
model: "gpt-4.1",
input: "Write a one-sentence bedtime story about a unicorn."
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4.1",
input="Write a one-sentence bedtime story about a unicorn."
)
print(response.output\_text)
```
```bash
curl "https://api.openai.com/v1/responses" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"input": "Write a one-sentence bedtime story about a unicorn."
}'
```
An array of content generated by the model is in the `output` property of the response. In this simple example, we have just one output which looks like this:
```json
[
{
"id": "msg\_67b73f697ba4819183a15cc17d011509",
"type": "message",
"role": "assistant",
"content": [
{
"type": "output\_text",
"text": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
"annotations": []
}
]
}
]
```
\*\*The `output` array often has more than one item in it!\*\* It can contain tool calls, data about reasoning tokens generated by [reasoning models](/docs/guides/reasoning), and other items. It is not safe to assume that the model's text output is present at `output[0].content[0].text`.
Some of our [official SDKs](/docs/libraries) include an `output\_text` property on model responses for convenience, which aggregates all text outputs from the model into a single string. This may be useful as a shortcut to access text output from the model.
In addition to plain text, you can also have the model return structured data in JSON format - this feature is called [\*\*Structured Outputs\*\*](/docs/guides/structured-outputs).
Choosing a model
----------------
A key choice to make when generating content through the API is which model you want to use - the `model` parameter of the code samples above. [You can find a full listing of available models here](/docs/models). Here are a few factors to consider when choosing a model for text generation.
\* \*\*[Reasoning models](/docs/guides/reasoning)\*\* generate an internal chain of thought to analyze the input prompt, and excel at understanding complex tasks and multi-step planning. They are also generally slower and more expensive to use than GPT models.
\* \*\*GPT models\*\* are fast, cost-efficient, and highly intelligent, but benefit from more explicit instructions around how to accomplish tasks.
\* \*\*Large and small (mini or nano) models\*\* offer trade-offs for speed, cost, and intelligence. Large models are more effective at understanding prompts and solving problems across domains, while small models are generally faster and cheaper to use.
When in doubt, [`gpt-4.1`](/docs/models/gpt-4.1) offers a solid combination of intelligence, speed, and cost effectiveness.
Prompt engineering
------------------
\*\*Prompt engineering\*\* is the process of writing effective instructions for a model, such that it consistently generates content that meets your requirements.
Because the content generated from a model is non-deterministic, it is a combination of art and science to build a prompt that will generate content in the format you want. However, there are a number of techniques and best practices you can apply to consistently get good results from a model.
Some prompt engineering techniques will work with every model, like using message roles. But different model types (like reasoning versus GPT models) might need to be prompted differently to produce the best results. Even different snapshots of models within the same family could produce different results. So as you are building more complex applications, we strongly recommend that you:
\* Pin your production applications to specific [model snapshots](/docs/models) (like `gpt-4.1-2025-04-14` for example) to ensure consistent behavior.
\* Build [evals](/docs/guides/evals) that will measure the behavior of your prompts, so that you can monitor the performance of your prompts as you iterate on them, or when you change and upgrade model versions.
Now, let's examine some tools and techniques available to you to construct prompts.
Message roles and instruction following
---------------------------------------
You can provide instructions to the model with [differing levels of authority](https://model-spec.openai.com/2025-02-12.html#chain\_of\_command) using the `instructions` API parameter or \*\*message roles\*\*.
The `instructions` parameter gives the model high-level instructions on how it should behave while generating a response, including tone, goals, and examples of correct responses. Any instructions provided this way will take priority over a prompt in the `input` parameter.
Generate text with instructions
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const response = await client.responses.create({
model: "gpt-4.1",
instructions: "Talk like a pirate.",
input: "Are semicolons optional in JavaScript?",
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4.1",
instructions="Talk like a pirate.",
input="Are semicolons optional in JavaScript?",
)
print(response.output\_text)
```
```bash
curl "https://api.openai.com/v1/responses" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"instructions": "Talk like a pirate.",
"input": "Are semicolons optional in JavaScript?"
}'
```
The example above is roughly equivalent to using the following input messages in the `input` array:
Generate text with messages using different roles
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const response = await client.responses.create({
model: "gpt-4.1",
input: [
{
role: "developer",
content: "Talk like a pirate."
},
{
role: "user",
content: "Are semicolons optional in JavaScript?",
},
],
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4.1",
input=[
{
"role": "developer",
"content": "Talk like a pirate."
},
{
"role": "user",
"content": "Are semicolons optional in JavaScript?"
}
]
)
print(response.output\_text)
```
```bash
curl "https://api.openai.com/v1/responses" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"input": [
{
"role": "developer",
"content": "Talk like a pirate."
},
{
"role": "user",
"content": "Are semicolons optional in JavaScript?"
}
]
}'
```
Note that the `instructions` parameter only applies to the current response generation request. If you are [managing conversation state](/docs/guides/conversation-state) with the `previous\_response\_id` parameter, the `instructions` used on previous turns will not be present in the context.
The [OpenAI model spec](https://model-spec.openai.com/2025-02-12.html#chain\_of\_command) describes how our models give different levels of priority to messages with different roles.
|developer|user|assistant|
|---|---|---|
|developer messages are instructions provided by the application developer, prioritized ahead of user messages.|user messages are instructions provided by an end user, prioritized behind developer messages.|Messages generated by the model have the assistant role.|
A multi-turn conversation may consist of several messages of these types, along with other content types provided by both you and the model. Learn more about [managing conversation state here](/docs/guides/conversation-state).
You could think about `developer` and `user` messages like a function and its arguments in a programming language.
\* `developer` messages provide the system's rules and business logic, like a function definition.
\* `user` messages provide inputs and configuration to which the `developer` message instructions are applied, like arguments to a function.
Reusable prompts
----------------
In the OpenAI dashboard, you can develop reusable [prompts](/playground/prompts) that you can use in API requests, rather than specifying the content of prompts in code. This way, you can more easily build and evaluate your prompts, and deploy improved versions of your prompts without changing your integration code.
Here's how it works:
1. \*\*Create a reusable prompt\*\* in the [dashboard](/playground/prompts) with placeholders like `{{customer\_name}}`.
2. \*\*Use the prompt\*\* in your API request with the `prompt` parameter. The prompt parameter object has three properties you can configure:
\* `id` — Unique identifier of your prompt, found in the dashboard
\* `version` — A specific version of your prompt (defaults to the "current" version as specified in the dashboard)
\* `variables` — A map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input message types like `input\_image` or `input\_file`. [See the full API reference](/docs/api-reference/responses/create).
String variables
Generate text with a prompt template
```javascript
import OpenAI from "openai";
const client = new OpenAI();
const response = await client.responses.create({
model: "gpt-4.1",
prompt: {
id: "pmpt\_abc123",
version: "2",
variables: {
customer\_name: "Jane Doe",
product: "40oz juice box"
}
}
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4.1",
prompt={
"id": "pmpt\_abc123",
"version": "2",
"variables": {
"customer\_name": "Jane Doe",
"product": "40oz juice box"
}
}
)
print(response.output\_text)
```
```bash
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4.1",
"prompt": {
"id": "pmpt\_abc123",
"version": "2",
"variables": {
"customer\_name": "Jane Doe",
"product": "40oz juice box"
}
}
}'
```
Variables with file input
Prompt template with file input variable
```javascript
import fs from "fs";
import OpenAI from "openai";
const client = new OpenAI();
// Upload a PDF we will reference in the prompt variables
const file = await client.files.create({
file: fs.createReadStream("draconomicon.pdf"),
purpose: "user\_data",
});
const response = await client.responses.create({
model: "gpt-4.1",
prompt: {
id: "pmpt\_abc123",
variables: {
topic: "Dragons",
reference\_pdf: {
type: "input\_file",
file\_id: file.id,
},
},
},
});
console.log(response.output\_text);
```
```python
import openai, pathlib
client = openai.OpenAI()

# Upload a PDF we will reference in the variables
file = client.files.create(
file=open("draconomicon.pdf", "rb"),
purpose="user\_data",
)
response = client.responses.create(
model="gpt-4.1",
prompt={
"id": "pmpt\_abc123",
"variables": {
"topic": "Dragons",
"reference\_pdf": {
"type": "input\_file",
"file\_id": file.id,
},
},
},
)
print(response.output\_text)
```
```bash

# Assume you have already uploaded the PDF and obtained FILE\_ID
curl https://api.openai.com/v1/responses -H "Authorization: Bearer $OPENAI\_API\_KEY" -H "Content-Type: application/json" -d '{
"model": "gpt-4.1",
"prompt": {
"id": "pmpt\_abc123",
"variables": {
"topic": "Dragons",
"reference\_pdf": {
"type": "input\_file",
"file\_id": "file-abc123"
}
}
}
}'
```
Message formatting with Markdown and XML
----------------------------------------
When writing `developer` and `user` messages, you can help the model understand logical boundaries of your prompt and context data using a combination of [Markdown](https://commonmark.org/help/) formatting and [XML tags](https://www.w3.org/TR/xml/).
Markdown headers and lists can be helpful to mark distinct sections of a prompt, and to communicate hierarchy to the model. They can also potentially make your prompts more readable during development. XML tags can help delineate where one piece of content (like a supporting document used for reference) begins and ends. XML attributes can also be used to define metadata about content in the prompt that can be referenced by your instructions.
In general, a developer message will contain the following sections, usually in this order (though the exact optimal content and order may vary by which model you are using):
\* \*\*Identity:\*\* Describe the purpose, communication style, and high-level goals of the assistant.
\* \*\*Instructions:\*\* Provide guidance to the model on how to generate the response you want. What rules should it follow? What should the model do, and what should the model never do? This section could contain many subsections as relevant for your use case, like how the model should [call custom functions](/docs/guides/function-calling).
\* \*\*Examples:\*\* Provide examples of possible inputs, along with the desired output from the model.
\* \*\*Context:\*\* Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant. This content is usually best positioned near the end of your prompt, as you may include different context for different generation requests.
Below is an example of using Markdown and XML tags to construct a `developer` message with distinct sections and supporting examples.
Example prompt
A developer message for code generation
```text

# Identity
You are coding assistant that helps enforce the use of snake case
variables in JavaScript code, and writing code that will run in
Internet Explorer version 6.

# Instructions
\* When defining variables, use snake case names (e.g. my\_variable)
instead of camel case names (e.g. myVariable).
\* To support old browsers, declare variables using the older
"var" keyword.
\* Do not give responses with Markdown formatting, just return
the code as requested.

# Examples
How do I declare a string variable for a first name?

var first\_name = "Anna";
```
API request
Send a prompt to generate code through the API
```javascript
import fs from "fs/promises";
import OpenAI from "openai";
const client = new OpenAI();
const instructions = await fs.readFile("prompt.txt", "utf-8");
const response = await client.responses.create({
model: "gpt-4.1",
instructions,
input: "How would I declare a variable for a last name?",
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
with open("prompt.txt", "r", encoding="utf-8") as f:
instructions = f.read()
response = client.responses.create(
model="gpt-4.1",
instructions=instructions,
input="How would I declare a variable for a last name?",
)
print(response.output\_text)
```
```bash
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4.1",
"instructions": "'"$(< prompt.txt)"'",
"input": "How would I declare a variable for a last name?"
}'
```

#### Save on cost and latency with prompt caching
When constructing a message, you should try and keep content that you expect to use over and over in your API requests at the beginning of your prompt, \*\*and\*\* among the first API parameters you pass in the JSON request body to [Chat Completions](/docs/api-reference/chat) or [Responses](/docs/api-reference/responses). This enables you to maximize cost and latency savings from [prompt caching](/docs/guides/prompt-caching).
Few-shot learning
-----------------
Few-shot learning lets you steer a large language model toward a new task by including a handful of input/output examples in the prompt, rather than [fine-tuning](/docs/guides/model-optimization) the model. The model implicitly "picks up" the pattern from those examples and applies it to a prompt. When providing examples, try to show a diverse range of possible inputs with the desired outputs.
Typically, you will provide examples as part of a `developer` message in your API request. Here's an example `developer` message containing examples that show a model how to classify positive or negative customer service reviews.
```text

# Identity
You are a helpful assistant that labels short product reviews as
Positive, Negative, or Neutral.

# Instructions
\* Only output a single word in your response with no additional formatting
or commentary.
\* Your response should only be one of the words "Positive", "Negative", or
"Neutral" depending on the sentiment of the product review you are given.

# Examples
I absolutely love this headphones — sound quality is amazing!

Positive

Battery life is okay, but the ear pads feel cheap.

Neutral

Terrible customer service, I'll never buy from them again.

Negative
```
Include relevant context information
------------------------------------
It is often useful to include additional context information the model can use to generate a response within the prompt you give the model. There are a few common reasons why you might do this:
\* To give the model access to proprietary data, or any other data outside the data set the model was trained on.
\* To constrain the model's response to a specific set of resources that you have determined will be most beneficial.
The technique of adding additional relevant context to the model generation request is sometimes called \*\*retrieval-augmented generation (RAG)\*\*. You can add additional context to the prompt in many different ways, from querying a vector database and including the text you get back into a prompt, or by using OpenAI's built-in [file search tool](/docs/guides/tools-file-search) to generate content based on uploaded documents.

#### Planning for the context window
Models can only handle so much data within the context they consider during a generation request. This memory limit is called a \*\*context window\*\*, which is defined in terms of [tokens](https://blogs.nvidia.com/blog/ai-tokens-explained) (chunks of data you pass in, from text to images).
Models have different context window sizes from the low 100k range up to one million tokens for newer GPT-4.1 models. [Refer to the model docs](/docs/models) for specific context window sizes per model.
Prompting GPT-4.1 models
------------------------
GPT models like [`gpt-4.1`](/docs/models/gpt-4.1) benefit from precise instructions that explicitly provide the logic and data required to complete the task in the prompt. GPT-4.1 in particular is highly steerable and responsive to well-specified prompts. To get the most out of GPT-4.1, refer to the prompting guide in the cookbook.
[
GPT-4.1 prompting guide
Get the most out of prompting GPT-4.1 with the tips and tricks in this prompting guide, extracted from real-world use cases and practical experience.
](https://cookbook.openai.com/examples/gpt4-1\_prompting\_guide)

#### GPT-4.1 prompting best practices
While the [cookbook](https://cookbook.openai.com/examples/gpt4-1\_prompting\_guide) has the best and most comprehensive guidance for prompting this model, here are a few best practices to keep in mind.
Building agentic workflows

### System Prompt Reminders
In order to best utilize the agentic capabilities of GPT-4.1, we recommend including three key types of reminders in all agent prompts for persistence, tool calling, and planning. As a whole, we find that these three instructions transform the model's behavior from chatbot-like into a much more "eager" agent, driving the interaction forward autonomously and independently. Here are a few examples:
```text

## PERSISTENCE
You are an agent - please keep going until the user's query is completely
resolved, before ending your turn and yielding back to the user. Only
terminate your turn when you are sure that the problem is solved.

## TOOL CALLING
If you are not sure about file content or codebase structure pertaining to
the user's request, use your tools to read files and gather the relevant
information: do NOT guess or make up an answer.

## PLANNING
You MUST plan extensively before each function call, and reflect
extensively on the outcomes of the previous function calls. DO NOT do this
entire process by making function calls only, as this can impair your
ability to solve the problem and think insightfully.
```

#### Tool Calls
Compared to previous models, GPT-4.1 has undergone more training on effectively utilizing tools passed as arguments in an OpenAI API request. We encourage developers to exclusively use the tools field of API requests to pass tools for best understanding and performance, rather than manually injecting tool descriptions into the system prompt and writing a separate parser for tool calls, as some have reported doing in the past.

#### Diff Generation
Correct diffs are critical for coding applications, so we've significantly improved performance at this task for GPT-4.1. In our cookbook, we open-source a recommended diff format on which GPT-4.1 has been extensively trained. That said, the model should generalize to any well-specified format.
Using long context
GPT-4.1 has a performant 1M token input context window, and will be useful for a variety of long context tasks, including structured document parsing, re-ranking, selecting relevant information while ignoring irrelevant context, and performing multi-hop reasoning using context.

#### Optimal Context Size
We show perfect performance at needle-in-a-haystack evals up to our full context size, and we've observed very strong performance at complex tasks with a mix of relevant and irrelevant code and documents in the range of hundreds of thousands of tokens.

#### Delimiters
We tested a variety of delimiters for separating context provided to the model against our long context evals. Briefly, XML and the format demonstrated by Lee et al. ([ref](https://arxiv.org/pdf/2406.13121)) tend to perform well, while JSON performed worse for this task. See our cookbook for prompt examples.

#### Prompt Organization
Especially in long context usage, placement of instructions and context can substantially impact performance. In our experiments, we found that it was optimal to put critical instructions, including the user query, at both the top and the bottom of the prompt; this elicited marginally better performance from the model than putting them only at the top, and much better performance than only at the bottom.
Prompting for chain of thought
As mentioned above, GPT-4.1 isn't a reasoning model, but prompting the model to think step by step (called "chain of thought") can be an effective way for a model to break down problems into more manageable pieces. The model has been trained to perform well at agentic reasoning and real-world problem solving, so it shouldn't require much prompting to do well.
We recommend starting with this basic chain-of-thought instruction at the end of your prompt:
```text
First, think carefully step by step about what documents are needed to answer the query. Then, print out the TITLE and ID of each document. Then, format the IDs into a list.
```
From there, you should improve your CoT prompt by auditing failures in your particular examples and evals, and addressing systematic planning and reasoning errors with more explicit instructions. See our cookbook for a prompt example demonstrating a more opinionated reasoning strategy.
Instruction following
GPT-4.1 exhibits outstanding instruction-following performance, which developers can leverage to precisely shape and control the outputs for their particular use cases. However, since the model follows instructions more literally than its predecessors, may need to provide more explicit specification around what to do or not do, and existing prompts optimized for other models may not immediately work with this model.

#### Recommended Workflow
Here is our recommended workflow for developing and debugging instructions in prompts:
\* Start with an overall "Response Rules" or "Instructions" section with high-level guidance and bullet points.
\* If you'd like to change a more specific behavior, add a section containing more details for that category, like `## Sample Phrases`.
\* If there are specific steps you'd like the model to follow in its workflow, add an ordered list and instruct the model to follow these steps.
\* If behavior still isn't working as expected, check for conflicting, underspecified, or incorrect instructions and examples. If there are conflicting instructions, GPT-4.1 tends to follow the one closer to the end of the prompt.
\* Add examples that demonstrate desired behavior; ensure that any important behavior demonstrated in your examples are also cited in your rules.
\* It's generally not necessary to use all-caps or other incentives like bribes or tips, but developers can experiment with this for extra emphasis if so desired.

#### Common Failure Modes
These failure modes are not unique to GPT-4.1, but we share them here for general awareness and ease of debugging.
\* Instructing a model to always follow a specific behavior can occasionally induce adverse effects. For instance, if told "you must call a tool before responding to the user," models may hallucinate tool inputs or call the tool with null values if they do not have enough information. Adding "if you don't have enough information to call the tool, ask the user for the information you need" should mitigate this.
\* When provided sample phrases, models can use those quotes verbatim and start to sound repetitive to users. Ensure you instruct the model to vary them as necessary.
\* Without specific instructions, some models can be eager to provide additional prose to explain their decisions, or output more formatting in responses than may be desired. Provide instructions and potentially examples to help mitigate.
See our cookbook for an example customer service prompt that demonstrates these principles.
Prompting reasoning models
--------------------------
There are some differences to consider when prompting a [reasoning model](/docs/guides/reasoning) versus prompting a GPT model. Generally speaking, reasoning models will provide better results on tasks with only high-level guidance. This differs from GPT models, which benefit from very precise instructions.
You could think about the difference between reasoning and GPT models like this.
\* A reasoning model is like a senior co-worker. You can give them a goal to achieve and trust them to work out the details.
\* A GPT model is like a junior coworker. They'll perform best with explicit instructions to create a specific output.
For more information on best practices when using reasoning models, [refer to this guide](/docs/guides/reasoning-best-practices).
Next steps
----------
Now that you known the basics of text inputs and outputs, you might want to check out one of these resources next.
[
Build a prompt in the Playground
Use the Playground to develop and iterate on prompts.
](/playground)[
Generate JSON data with Structured Outputs
Ensure JSON data emitted from a model conforms to a JSON schema.
](/docs/guides/structured-outputs)[
Full API reference
Check out all the options for text generation in the API reference.
](/docs/api-reference/responses)
Was this page useful?


## Imported snippet – 2025-07-03 11:47:40

Images and vision
=================
Learn how to understand or generate images.
Overview
--------
[
![Create images](https://cdn.openai.com/API/docs/images/images.png)
Create images
Use GPT Image or DALL·E to generate or edit images.
](/docs/guides/image-generation)[
![Process image inputs](https://cdn.openai.com/API/docs/images/vision.png)
Process image inputs
Use our models' vision capabilities to analyze images.
](/docs/guides/images-vision#analyze-images)
In this guide, you will learn about building applications involving images with the OpenAI API. If you know what you want to build, find your use case below to get started. If you're not sure where to start, continue reading to get an overview.

### A tour of image-related use cases
Recent language models can process image inputs and analyze them — a capability known as \*\*vision\*\*. With `gpt-image-1`, they can both analyze visual inputs and create images.
The OpenAI API offers several endpoints to process images as input or generate them as output, enabling you to build powerful multimodal applications.
|API|Supported use cases|
|---|---|
|Responses API|Analyze images and use them as input and/or generate images as output|
|Images API|Generate images as output, optionally using images as input|
|Chat Completions API|Analyze images and use them as input to generate text or audio|
To learn more about the input and output modalities supported by our models, refer to our [models page](/docs/models).
Generate or edit images
-----------------------
You can generate or edit images using the Image API or the Responses API.
Our latest image generation model, `gpt-image-1`, is a natively multimodal large language model. It can understand text and images and leverage its broad world knowledge to generate images with better instruction following and contextual awareness.
In contrast, we also offer specialized image generation models - DALL·E 2 and 3 - which don't have the same inherent understanding of the world as GPT Image.
Generate images with Responses
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input: "Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools: [{type: "image\_generation"}],
});
// Save the image to a file
const imageData = response.output
.filter((output) => output.type === "image\_generation\_call")
.map((output) => output.result);
if (imageData.length > 0) {
const imageBase64 = imageData[0];
const fs = await import("fs");
fs.writeFileSync("cat\_and\_otter.png", Buffer.from(imageBase64, "base64"));
}
```
```python
from openai import OpenAI
import base64
client = OpenAI()
response = client.responses.create(
model="gpt-4.1-mini",
input="Generate an image of gray tabby cat hugging an otter with an orange scarf",
tools=[{"type": "image\_generation"}],
)
// Save the image to a file
image\_data = [
output.result
for output in response.output
if output.type == "image\_generation\_call"
]
if image\_data:
image\_base64 = image\_data[0]
with open("cat\_and\_otter.png", "wb") as f:
f.write(base64.b64decode(image\_base64))
```
You can learn more about image generation in our [Image generation](/docs/guides/image-generation) guide.

### Using world knowledge for image generation
The difference between DALL·E models and GPT Image is that a natively multimodal language model can use its visual understanding of the world to generate lifelike images including real-life details without a reference.
For example, if you prompt GPT Image to generate an image of a glass cabinet with the most popular semi-precious stones, the model knows enough to select gemstones like amethyst, rose quartz, jade, etc, and depict them in a realistic way.
Analyze images
--------------
\*\*Vision\*\* is the ability for a model to "see" and understand images. If there is text in an image, the model can also understand the text. It can understand most visual elements, including objects, shapes, colors, and textures, even if there are some [limitations](#limitations).

### Giving a model images as input
You can provide images as input to generation requests in multiple ways:
\* By providing a fully qualified URL to an image file
\* By providing an image as a Base64-encoded data URL
\* By providing a file ID (created with the [Files API](/docs/api-reference/files))
You can provide multiple images as input in a single request by including multiple images in the `content` array, but keep in mind that [images count as tokens](#calculating-costs) and will be billed accordingly.
Passing a URL
Analyze the content of an image
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input: [{
role: "user",
content: [
{ type: "input\_text", text: "what's in this image?" },
{
type: "input\_image",
image\_url: "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
},
],
}],
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4.1-mini",
input=[{
"role": "user",
"content": [
{"type": "input\_text", "text": "what's in this image?"},
{
"type": "input\_image",
"image\_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
},
],
}],
)
print(response.output\_text)
```
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1-mini",
"input": [
{
"role": "user",
"content": [
{"type": "input\_text", "text": "what is in this image?"},
{
"type": "input\_image",
"image\_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
}
]
}
]
}'
```
Passing a Base64 encoded image
Analyze the content of an image
```javascript
import fs from "fs";
import OpenAI from "openai";
const openai = new OpenAI();
const imagePath = "path\_to\_your\_image.jpg";
const base64Image = fs.readFileSync(imagePath, "base64");
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input: [
{
role: "user",
content: [
{ type: "input\_text", text: "what's in this image?" },
{
type: "input\_image",
image\_url: `data:image/jpeg;base64,${base64Image}`,
},
],
},
],
});
console.log(response.output\_text);
```
```python
import base64
from openai import OpenAI
client = OpenAI()

# Function to encode the image
def encode\_image(image\_path):
with open(image\_path, "rb") as image\_file:
return base64.b64encode(image\_file.read()).decode("utf-8")

# Path to your image
image\_path = "path\_to\_your\_image.jpg"

# Getting the Base64 string
base64\_image = encode\_image(image\_path)
response = client.responses.create(
model="gpt-4.1",
input=[
{
"role": "user",
"content": [
{ "type": "input\_text", "text": "what's in this image?" },
{
"type": "input\_image",
"image\_url": f"data:image/jpeg;base64,{base64\_image}",
},
],
}
],
)
print(response.output\_text)
```
Passing a file ID
Analyze the content of an image
```javascript
import OpenAI from "openai";
import fs from "fs";
const openai = new OpenAI();
// Function to create a file with the Files API
async function createFile(filePath) {
const fileContent = fs.createReadStream(filePath);
const result = await openai.files.create({
file: fileContent,
purpose: "vision",
});
return result.id;
}
// Getting the file ID
const fileId = await createFile("path\_to\_your\_image.jpg");
const response = await openai.responses.create({
model: "gpt-4.1-mini",
input: [
{
role: "user",
content: [
{ type: "input\_text", text: "what's in this image?" },
{
type: "input\_image",
file\_id: fileId,
},
],
},
],
});
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI()

# Function to create a file with the Files API
def create\_file(file\_path):
with open(file\_path, "rb") as file\_content:
result = client.files.create(
file=file\_content,
purpose="vision",
)
return result.id

# Getting the file ID
file\_id = create\_file("path\_to\_your\_image.jpg")
response = client.responses.create(
model="gpt-4.1-mini",
input=[{
"role": "user",
"content": [
{"type": "input\_text", "text": "what's in this image?"},
{
"type": "input\_image",
"file\_id": file\_id,
},
],
}],
)
print(response.output\_text)
```

### Image input requirements
Input images must meet the following requirements to be used in the API.
|Supported file types|PNG (.png)JPEG (.jpeg and .jpg)WEBP (.webp)Non-animated GIF (.gif)|
|Size limits|Up to 50 MB total payload size per requestUp to 500 individual image inputs per request|
|Other requirements|No watermarks or logosNo NSFW contentClear enough for a human to understand|

### Specify image input detail level
The `detail` parameter tells the model what level of detail to use when processing and understanding the image (`low`, `high`, or `auto` to let the model decide). If you skip the parameter, the model will use `auto`.
```plain
{
"type": "input\_image",
"image\_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
"detail": "high"
}
```
You can save tokens and speed up responses by using `"detail": "low"`. This lets the model process the image with a budget of 85 tokens. The model receives a low-resolution 512px x 512px version of the image. This is fine if your use case doesn't require the model to see with high-resolution detail (for example, if you're asking about the dominant shape or color in the image).
On the other hand, you can use `"detail": "high"` if you want the model to have a better understanding of the image.
Read more about calculating image processing costs in the [Calculating costs](#calculating-costs) section below.
Limitations
-----------
While models with vision capabilities are powerful and can be used in many situations, it's important to understand the limitations of these models. Here are some known limitations:
\* \*\*Medical images\*\*: The model is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.
\* \*\*Non-English\*\*: The model may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean.
\* \*\*Small text\*\*: Enlarge text within the image to improve readability, but avoid cropping important details.
\* \*\*Rotation\*\*: The model may misinterpret rotated or upside-down text and images.
\* \*\*Visual elements\*\*: The model may struggle to understand graphs or text where colors or styles—like solid, dashed, or dotted lines—vary.
\* \*\*Spatial reasoning\*\*: The model struggles with tasks requiring precise spatial localization, such as identifying chess positions.
\* \*\*Accuracy\*\*: The model may generate incorrect descriptions or captions in certain scenarios.
\* \*\*Image shape\*\*: The model struggles with panoramic and fisheye images.
\* \*\*Metadata and resizing\*\*: The model doesn't process original file names or metadata, and images are resized before analysis, affecting their original dimensions.
\* \*\*Counting\*\*: The model may give approximate counts for objects in images.
\* \*\*CAPTCHAS\*\*: For safety reasons, our system blocks the submission of CAPTCHAs.
Calculating costs
-----------------
Image inputs are metered and charged in tokens, just as text inputs are. How images are converted to text token inputs varies based on the model. You can find a vision pricing calculator in the FAQ section of the [pricing page](https://openai.com/api/pricing/).

### GPT-4.1-mini, GPT-4.1-nano, o4-mini
Image inputs are metered and charged in tokens based on their dimensions. The token cost of an image is determined as follows:
\* Calculate the number of 32px x 32px patches that are needed to fully cover the image
\* If the number of patches exceeds 1536, we scale down the image so that it can be covered by no more than 1536 patches
\* The token cost is the number of patches, capped at a maximum of 1536 tokens
\* For `gpt-4.1-mini` we multiply image tokens by 1.62 to get total tokens, for `gpt-4.1-nano` we multiply image tokens by 2.46 to get total tokens, and for `o4-mini` we multiply image tokens by 1.72 to get total tokens, that are then billed at normal text token rates.
Note:
\*\*Cost calculation examples\*\*
\* A 1024 x 1024 image is \*\*1024 tokens\*\*
\* Width is 1024, resulting in `(1024 + 32 - 1) // 32 = 32` patches
\* Height is 1024, resulting in `(1024 + 32 - 1) // 32 = 32` patches
\* Tokens calculated as `32 \* 32 = 1024`, below the cap of 1536
\* A 1800 x 2400 image is \*\*1452 tokens\*\*
\* Width is 1800, resulting in `(1800 + 32 - 1) // 32 = 57` patches
\* Height is 2400, resulting in `(2400 + 32 - 1) // 32 = 75` patches
\* We need `57 \* 75 = 4275` patches to cover the full image. Since that exceeds 1536, we need to scale down the image while preserving the aspect ratio.
\* We can calculate the shrink factor as `sqrt(token\_budget × patch\_size^2 / (width \* height))`. In our example, the shrink factor is `sqrt(1536 \* 32^2 / (1800 \* 2400)) = 0.603`.
\* Width is now 1086, resulting in `1086 / 32 = 33.94` patches
\* Height is now 1448, resulting in `1448 / 32 = 45.25` patches
\* We want to make sure the image fits in a whole number of patches. In this case we scale again by `33 / 33.94 = 0.97` to fit the width in 33 patches.
\* The final width is then `1086 \* (33 / 33.94) = 1056)` and the final height is `1448 \* (33 / 33.94) = 1408`
\* The image now requires `1056 / 32 = 33` patches to cover the width and `1408 / 32 = 44` patches to cover the height
\* The total number of tokens is the `33 \* 44 = 1452`, below the cap of 1536

### GPT 4o, GPT-4.1, GPT-4o-mini, CUA, and o-series (except o4-mini)
The token cost of an image is determined by two factors: size and detail.
Any image with `"detail": "low"` costs a set, base number of tokens. This amount varies by model (see charte below). To calculate the cost of an image with `"detail": "high"`, we do the following:
\* Scale to fit in a 2048px x 2048px square, maintaining original aspect ratio
\* Scale so that the image's shortest side is 768px long
\* Count the number of 512px squares in the image—each square costs a set amount of tokens (see chart below)
\* Add the base tokens to the total
|Model|Base tokens|Tile tokens|
|---|---|---|
|4o, 4.1, 4.5|85|170|
|4o-mini|2833|5667|
|o1, o1-pro, o3|75|150|
|computer-use-preview|65|129|
\*\*Cost calculation examples (for gpt-4o)\*\*
\* A 1024 x 1024 square image in `"detail": "high"` mode costs 765 tokens
\* 1024 is less than 2048, so there is no initial resize.
\* The shortest side is 1024, so we scale the image down to 768 x 768.
\* 4 512px square tiles are needed to represent the image, so the final token cost is `170 \* 4 + 85 = 765`.
\* A 2048 x 4096 image in `"detail": "high"` mode costs 1105 tokens
\* We scale down the image to 1024 x 2048 to fit within the 2048 square.
\* The shortest side is 1024, so we further scale down to 768 x 1536.
\* 6 512px tiles are needed, so the final token cost is `170 \* 6 + 85 = 1105`.
\* A 4096 x 8192 image in `"detail": "low"` most costs 85 tokens
\* Regardless of input size, low detail images are a fixed cost.

### GPT Image 1
For GPT Image 1, we calculate the cost of an image input the same way as described above, except that we scale down the image so that the shortest side is 512px instead of 768px. There is no detail level configuration for this model, so the price depends on the dimensions of the image.
The base cost is 65 image tokens, and each tile costs 129 image tokens.
To see pricing for image input tokens, refer to our [pricing page](/docs/pricing#latest-models).
\* \* \*
We process images at the token level, so each image we process counts towards your tokens per minute (TPM) limit.
For the most precise and up-to-date estimates for image processing, please use our image pricing calculator available [here](https://openai.com/api/pricing/).
Was this page useful?


## Imported snippet – 2025-07-03 11:47:43

Audio and speech
================
Explore audio and speech features in the OpenAI API.
The OpenAI API provides a range of audio capabilities. If you know what you want to build, find your use case below to get started. If you're not sure where to start, read this page as an overview.
Build with audio
----------------
[
![Build voice agents](https://cdn.openai.com/API/docs/images/voice-agents-rounded.png)
Build voice agents
Build interactive voice-driven applications.
](/docs/guides/voice-agents)[
![Transcribe audio](https://cdn.openai.com/API/docs/images/stt-rounded.png)
Transcribe audio
Convert speech to text instantly and accurately.
](/docs/guides/speech-to-text)[
![Speak text](https://cdn.openai.com/API/docs/images/tts-rounded.png)
Speak text
Turn text into natural-sounding speech in real time.
](/docs/guides/text-to-speech)
A tour of audio use cases
-------------------------
LLMs can process audio by using sound as input, creating sound as output, or both. OpenAI has several API endpoints that help you build audio applications or voice agents.

### Voice agents
Voice agents understand audio to handle tasks and respond back in natural language. There are two main ways to approach voice agents: either with speech-to-speech models and the [Realtime API](/docs/guides/realtime), or by chaining together a speech-to-text model, a text language model to process the request, and a text-to-speech model to respond. Speech-to-speech is lower latency and more natural, but chaining together a voice agent is a reliable way to extend a text-based agent into a voice agent. If you are already using the [Agents SDK](/docs/guides/agents), you can [extend your existing agents with voice capabilities](https://openai.github.io/openai-agents-python/voice/quickstart/) using the chained approach.

### Streaming audio
Process audio in real time to build voice agents and other low-latency applications, including transcription use cases. You can stream audio in and out of a model with the [Realtime API](/docs/guides/realtime). Our advanced speech models provide automatic speech recognition for improved accuracy, low-latency interactions, and multilingual support.

### Text to speech
For turning text into speech, use the [Audio API](/docs/api-reference/audio/) `audio/speech` endpoint. Models compatible with this endpoint are `gpt-4o-mini-tts`, `tts-1`, and `tts-1-hd`. With `gpt-4o-mini-tts`, you can ask the model to speak a certain way or with a certain tone of voice.

### Speech to text
For speech to text, use the [Audio API](/docs/api-reference/audio/) `audio/transcriptions` endpoint. Models compatible with this endpoint are `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, and `whisper-1`. With streaming, you can continuously pass in audio and get a continuous stream of text back.
Choosing the right API
----------------------
There are multiple APIs for transcribing or generating audio:
|API|Supported modalities|Streaming support|
|---|---|---|
|Realtime API|Audio and text inputs and outputs|Audio streaming in and out|
|Chat Completions API|Audio and text inputs and outputs|Audio streaming out|
|Transcription API|Audio inputs|Audio streaming out|
|Speech API|Text inputs and audio outputs|Audio streaming out|

### General use APIs vs. specialized APIs
The main distinction is general use APIs vs. specialized APIs. With the Realtime and Chat Completions APIs, you can use our latest models' native audio understanding and generation capabilities and combine them with other features like function calling. These APIs can be used for a wide range of use cases, and you can select the model you want to use.
On the other hand, the Transcription, Translation and Speech APIs are specialized to work with specific models and only meant for one purpose.

### Talking with a model vs. controlling the script
Another way to select the right API is asking yourself how much control you need. To design conversational interactions, where the model thinks and responds in speech, use the Realtime or Chat Completions API, depending if you need low-latency or not.
You won't know exactly what the model will say ahead of time, as it will generate audio responses directly, but the conversation will feel natural.
For more control and predictability, you can use the Speech-to-text / LLM / Text-to-speech pattern, so you know exactly what the model will say and can control the response. Please note that with this method, there will be added latency.
This is what the Audio APIs are for: pair an LLM with the `audio/transcriptions` and `audio/speech` endpoints to take spoken user input, process and generate a text response, and then convert that to speech that the user can hear.

### Recommendations
\* If you need [real-time interactions](/docs/guides/realtime-conversations) or [transcription](/docs/guides/realtime-transcription), use the Realtime API.
\* If realtime is not a requirement but you're looking to build a [voice agent](/docs/guides/voice-agents) or an audio-based application that requires features such as [function calling](/docs/guides/function-calling), use the Chat Completions API.
\* For use cases with one specific purpose, use the Transcription, Translation, or Speech APIs.
Add audio to your existing application
--------------------------------------
Models such as GPT-4o or GPT-4o mini are natively multimodal, meaning they can understand and generate multiple modalities as input and output.
If you already have a text-based LLM application with the [Chat Completions endpoint](/docs/api-reference/chat/), you may want to add audio capabilities. For example, if your chat application supports text input, you can add audio input and output—just include `audio` in the `modalities` array and use an audio model, like `gpt-4o-audio-preview`.
Audio is not yet supported in the [Responses API](/docs/api-reference/chat/completions/responses).
Audio output from model
Create a human-like audio response to a prompt
```javascript
import { writeFileSync } from "node:fs";
import OpenAI from "openai";
const openai = new OpenAI();
// Generate an audio response to the given prompt
const response = await openai.chat.completions.create({
model: "gpt-4o-audio-preview",
modalities: ["text", "audio"],
audio: { voice: "alloy", format: "wav" },
messages: [
{
role: "user",
content: "Is a golden retriever a good family dog?"
}
],
store: true,
});
// Inspect returned data
console.log(response.choices[0]);
// Write audio data to a file
writeFileSync(
"dog.wav",
Buffer.from(response.choices[0].message.audio.data, 'base64'),
{ encoding: "utf-8" }
);
```
```python
import base64
from openai import OpenAI
client = OpenAI()
completion = client.chat.completions.create(
model="gpt-4o-audio-preview",
modalities=["text", "audio"],
audio={"voice": "alloy", "format": "wav"},
messages=[
{
"role": "user",
"content": "Is a golden retriever a good family dog?"
}
]
)
print(completion.choices[0])
wav\_bytes = base64.b64decode(completion.choices[0].message.audio.data)
with open("dog.wav", "wb") as f:
f.write(wav\_bytes)
```
```bash
curl "https://api.openai.com/v1/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4o-audio-preview",
"modalities": ["text", "audio"],
"audio": { "voice": "alloy", "format": "wav" },
"messages": [
{
"role": "user",
"content": "Is a golden retriever a good family dog?"
}
]
}'
```
Audio input to model
Use audio inputs for prompting a model
```javascript
import OpenAI from "openai";
const openai = new OpenAI();
// Fetch an audio file and convert it to a base64 string
const url = "https://cdn.openai.com/API/docs/audio/alloy.wav";
const audioResponse = await fetch(url);
const buffer = await audioResponse.arrayBuffer();
const base64str = Buffer.from(buffer).toString("base64");
const response = await openai.chat.completions.create({
model: "gpt-4o-audio-preview",
modalities: ["text", "audio"],
audio: { voice: "alloy", format: "wav" },
messages: [
{
role: "user",
content: [
{ type: "text", text: "What is in this recording?" },
{ type: "input\_audio", input\_audio: { data: base64str, format: "wav" }}
]
}
],
store: true,
});
console.log(response.choices[0]);
```
```python
import base64
import requests
from openai import OpenAI
client = OpenAI()

# Fetch the audio file and convert it to a base64 encoded string
url = "https://cdn.openai.com/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise\_for\_status()
wav\_data = response.content
encoded\_string = base64.b64encode(wav\_data).decode('utf-8')
completion = client.chat.completions.create(
model="gpt-4o-audio-preview",
modalities=["text", "audio"],
audio={"voice": "alloy", "format": "wav"},
messages=[
{
"role": "user",
"content": [
{
"type": "text",
"text": "What is in this recording?"
},
{
"type": "input\_audio",
"input\_audio": {
"data": encoded\_string,
"format": "wav"
}
}
]
},
]
)
print(completion.choices[0].message)
```
```bash
curl "https://api.openai.com/v1/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4o-audio-preview",
"modalities": ["text", "audio"],
"audio": { "voice": "alloy", "format": "wav" },
"messages": [
{
"role": "user",
"content": [
{ "type": "text", "text": "What is in this recording?" },
{
"type": "input\_audio",
"input\_audio": {
"data": "",
"format": "wav"
}
}
]
}
]
}'
```
Was this page useful?


## Imported snippet – 2025-07-03 11:47:47

Structured Outputs
==================
Ensure responses adhere to a JSON schema.
Try it out
----------
Try it out in the [Playground](/playground) or generate a ready-to-use schema definition to experiment with structured outputs.
Generate
Introduction
------------
JSON is one of the most widely used formats in the world for applications to exchange data.
Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied [JSON Schema](https://json-schema.org/overview/what-is-jsonschema), so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.
Some benefits of Structured Outputs include:
1. \*\*Reliable type-safety:\*\* No need to validate or retry incorrectly formatted responses
2. \*\*Explicit refusals:\*\* Safety-based model refusals are now programmatically detectable
3. \*\*Simpler prompting:\*\* No need for strongly worded prompts to achieve consistent formatting
In addition to supporting JSON Schema in the REST API, the OpenAI SDKs for [Python](https://github.com/openai/openai-python/blob/main/helpers.md#structured-outputs-parsing-helpers) and [JavaScript](https://github.com/openai/openai-node/blob/master/helpers.md#structured-outputs-parsing-helpers) also make it easy to define object schemas using [Pydantic](https://docs.pydantic.dev/latest/) and [Zod](https://zod.dev/) respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.
Getting a structured response
```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";
const openai = new OpenAI();
const CalendarEvent = z.object({
name: z.string(),
date: z.string(),
participants: z.array(z.string()),
});
const response = await openai.responses.parse({
model: "gpt-4o-2024-08-06",
input: [
{ role: "system", content: "Extract the event information." },
{
role: "user",
content: "Alice and Bob are going to a science fair on Friday.",
},
],
text: {
format: zodTextFormat(CalendarEvent, "event"),
},
});
const event = response.output\_parsed;
```
```python
from openai import OpenAI
from pydantic import BaseModel
client = OpenAI()
class CalendarEvent(BaseModel):
name: str
date: str
participants: list[str]
response = client.responses.parse(
model="gpt-4o-2024-08-06",
input=[
{"role": "system", "content": "Extract the event information."},
{
"role": "user",
"content": "Alice and Bob are going to a science fair on Friday.",
},
],
text\_format=CalendarEvent,
)
event = response.output\_parsed
```

### Supported models
Structured Outputs is available in our [latest large language models](/docs/models), starting with GPT-4o. Older models like `gpt-4-turbo` and earlier may use [JSON mode](#json-mode) instead.
When to use Structured Outputs via function calling vs via text.format
----------------------------------------------------------------------
Structured Outputs is available in two forms in the OpenAI API:
1. When using [function calling](/docs/guides/function-calling)
2. When using a `json\_schema` response format
Function calling is useful when you are building an application that bridges the models and functionality of your application.
For example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI.
Conversely, Structured Outputs via `response\_format` are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool.
For example, if you are building a math tutoring application, you might want the assistant to respond to your user using a specific JSON Schema so that you can generate a UI that displays different parts of the model's output in distinct ways.
Put simply:
\* If you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling
\* If you want to structure the model's output when it responds to the user, then you should use a structured `text.format`
The remainder of this guide will focus on non-function calling use cases in the Responses API. To learn more about how to use Structured Outputs with function calling, check out the [Function Calling](/docs/guides/function-calling#function-calling-with-structured-outputs) guide.

### Structured Outputs vs JSON mode
Structured Outputs is the evolution of [JSON mode](#json-mode). While both ensure valid JSON is produced, only Structured Outputs ensure schema adherance. Both Structured Outputs and JSON mode are supported in the Responses API,Chat Completions API, Assistants API, Fine-tuning API and Batch API.
We recommend always using Structured Outputs instead of JSON mode when possible.
However, Structured Outputs with `response\_format: {type: "json\_schema", ...}` is only supported with the `gpt-4o-mini`, `gpt-4o-mini-2024-07-18`, and `gpt-4o-2024-08-06` model snapshots and later.
||Structured Outputs|JSON Mode|
|---|---|---|
|Outputs valid JSON|Yes|Yes|
|Adheres to schema|Yes (see supported schemas)|No|
|Compatible models|gpt-4o-mini, gpt-4o-2024-08-06, and later|gpt-3.5-turbo, gpt-4-\* and gpt-4o-\* models|
|Enabling|text: { format: { type: "json\_schema", "strict": true, "schema": ... } }|text: { format: { type: "json\_object" } }|
Examples
--------
Chain of thought

### Chain of thought
You can ask the model to output an answer in a structured, step-by-step way, to guide the user through the solution.
Structured Outputs for chain-of-thought math tutoring
```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";
const openai = new OpenAI();
const Step = z.object({
explanation: z.string(),
output: z.string(),
});
const MathReasoning = z.object({
steps: z.array(Step),
final\_answer: z.string(),
});
const response = await openai.responses.parse({
model: "gpt-4o-2024-08-06",
input: [
{
role: "system",
content:
"You are a helpful math tutor. Guide the user through the solution step by step.",
},
{ role: "user", content: "how can I solve 8x + 7 = -23" },
],
text: {
format: zodTextFormat(MathReasoning, "math\_reasoning"),
},
});
const math\_reasoning = response.output\_parsed;
```
```python
from openai import OpenAI
from pydantic import BaseModel
client = OpenAI()
class Step(BaseModel):
explanation: str
output: str
class MathReasoning(BaseModel):
steps: list[Step]
final\_answer: str
response = client.responses.parse(
model="gpt-4o-2024-08-06",
input=[
{
"role": "system",
"content": "You are a helpful math tutor. Guide the user through the solution step by step.",
},
{"role": "user", "content": "how can I solve 8x + 7 = -23"},
],
text\_format=MathReasoning,
)
math\_reasoning = response.output\_parsed
```
```bash
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4o-2024-08-06",
"input": [
{
"role": "system",
"content": "You are a helpful math tutor. Guide the user through the solution step by step."
},
{
"role": "user",
"content": "how can I solve 8x + 7 = -23"
}
],
"text": {
"format": {
"type": "json\_schema",
"name": "math\_reasoning",
"schema": {
"type": "object",
"properties": {
"steps": {
"type": "array",
"items": {
"type": "object",
"properties": {
"explanation": { "type": "string" },
"output": { "type": "string" }
},
"required": ["explanation", "output"],
"additionalProperties": false
}
},
"final\_answer": { "type": "string" }
},
"required": ["steps", "final\_answer"],
"additionalProperties": false
},
"strict": true
}
}
}'
```

#### Example response
```json
{
"steps": [
{
"explanation": "Start with the equation 8x + 7 = -23.",
"output": "8x + 7 = -23"
},
{
"explanation": "Subtract 7 from both sides to isolate the term with the variable.",
"output": "8x = -23 - 7"
},
{
"explanation": "Simplify the right side of the equation.",
"output": "8x = -30"
},
{
"explanation": "Divide both sides by 8 to solve for x.",
"output": "x = -30 / 8"
},
{
"explanation": "Simplify the fraction.",
"output": "x = -15 / 4"
}
],
"final\_answer": "x = -15 / 4"
}
```
Structured data extraction

### Structured data extraction
You can define structured fields to extract from unstructured input data, such as research papers.
Extracting data from research papers using Structured Outputs
```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";
const openai = new OpenAI();
const ResearchPaperExtraction = z.object({
title: z.string(),
authors: z.array(z.string()),
abstract: z.string(),
keywords: z.array(z.string()),
});
const response = await openai.responses.parse({
model: "gpt-4o-2024-08-06",
input: [
{
role: "system",
content:
"You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.",
},
{ role: "user", content: "..." },
],
text: {
format: zodTextFormat(ResearchPaperExtraction, "research\_paper\_extraction"),
},
});
const research\_paper = response.output\_parsed;
```
```python
from openai import OpenAI
from pydantic import BaseModel
client = OpenAI()
class ResearchPaperExtraction(BaseModel):
title: str
authors: list[str]
abstract: str
keywords: list[str]
response = client.responses.parse(
model="gpt-4o-2024-08-06",
input=[
{
"role": "system",
"content": "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.",
},
{"role": "user", "content": "..."},
],
text\_format=ResearchPaperExtraction,
)
research\_paper = response.output\_parsed
```
```bash
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4o-2024-08-06",
"input": [
{
"role": "system",
"content": "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure."
},
{
"role": "user",
"content": "..."
}
],
"text": {
"format": {
"type": "json\_schema",
"name": "research\_paper\_extraction",
"schema": {
"type": "object",
"properties": {
"title": { "type": "string" },
"authors": {
"type": "array",
"items": { "type": "string" }
},
"abstract": { "type": "string" },
"keywords": {
"type": "array",
"items": { "type": "string" }
}
},
"required": ["title", "authors", "abstract", "keywords"],
"additionalProperties": false
},
"strict": true
}
}
}'
```

#### Example response
```json
{
"title": "Application of Quantum Algorithms in Interstellar Navigation: A New Frontier",
"authors": [
"Dr. Stella Voyager",
"Dr. Nova Star",
"Dr. Lyra Hunter"
],
"abstract": "This paper investigates the utilization of quantum algorithms to improve interstellar navigation systems. By leveraging quantum superposition and entanglement, our proposed navigation system can calculate optimal travel paths through space-time anomalies more efficiently than classical methods. Experimental simulations suggest a significant reduction in travel time and fuel consumption for interstellar missions.",
"keywords": [
"Quantum algorithms",
"interstellar navigation",
"space-time anomalies",
"quantum superposition",
"quantum entanglement",
"space travel"
]
}
```
UI generation

### UI Generation
You can generate valid HTML by representing it as recursive data structures with constraints, like enums.
Generating HTML using Structured Outputs
```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";
const openai = new OpenAI();
const UI = z.lazy(() =>
z.object({
type: z.enum(["div", "button", "header", "section", "field", "form"]),
label: z.string(),
children: z.array(UI),
attributes: z.array(
z.object({
name: z.string(),
value: z.string(),
})
),
})
);
const response = await openai.responses.parse({
model: "gpt-4o-2024-08-06",
input: [
{
role: "system",
content: "You are a UI generator AI. Convert the user input into a UI.",
},
{
role: "user",
content: "Make a User Profile Form",
},
],
text: {
format: zodTextFormat(UI, "ui"),
},
});
const ui = response.output\_parsed;
```
```python
from enum import Enum
from typing import List
from openai import OpenAI
from pydantic import BaseModel
client = OpenAI()
class UIType(str, Enum):
div = "div"
button = "button"
header = "header"
section = "section"
field = "field"
form = "form"
class Attribute(BaseModel):
name: str
value: str
class UI(BaseModel):
type: UIType
label: str
children: List["UI"]
attributes: List[Attribute]
UI.model\_rebuild() # This is required to enable recursive types
class Response(BaseModel):
ui: UI
response = client.responses.parse(
model="gpt-4o-2024-08-06",
input=[
{
"role": "system",
"content": "You are a UI generator AI. Convert the user input into a UI.",
},
{"role": "user", "content": "Make a User Profile Form"},
],
text\_format=Response,
)
ui = response.output\_parsed
```
```bash
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4o-2024-08-06",
"input": [
{
"role": "system",
"content": "You are a UI generator AI. Convert the user input into a UI."
},
{
"role": "user",
"content": "Make a User Profile Form"
}
],
"text": {
"format": {
"type": "json\_schema",
"name": "ui",
"description": "Dynamically generated UI",
"schema": {
"type": "object",
"properties": {
"type": {
"type": "string",
"description": "The type of the UI component",
"enum": ["div", "button", "header", "section", "field", "form"]
},
"label": {
"type": "string",
"description": "The label of the UI component, used for buttons or form fields"
},
"children": {
"type": "array",
"description": "Nested UI components",
"items": {"$ref": "#"}
},
"attributes": {
"type": "array",
"description": "Arbitrary attributes for the UI component, suitable for any element",
"items": {
"type": "object",
"properties": {
"name": {
"type": "string",
"description": "The name of the attribute, for example onClick or className"
},
"value": {
"type": "string",
"description": "The value of the attribute"
}
},
"required": ["name", "value"],
"additionalProperties": false
}
}
},
"required": ["type", "label", "children", "attributes"],
"additionalProperties": false
},
"strict": true
}
}
}'
```

#### Example response
```json
{
"type": "form",
"label": "User Profile Form",
"children": [
{
"type": "div",
"label": "",
"children": [
{
"type": "field",
"label": "First Name",
"children": [],
"attributes": [
{
"name": "type",
"value": "text"
},
{
"name": "name",
"value": "firstName"
},
{
"name": "placeholder",
"value": "Enter your first name"
}
]
},
{
"type": "field",
"label": "Last Name",
"children": [],
"attributes": [
{
"name": "type",
"value": "text"
},
{
"name": "name",
"value": "lastName"
},
{
"name": "placeholder",
"value": "Enter your last name"
}
]
}
],
"attributes": []
},
{
"type": "button",
"label": "Submit",
"children": [],
"attributes": [
{
"name": "type",
"value": "submit"
}
]
}
],
"attributes": [
{
"name": "method",
"value": "post"
},
{
"name": "action",
"value": "/submit-profile"
}
]
}
```
Moderation

### Moderation
You can classify inputs on multiple categories, which is a common way of doing moderation.
Moderation using Structured Outputs
```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";
const openai = new OpenAI();
const ContentCompliance = z.object({
is\_violating: z.boolean(),
category: z.enum(["violence", "sexual", "self\_harm"]).nullable(),
explanation\_if\_violating: z.string().nullable(),
});
const response = await openai.responses.parse({
model: "gpt-4o-2024-08-06",
input: [
{
"role": "system",
"content": "Determine if the user input violates specific guidelines and explain if they do."
},
{
"role": "user",
"content": "How do I prepare for a job interview?"
}
],
text: {
format: zodTextFormat(ContentCompliance, "content\_compliance"),
},
});
const compliance = response.output\_parsed;
```
```python
from enum import Enum
from typing import Optional
from openai import OpenAI
from pydantic import BaseModel
client = OpenAI()
class Category(str, Enum):
violence = "violence"
sexual = "sexual"
self\_harm = "self\_harm"
class ContentCompliance(BaseModel):
is\_violating: bool
category: Optional[Category]
explanation\_if\_violating: Optional[str]
response = client.responses.parse(
model="gpt-4o-2024-08-06",
input=[
{
"role": "system",
"content": "Determine if the user input violates specific guidelines and explain if they do.",
},
{"role": "user", "content": "How do I prepare for a job interview?"},
],
text\_format=ContentCompliance,
)
compliance = response.output\_parsed
```
```bash
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4o-2024-08-06",
"input": [
{
"role": "system",
"content": "Determine if the user input violates specific guidelines and explain if they do."
},
{
"role": "user",
"content": "How do I prepare for a job interview?"
}
],
"text": {
"format": {
"type": "json\_schema",
"name": "content\_compliance",
"description": "Determines if content is violating specific moderation rules",
"schema": {
"type": "object",
"properties": {
"is\_violating": {
"type": "boolean",
"description": "Indicates if the content is violating guidelines"
},
"category": {
"type": ["string", "null"],
"description": "Type of violation, if the content is violating guidelines. Null otherwise.",
"enum": ["violence", "sexual", "self\_harm"]
},
"explanation\_if\_violating": {
"type": ["string", "null"],
"description": "Explanation of why the content is violating"
}
},
"required": ["is\_violating", "category", "explanation\_if\_violating"],
"additionalProperties": false
},
"strict": true
}
}
}'
```

#### Example response
```json
{
"is\_violating": false,
"category": null,
"explanation\_if\_violating": null
}
```
How to use Structured Outputs with text.format
----------------------------------------------
Step 1: Define your schema
First you must design the JSON Schema that the model should be constrained to follow. See the [examples](/docs/guides/structured-outputs#examples) at the top of this guide for reference.
While Structured Outputs supports much of JSON Schema, some features are unavailable either for performance or technical reasons. See [here](/docs/guides/structured-outputs#supported-schemas) for more details.

#### Tips for your JSON Schema
To maximize the quality of model generations, we recommend the following:
\* Name keys clearly and intuitively
\* Create clear titles and descriptions for important keys in your structure
\* Create and use evals to determine the structure that works best for your use case
Step 2: Supply your schema in the API call
To use Structured Outputs, simply specify
```json
text: { format: { type: "json\_schema", "strict": true, "schema": … } }
```
For example:
```python
response = client.responses.create(
model="gpt-4o-2024-08-06",
input=[
{"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},
{"role": "user", "content": "how can I solve 8x + 7 = -23"}
],
text={
"format": {
"type": "json\_schema",
"name": "math\_response",
"schema": {
"type": "object",
"properties": {
"steps": {
"type": "array",
"items": {
"type": "object",
"properties": {
"explanation": {"type": "string"},
"output": {"type": "string"}
},
"required": ["explanation", "output"],
"additionalProperties": False
}
},
"final\_answer": {"type": "string"}
},
"required": ["steps", "final\_answer"],
"additionalProperties": False
},
"strict": True
}
}
)
print(response.output\_text)
```
```javascript
const response = await openai.responses.create({
model: "gpt-4o-2024-08-06",
input: [
{ role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },
{ role: "user", content: "how can I solve 8x + 7 = -23" }
],
text: {
format: {
type: "json\_schema",
name: "math\_response",
schema: {
type: "object",
properties: {
steps: {
type: "array",
items: {
type: "object",
properties: {
explanation: { type: "string" },
output: { type: "string" }
},
required: ["explanation", "output"],
additionalProperties: false
}
},
final\_answer: { type: "string" }
},
required: ["steps", "final\_answer"],
additionalProperties: false
},
strict: true
}
}
});
console.log(response.output\_text);
```
```bash
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "gpt-4o-2024-08-06",
"input": [
{
"role": "system",
"content": "You are a helpful math tutor. Guide the user through the solution step by step."
},
{
"role": "user",
"content": "how can I solve 8x + 7 = -23"
}
],
"text": {
"format": {
"type": "json\_schema",
"name": "math\_response",
"schema": {
"type": "object",
"properties": {
"steps": {
"type": "array",
"items": {
"type": "object",
"properties": {
"explanation": { "type": "string" },
"output": { "type": "string" }
},
"required": ["explanation", "output"],
"additionalProperties": false
}
},
"final\_answer": { "type": "string" }
},
"required": ["steps", "final\_answer"],
"additionalProperties": false
},
"strict": true
}
}
}'
```
\*\*Note:\*\* the first request you make with any schema will have additional latency as our API processes the schema, but subsequent requests with the same schema will not have additional latency.
Step 3: Handle edge cases
In some cases, the model might not generate a valid response that matches the provided JSON schema.
This can happen in the case of a refusal, if the model refuses to answer for safety reasons, or if for example you reach a max tokens limit and the response is incomplete.
```javascript
try {
const response = await openai.responses.create({
model: "gpt-4o-2024-08-06",
input: [{
role: "system",
content: "You are a helpful math tutor. Guide the user through the solution step by step.",
},
{
role: "user",
content: "how can I solve 8x + 7 = -23"
},
],
max\_output\_tokens: 50,
text: {
format: {
type: "json\_schema",
name: "math\_response",
schema: {
type: "object",
properties: {
steps: {
type: "array",
items: {
type: "object",
properties: {
explanation: {
type: "string"
},
output: {
type: "string"
},
},
required: ["explanation", "output"],
additionalProperties: false,
},
},
final\_answer: {
type: "string"
},
},
required: ["steps", "final\_answer"],
additionalProperties: false,
},
strict: true,
},
}
});
if (response.status === "incomplete" && response.incomplete\_details.reason === "max\_output\_tokens") {
// Handle the case where the model did not return a complete response
throw new Error("Incomplete response");
}
const math\_response = response.output[0].content[0];
if (math\_response.type === "refusal") {
// handle refusal
console.log(math\_response.refusal);
} else if (math\_response.type === "output\_text") {
console.log(math\_response.text);
} else {
throw new Error("No response content");
}
} catch (e) {
// Handle edge cases
console.error(e);
}
```
```python
try:
response = client.responses.create(
model="gpt-4o-2024-08-06",
input=[
{
"role": "system",
"content": "You are a helpful math tutor. Guide the user through the solution step by step.",
},
{"role": "user", "content": "how can I solve 8x + 7 = -23"},
],
text={
"format": {
"type": "json\_schema",
"name": "math\_response",
"strict": True,
"schema": {
"type": "object",
"properties": {
"steps": {
"type": "array",
"items": {
"type": "object",
"properties": {
"explanation": {"type": "string"},
"output": {"type": "string"},
},
"required": ["explanation", "output"],
"additionalProperties": False,
},
},
"final\_answer": {"type": "string"},
},
"required": ["steps", "final\_answer"],
"additionalProperties": False,
},
"strict": True,
},
},
)
except Exception as e:

# handle errors like finish\_reason, refusal, content\_filter, etc.
pass
```

### Refusals with Structured Outputs
When using Structured Outputs with user-generated input, OpenAI models may occasionally refuse to fulfill the request for safety reasons. Since a refusal does not necessarily follow the schema you have supplied in `response\_format`, the API response will include a new field called `refusal` to indicate that the model refused to fulfill the request.
When the `refusal` property appears in your output object, you might present the refusal in your UI, or include conditional logic in code that consumes the response to handle the case of a refused request.
```python
class Step(BaseModel):
explanation: str
output: str
class MathReasoning(BaseModel):
steps: list[Step]
final\_answer: str
completion = client.chat.completions.parse(
model="gpt-4o-2024-08-06",
messages=[
{"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},
{"role": "user", "content": "how can I solve 8x + 7 = -23"}
],
response\_format=MathReasoning,
)
math\_reasoning = completion.choices[0].message

# If the model refuses to respond, you will get a refusal message
if (math\_reasoning.refusal):
print(math\_reasoning.refusal)
else:
print(math\_reasoning.parsed)
```
```javascript
const Step = z.object({
explanation: z.string(),
output: z.string(),
});
const MathReasoning = z.object({
steps: z.array(Step),
final\_answer: z.string(),
});
const completion = await openai.chat.completions.parse({
model: "gpt-4o-2024-08-06",
messages: [
{ role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },
{ role: "user", content: "how can I solve 8x + 7 = -23" },
],
response\_format: zodResponseFormat(MathReasoning, "math\_reasoning"),
});
const math\_reasoning = completion.choices[0].message
// If the model refuses to respond, you will get a refusal message
if (math\_reasoning.refusal) {
console.log(math\_reasoning.refusal);
} else {
console.log(math\_reasoning.parsed);
}
```
The API response from a refusal will look something like this:
```json
{
"id": "resp\_1234567890",
"object": "response",
"created\_at": 1721596428,
"status": "completed",
"error": null,
"incomplete\_details": null,
"input": [],
"instructions": null,
"max\_output\_tokens": null,
"model": "gpt-4o-2024-08-06",
"output": [{
"id": "msg\_1234567890",
"type": "message",
"role": "assistant",
"content": [
{
"type": "refusal",
"refusal": "I'm sorry, I cannot assist with that request."
}
]
}],
"usage": {
"input\_tokens": 81,
"output\_tokens": 11,
"total\_tokens": 92,
"output\_tokens\_details": {
"reasoning\_tokens": 0,
}
},
}
```

### Tips and best practices

#### Handling user-generated input
If your application is using user-generated input, make sure your prompt includes instructions on how to handle situations where the input cannot result in a valid response.
The model will always try to adhere to the provided schema, which can result in hallucinations if the input is completely unrelated to the schema.
You could include language in your prompt to specify that you want to return empty parameters, or a specific sentence, if the model detects that the input is incompatible with the task.

#### Handling mistakes
Structured Outputs can still contain mistakes. If you see mistakes, try adjusting your instructions, providing examples in the system instructions, or splitting tasks into simpler subtasks. Refer to the [prompt engineering guide](/docs/guides/prompt-engineering) for more guidance on how to tweak your inputs.

#### Avoid JSON schema divergence
To prevent your JSON Schema and corresponding types in your programming language from diverging, we strongly recommend using the native Pydantic/zod sdk support.
If you prefer to specify the JSON schema directly, you could add CI rules that flag when either the JSON schema or underlying data objects are edited, or add a CI step that auto-generates the JSON Schema from type definitions (or vice-versa).
Streaming
---------
You can use streaming to process model responses or function call arguments as they are being generated, and parse them as structured data.
That way, you don't have to wait for the entire response to complete before handling it. This is particularly useful if you would like to display JSON fields one by one, or handle function call arguments as soon as they are available.
We recommend relying on the SDKs to handle streaming with Structured Outputs.
```python
from typing import List
from openai import OpenAI
from pydantic import BaseModel
class EntitiesModel(BaseModel):
attributes: List[str]
colors: List[str]
animals: List[str]
client = OpenAI()
with client.responses.stream(
model="gpt-4.1",
input=[
{"role": "system", "content": "Extract entities from the input text"},
{
"role": "user",
"content": "The quick brown fox jumps over the lazy dog with piercing blue eyes",
},
],
text\_format=EntitiesModel,
) as stream:
for event in stream:
if event.type == "response.refusal.delta":
print(event.delta, end="")
elif event.type == "response.output\_text.delta":
print(event.delta, end="")
elif event.type == "response.error":
print(event.error, end="")
elif event.type == "response.completed":
print("Completed")

# print(event.response.output)
final\_response = stream.get\_final\_response()
print(final\_response)
```
```javascript
import { OpenAI } from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";
const EntitiesSchema = z.object({
attributes: z.array(z.string()),
colors: z.array(z.string()),
animals: z.array(z.string()),
});
const openai = new OpenAI();
const stream = openai.responses
.stream({
model: "gpt-4.1",
input: [
{ role: "user", content: "What's the weather like in Paris today?" },
],
text: {
format: zodTextFormat(EntitiesSchema, "entities"),
},
})
.on("response.refusal.delta", (event) => {
process.stdout.write(event.delta);
})
.on("response.output\_text.delta", (event) => {
process.stdout.write(event.delta);
})
.on("response.output\_text.done", () => {
process.stdout.write("\n");
})
.on("response.error", (event) => {
console.error(event.error);
});
const result = await stream.finalResponse();
console.log(result);
```
Supported schemas
-----------------
Structured Outputs supports a subset of the [JSON Schema](https://json-schema.org/docs) language.

#### Supported types
The following types are supported for Structured Outputs:
\* String
\* Number
\* Boolean
\* Integer
\* Object
\* Array
\* Enum
\* anyOf

#### Supported properties
In addition to specifying the type of a property, you can specify a selection of additional constraints:
\*\*Supported `string` properties:\*\*
\* `pattern` — A regular expression that the string must match.
\* `format` — Predefined formats for strings. Currently supported:
\* `date-time`
\* `time`
\* `date`
\* `duration`
\* `email`
\* `hostname`
\* `ipv4`
\* `ipv6`
\* `uuid`
\*\*Supported `number` properties:\*\*
\* `multipleOf` — The number must be a multiple of this value.
\* `maximum` — The number must be less than or equal to this value.
\* `exclusiveMaximum` — The number must be less than this value.
\* `minimum` — The number must be greater than or equal to this value.
\* `exclusiveMinimum` — The number must be greater than this value.
\*\*Supported `array` properties:\*\*
\* `minItems` — The array must have at least this many items.
\* `maxItems` — The array must have at most this many items.
Here are some examples on how you can use these type restrictions:
String Restrictions
```json
{
"name": "user\_data",
"strict": true,
"schema": {
"type": "object",
"properties": {
"name": {
"type": "string",
"description": "The name of the user"
},
"username": {
"type": "string",
"description": "The username of the user. Must start with @",
"pattern": "^@[a-zA-Z0-9\_]+$"
},
"email": {
"type": "string",
"description": "The email of the user",
"format": "email"
}
},
"additionalProperties": false,
"required": [
"name", "username", "email"
]
}
}
```
Number Restrictions
```json
{
"name": "weather\_data",
"strict": true,
"schema": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "The location to get the weather for"
},
"unit": {
"type": ["string", "null"],
"description": "The unit to return the temperature in",
"enum": ["F", "C"]
},
"value": {
"type": "number",
"description": "The actual temperature value in the location",
"minimum": -130,
"maximum": 130
}
},
"additionalProperties": false,
"required": [
"location", "unit", "value"
]
}
}
```
Note these constraints are [not yet supported for fine-tuned models](#some-type-specific-keywords-are-not-yet-supported).

#### Root objects must not be `anyOf` and must be an object
Note that the root level object of a schema must be an object, and not use `anyOf`. A pattern that appears in Zod (as one example) is using a discriminated union, which produces an `anyOf` at the top level. So code such as the following won't work:
```javascript
import { z } from 'zod';
import { zodResponseFormat } from 'openai/helpers/zod';
const BaseResponseSchema = z.object({/\* ... \*/});
const UnsuccessfulResponseSchema = z.object({/\* ... \*/});
const finalSchema = z.discriminatedUnion('status', [
BaseResponseSchema,
UnsuccessfulResponseSchema,
]);
// Invalid JSON Schema for Structured Outputs
const json = zodResponseFormat(finalSchema, 'final\_schema');
```

#### All fields must be `required`
To use Structured Outputs, all fields or function parameters must be specified as `required`.
```json
{
"name": "get\_weather",
"description": "Fetches the weather in the given location",
"strict": true,
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "The location to get the weather for"
},
"unit": {
"type": "string",
"description": "The unit to return the temperature in",
"enum": ["F", "C"]
}
},
"additionalProperties": false,
"required": ["location", "unit"]
}
}
```
Although all fields must be required (and the model will return a value for each parameter), it is possible to emulate an optional parameter by using a union type with `null`.
```json
{
"name": "get\_weather",
"description": "Fetches the weather in the given location",
"strict": true,
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "The location to get the weather for"
},
"unit": {
"type": ["string", "null"],
"description": "The unit to return the temperature in",
"enum": ["F", "C"]
}
},
"additionalProperties": false,
"required": [
"location", "unit"
]
}
}
```

#### Objects have limitations on nesting depth and size
A schema may have up to 100 object properties total, with up to 5 levels of nesting.

#### Limitations on total string size
In a schema, total string length of all property names, definition names, enum values, and const values cannot exceed 15,000 characters.

#### Limitations on enum size
A schema may have up to 500 enum values across all enum properties.
For a single enum property with string values, the total string length of all enum values cannot exceed 7,500 characters when there are more than 250 enum values.

#### `additionalProperties: false` must always be set in objects
`additionalProperties` controls whether it is allowable for an object to contain additional keys / values that were not defined in the JSON Schema.
Structured Outputs only supports generating specified keys / values, so we require developers to set `additionalProperties: false` to opt into Structured Outputs.
```json
{
"name": "get\_weather",
"description": "Fetches the weather in the given location",
"strict": true,
"schema": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "The location to get the weather for"
},
"unit": {
"type": "string",
"description": "The unit to return the temperature in",
"enum": ["F", "C"]
}
},
"additionalProperties": false,
"required": [
"location", "unit"
]
}
}
```

#### Key ordering
When using Structured Outputs, outputs will be produced in the same order as the ordering of keys in the schema.

#### Some type-specific keywords are not yet supported
\* \*\*Composition:\*\* `allOf`, `not`, `dependentRequired`, `dependentSchemas`, `if`, `then`, `else`
For fine-tuned models, we additionally do not support the following:
\* \*\*For strings:\*\* `minLength`, `maxLength`, `pattern`, `format`
\* \*\*For numbers:\*\* `minimum`, `maximum`, `multipleOf`
\* \*\*For objects:\*\* `patternProperties`
\* \*\*For arrays:\*\* `minItems`, `maxItems`
If you turn on Structured Outputs by supplying `strict: true` and call the API with an unsupported JSON Schema, you will receive an error.

#### For `anyOf`, the nested schemas must each be a valid JSON Schema per this subset
Here's an example supported anyOf schema:
```json
{
"type": "object",
"properties": {
"item": {
"anyOf": [
{
"type": "object",
"description": "The user object to insert into the database",
"properties": {
"name": {
"type": "string",
"description": "The name of the user"
},
"age": {
"type": "number",
"description": "The age of the user"
}
},
"additionalProperties": false,
"required": [
"name",
"age"
]
},
{
"type": "object",
"description": "The address object to insert into the database",
"properties": {
"number": {
"type": "string",
"description": "The number of the address. Eg. for 123 main st, this would be 123"
},
"street": {
"type": "string",
"description": "The street name. Eg. for 123 main st, this would be main st"
},
"city": {
"type": "string",
"description": "The city of the address"
}
},
"additionalProperties": false,
"required": [
"number",
"street",
"city"
]
}
]
}
},
"additionalProperties": false,
"required": [
"item"
]
}
```

#### Definitions are supported
You can use definitions to define subschemas which are referenced throughout your schema. The following is a simple example.
```json
{
"type": "object",
"properties": {
"steps": {
"type": "array",
"items": {
"$ref": "#/$defs/step"
}
},
"final\_answer": {
"type": "string"
}
},
"$defs": {
"step": {
"type": "object",
"properties": {
"explanation": {
"type": "string"
},
"output": {
"type": "string"
}
},
"required": [
"explanation",
"output"
],
"additionalProperties": false
}
},
"required": [
"steps",
"final\_answer"
],
"additionalProperties": false
}
```

#### Recursive schemas are supported
Sample recursive schema using `#` to indicate root recursion.
```json
{
"name": "ui",
"description": "Dynamically generated UI",
"strict": true,
"schema": {
"type": "object",
"properties": {
"type": {
"type": "string",
"description": "The type of the UI component",
"enum": ["div", "button", "header", "section", "field", "form"]
},
"label": {
"type": "string",
"description": "The label of the UI component, used for buttons or form fields"
},
"children": {
"type": "array",
"description": "Nested UI components",
"items": {
"$ref": "#"
}
},
"attributes": {
"type": "array",
"description": "Arbitrary attributes for the UI component, suitable for any element",
"items": {
"type": "object",
"properties": {
"name": {
"type": "string",
"description": "The name of the attribute, for example onClick or className"
},
"value": {
"type": "string",
"description": "The value of the attribute"
}
},
"additionalProperties": false,
"required": ["name", "value"]
}
}
},
"required": ["type", "label", "children", "attributes"],
"additionalProperties": false
}
}
```
Sample recursive schema using explicit recursion:
```json
{
"type": "object",
"properties": {
"linked\_list": {
"$ref": "#/$defs/linked\_list\_node"
}
},
"$defs": {
"linked\_list\_node": {
"type": "object",
"properties": {
"value": {
"type": "number"
},
"next": {
"anyOf": [
{
"$ref": "#/$defs/linked\_list\_node"
},
{
"type": "null"
}
]
}
},
"additionalProperties": false,
"required": [
"next",
"value"
]
}
},
"additionalProperties": false,
"required": [
"linked\_list"
]
}
```
JSON mode
---------
JSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify. We recommend you use Structured Outputs if it is supported for your use case.
When JSON mode is turned on, the model's output is ensured to be valid JSON, except for in some edge cases that you should detect and handle appropriately.
To turn on JSON mode with the Responses API you can set the `text.format` to `{ "type": "json\_object" }`. If you are using function calling, JSON mode is always turned on.
Important notes:
\* When using JSON mode, you must always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string "JSON" does not appear somewhere in the context.
\* JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors. You should use Structured Outputs to ensure it matches your schema, or if that is not possible, you should use a validation library and potentially retries to ensure that the output matches your desired schema.
\* Your application must detect and handle the edge cases that can result in the model output not being a complete JSON object (see below)
Handling edge cases
```javascript
const we\_did\_not\_specify\_stop\_tokens = true;
try {
const response = await openai.responses.create({
model: "gpt-3.5-turbo-0125",
input: [
{
role: "system",
content: "You are a helpful assistant designed to output JSON.",
},
{ role: "user", content: "Who won the world series in 2020? Please respond in the format {winner: ...}" },
],
text: { format: { type: "json\_object" } },
});
// Check if the conversation was too long for the context window, resulting in incomplete JSON
if (response.status === "incomplete" && response.incomplete\_details.reason === "max\_output\_tokens") {
// your code should handle this error case
}
// Check if the OpenAI safety system refused the request and generated a refusal instead
if (response.output[0].content[0].type === "refusal") {
// your code should handle this error case
// In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing
console.log(response.output[0].content[0].refusal)
}
// Check if the model's output included restricted content, so the generation of JSON was halted and may be partial
if (response.status === "incomplete" && response.incomplete\_details.reason === "content\_filter") {
// your code should handle this error case
}
if (response.status === "completed") {
// In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"
if (we\_did\_not\_specify\_stop\_tokens) {
// If you didn't specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object
// This will parse successfully and should now contain {"winner": "Los Angeles Dodgers"}
console.log(JSON.parse(response.output\_text))
} else {
// Check if the response.output\_text ends with one of your stop tokens and handle appropriately
}
}
} catch (e) {
// Your code should handle errors here, for example a network error calling the API
console.error(e)
}
```
```python
we\_did\_not\_specify\_stop\_tokens = True
try:
response = client.responses.create(
model="gpt-3.5-turbo-0125",
input=[
{"role": "system", "content": "You are a helpful assistant designed to output JSON."},
{"role": "user", "content": "Who won the world series in 2020? Please respond in the format {winner: ...}"}
],
text={"format": {"type": "json\_object"}}
)

# Check if the conversation was too long for the context window, resulting in incomplete JSON
if response.status == "incomplete" and response.incomplete\_details.reason == "max\_output\_tokens":

# your code should handle this error case
pass

# Check if the OpenAI safety system refused the request and generated a refusal instead
if response.output[0].content[0].type == "refusal":

# your code should handle this error case

# In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing
print(response.output[0].content[0]["refusal"])

# Check if the model's output included restricted content, so the generation of JSON was halted and may be partial
if response.status == "incomplete" and response.incomplete\_details.reason == "content\_filter":

# your code should handle this error case
pass
if response.status == "completed":

# In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"
if we\_did\_not\_specify\_stop\_tokens:

# If you didn't specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object

# This will parse successfully and should now contain "{"winner": "Los Angeles Dodgers"}"
print(response.output\_text)
else:

# Check if the response.output\_text ends with one of your stop tokens and handle appropriately
pass
except Exception as e:

# Your code should handle errors here, for example a network error calling the API
print(e)
```
Resources
---------
To learn more about Structured Outputs, we recommend browsing the following resources:
\* Check out our [introductory cookbook](https://cookbook.openai.com/examples/structured\_outputs\_intro) on Structured Outputs
\* Learn [how to build multi-agent systems](https://cookbook.openai.com/examples/structured\_outputs\_multi\_agent) with Structured Outputs
Was this page useful?


## Imported snippet – 2025-07-03 11:47:53

Function calling
================
Enable models to fetch data and take actions.
\*\*Function calling\*\* provides a powerful and flexible way for OpenAI models to interface with your code or external services. This guide will explain how to connect the models to your own custom code to fetch data or take action.
Get weather
Function calling example with get\\_weather function
```python
from openai import OpenAI
client = OpenAI()
tools = [{
"type": "function",
"name": "get\_weather",
"description": "Get current temperature for a given location.",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City and country e.g. Bogotá, Colombia"
}
},
"required": [
"location"
],
"additionalProperties": False
}
}]
response = client.responses.create(
model="gpt-4.1",
input=[{"role": "user", "content": "What is the weather like in Paris today?"}],
tools=tools
)
print(response.output)
```
```javascript
import { OpenAI } from "openai";
const openai = new OpenAI();
const tools = [{
"type": "function",
"name": "get\_weather",
"description": "Get current temperature for a given location.",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City and country e.g. Bogotá, Colombia"
}
},
"required": [
"location"
],
"additionalProperties": false
}
}];
const response = await openai.responses.create({
model: "gpt-4.1",
input: [{ role: "user", content: "What is the weather like in Paris today?" }],
tools,
});
console.log(response.output);
```
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"input": "What is the weather like in Paris today?",
"tools": [
{
"type": "function",
"name": "get\_weather",
"description": "Get current temperature for a given location.",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City and country e.g. Bogotá, Colombia"
}
},
"required": [
"location"
],
"additionalProperties": false
}
}
]
}'
```
Output
```json
[{
"type": "function\_call",
"id": "fc\_12345xyz",
"call\_id": "call\_12345xyz",
"name": "get\_weather",
"arguments": "{\"location\":\"Paris, France\"}"
}]
```
Send email
Function calling example with send\\_email function
```python
from openai import OpenAI
client = OpenAI()
tools = [{
"type": "function",
"name": "send\_email",
"description": "Send an email to a given recipient with a subject and message.",
"parameters": {
"type": "object",
"properties": {
"to": {
"type": "string",
"description": "The recipient email address."
},
"subject": {
"type": "string",
"description": "Email subject line."
},
"body": {
"type": "string",
"description": "Body of the email message."
}
},
"required": [
"to",
"subject",
"body"
],
"additionalProperties": False
}
}]
response = client.responses.create(
model="gpt-4.1",
input=[{"role": "user", "content": "Can you send an email to ilan@example.com and katia@example.com saying hi?"}],
tools=tools
)
print(response.output)
```
```javascript
import { OpenAI } from "openai";
const openai = new OpenAI();
const tools = [{
"type": "function",
"name": "send\_email",
"description": "Send an email to a given recipient with a subject and message.",
"parameters": {
"type": "object",
"properties": {
"to": {
"type": "string",
"description": "The recipient email address."
},
"subject": {
"type": "string",
"description": "Email subject line."
},
"body": {
"type": "string",
"description": "Body of the email message."
}
},
"required": [
"to",
"subject",
"body"
],
"additionalProperties": false
}
}];
const response = await openai.responses.create({
model: "gpt-4.1",
input: [{ role: "user", content: "Can you send an email to ilan@example.com and katia@example.com saying hi?" }],
tools,
});
console.log(response.output);
```
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"input": "Can you send an email to ilan@example.com and katia@example.com saying hi?",
"tools": [
{
"type": "function",
"name": "send\_email",
"description": "Send an email to a given recipient with a subject and message.",
"parameters": {
"type": "object",
"properties": {
"to": {
"type": "string",
"description": "The recipient email address."
},
"subject": {
"type": "string",
"description": "Email subject line."
},
"body": {
"type": "string",
"description": "Body of the email message."
}
},
"required": [
"to",
"subject",
"body"
],
"additionalProperties": false
}
}
]
}'
```
Output
```json
[
{
"type": "function\_call",
"id": "fc\_12345xyz",
"call\_id": "call\_9876abc",
"name": "send\_email",
"arguments": "{\"to\":\"ilan@example.com\",\"subject\":\"Hello!\",\"body\":\"Just wanted to say hi\"}"
},
{
"type": "function\_call",
"id": "fc\_12345xyz",
"call\_id": "call\_9876abc",
"name": "send\_email",
"arguments": "{\"to\":\"katia@example.com\",\"subject\":\"Hello!\",\"body\":\"Just wanted to say hi\"}"
}
]
```
Search knowledge base
Function calling example with search\\_knowledge\\_base function
```python
from openai import OpenAI
client = OpenAI()
tools = [{
"type": "function",
"name": "search\_knowledge\_base",
"description": "Query a knowledge base to retrieve relevant info on a topic.",
"parameters": {
"type": "object",
"properties": {
"query": {
"type": "string",
"description": "The user question or search query."
},
"options": {
"type": "object",
"properties": {
"num\_results": {
"type": "number",
"description": "Number of top results to return."
},
"domain\_filter": {
"type": [
"string",
"null"
],
"description": "Optional domain to narrow the search (e.g. 'finance', 'medical'). Pass null if not needed."
},
"sort\_by": {
"type": [
"string",
"null"
],
"enum": [
"relevance",
"date",
"popularity",
"alphabetical"
],
"description": "How to sort results. Pass null if not needed."
}
},
"required": [
"num\_results",
"domain\_filter",
"sort\_by"
],
"additionalProperties": False
}
},
"required": [
"query",
"options"
],
"additionalProperties": False
}
}]
response = client.responses.create(
model="gpt-4.1",
input=[{"role": "user", "content": "Can you find information about ChatGPT in the AI knowledge base?"}],
tools=tools
)
print(response.output)
```
```javascript
import { OpenAI } from "openai";
const openai = new OpenAI();
const tools = [{
"type": "function",
"name": "search\_knowledge\_base",
"description": "Query a knowledge base to retrieve relevant info on a topic.",
"parameters": {
"type": "object",
"properties": {
"query": {
"type": "string",
"description": "The user question or search query."
},
"options": {
"type": "object",
"properties": {
"num\_results": {
"type": "number",
"description": "Number of top results to return."
},
"domain\_filter": {
"type": [
"string",
"null"
],
"description": "Optional domain to narrow the search (e.g. 'finance', 'medical'). Pass null if not needed."
},
"sort\_by": {
"type": [
"string",
"null"
],
"enum": [
"relevance",
"date",
"popularity",
"alphabetical"
],
"description": "How to sort results. Pass null if not needed."
}
},
"required": [
"num\_results",
"domain\_filter",
"sort\_by"
],
"additionalProperties": false
}
},
"required": [
"query",
"options"
],
"additionalProperties": false
}
}];
const response = await openai.responses.create({
model: "gpt-4.1",
input: [{ role: "user", content: "Can you find information about ChatGPT in the AI knowledge base?" }],
tools,
});
console.log(response.output);
```
```bash
curl https://api.openai.com/v1/responses \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"input": "Can you find information about ChatGPT in the AI knowledge base?",
"tools": [
{
"type": "function",
"name": "search\_knowledge\_base",
"description": "Query a knowledge base to retrieve relevant info on a topic.",
"parameters": {
"type": "object",
"properties": {
"query": {
"type": "string",
"description": "The user question or search query."
},
"options": {
"type": "object",
"properties": {
"num\_results": {
"type": "number",
"description": "Number of top results to return."
},
"domain\_filter": {
"type": [
"string",
"null"
],
"description": "Optional domain to narrow the search (e.g. 'finance', 'medical'). Pass null if not needed."
},
"sort\_by": {
"type": [
"string",
"null"
],
"enum": [
"relevance",
"date",
"popularity",
"alphabetical"
],
"description": "How to sort results. Pass null if not needed."
}
},
"required": [
"num\_results",
"domain\_filter",
"sort\_by"
],
"additionalProperties": false
}
},
"required": [
"query",
"options"
],
"additionalProperties": false
}
}
]
}'
```
Output
```json
[{
"type": "function\_call",
"id": "fc\_12345xyz",
"call\_id": "call\_4567xyz",
"name": "search\_knowledge\_base",
"arguments": "{\"query\":\"What is ChatGPT?\",\"options\":{\"num\_results\":3,\"domain\_filter\":null,\"sort\_by\":\"relevance\"}}"
}]
```
Experiment with function calling and [generate function schemas](/docs/guides/prompt-generation) in the [Playground](/playground)!
Overview
--------
You can give the model access to your own custom code through \*\*function calling\*\*. Based on the system prompt and messages, the model may decide to call these functions — \*\*instead of (or in addition to) generating text or audio\*\*.
You'll then execute the function code, send back the results, and the model will incorporate them into its final response.
![Function Calling Diagram Steps](https://cdn.openai.com/API/docs/images/function-calling-diagram-steps.png)
Function calling has two primary use cases:
|||
|---|---|
|Fetching Data|Retrieve up-to-date information to incorporate into the model's response (RAG). Useful for searching knowledge bases and retrieving specific data from APIs (e.g. current weather data).|
|Taking Action|Perform actions like submitting a form, calling APIs, modifying application state (UI/frontend or backend), or taking agentic workflow actions (like handing off the conversation).|

### Sample function
Let's look at the steps to allow a model to use a real `get\_weather` function defined below:
Sample get\\_weather function implemented in your codebase
```python
import requests
def get\_weather(latitude, longitude):
response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature\_2m,wind\_speed\_10m&hourly=temperature\_2m,relative\_humidity\_2m,wind\_speed\_10m")
data = response.json()
return data['current']['temperature\_2m']
```
```javascript
async function getWeather(latitude, longitude) {
const response = await fetch(`https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature\_2m,wind\_speed\_10m&hourly=temperature\_2m,relative\_humidity\_2m,wind\_speed\_10m`);
const data = await response.json();
return data.current.temperature\_2m;
}
```
Unlike the diagram earlier, this function expects precise `latitude` and `longitude` instead of a general `location` parameter. (However, our models can automatically determine the coordinates for many locations!)

### Function calling steps
\* \*\*Call model with [functions defined](#defining-functions)\*\* – along with your system and user messages.
Step 1: Call model with get\\_weather tool defined
```python
from openai import OpenAI
import json
client = OpenAI()
tools = [{
"type": "function",
"name": "get\_weather",
"description": "Get current temperature for provided coordinates in celsius.",
"parameters": {
"type": "object",
"properties": {
"latitude": {"type": "number"},
"longitude": {"type": "number"}
},
"required": ["latitude", "longitude"],
"additionalProperties": False
},
"strict": True
}]
input\_messages = [{"role": "user", "content": "What's the weather like in Paris today?"}]
response = client.responses.create(
model="gpt-4.1",
input=input\_messages,
tools=tools,
)
```
```javascript
import { OpenAI } from "openai";
const openai = new OpenAI();
const tools = [{
type: "function",
name: "get\_weather",
description: "Get current temperature for provided coordinates in celsius.",
parameters: {
type: "object",
properties: {
latitude: { type: "number" },
longitude: { type: "number" }
},
required: ["latitude", "longitude"],
additionalProperties: false
},
strict: true
}];
const input = [
{
role: "user",
content: "What's the weather like in Paris today?"
}
];
const response = await openai.responses.create({
model: "gpt-4.1",
input,
tools,
});
```
\* \*\*Model decides to call function(s)\*\* – model returns the \*\*name\*\* and \*\*input arguments\*\*.
response.output
```json
[{
"type": "function\_call",
"id": "fc\_12345xyz",
"call\_id": "call\_12345xyz",
"name": "get\_weather",
"arguments": "{\"latitude\":48.8566,\"longitude\":2.3522}"
}]
```
\* \*\*Execute function code\*\* – parse the model's response and [handle function calls](#handling-function-calls).
Step 3: Execute get\\_weather function
```python
tool\_call = response.output[0]
args = json.loads(tool\_call.arguments)
result = get\_weather(args["latitude"], args["longitude"])
```
```javascript
const toolCall = response.output[0];
const args = JSON.parse(toolCall.arguments);
const result = await getWeather(args.latitude, args.longitude);
```
\* \*\*Supply model with results\*\* – so it can incorporate them into its final response.
Step 4: Supply result and call model again
```python
input\_messages.append(tool\_call) # append model's function call message
input\_messages.append({ # append result message
"type": "function\_call\_output",
"call\_id": tool\_call.call\_id,
"output": str(result)
})
response\_2 = client.responses.create(
model="gpt-4.1",
input=input\_messages,
tools=tools,
)
print(response\_2.output\_text)
```
```javascript
input.push(toolCall); // append model's function call message
input.push({ // append result message
type: "function\_call\_output",
call\_id: toolCall.call\_id,
output: result.toString()
});
const response2 = await openai.responses.create({
model: "gpt-4.1",
input,
tools,
store: true,
});
console.log(response2.output\_text)
```
\* \*\*Model responds\*\* – incorporating the result in its output.
response\\_2.output\\_text
```json
"The current temperature in Paris is 14°C (57.2°F)."
```
Defining functions
------------------
Functions can be set in the `tools` parameter of each API request.
A function is defined by its schema, which informs the model what it does and what input arguments it expects. It comprises the following fields:
|Field|Description|
|---|---|
|type|This should always be function|
|name|The function's name (e.g. get\_weather)|
|description|Details on when and how to use the function|
|parameters|JSON schema defining the function's input arguments|
|strict|Whether to enforce strict mode for the function call|
Take a look at this example or generate your own below (or in our [Playground](/playground)).
```json
{
"type": "function",
"name": "get\_weather",
"description": "Retrieves current weather for the given location.",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City and country e.g. Bogotá, Colombia"
},
"units": {
"type": "string",
"enum": [
"celsius",
"fahrenheit"
],
"description": "Units the temperature will be returned in."
}
},
"required": [
"location",
"units"
],
"additionalProperties": false
},
"strict": true
}
```
Because the `parameters` are defined by a [JSON schema](https://json-schema.org/), you can leverage many of its rich features like property types, enums, descriptions, nested objects, and, recursive objects.

### Best practices for defining functions
1. \*\*Write clear and detailed function names, parameter descriptions, and instructions.\*\*
\* \*\*Explicitly describe the purpose of the function and each parameter\*\* (and its format), and what the output represents.
\* \*\*Use the system prompt to describe when (and when not) to use each function.\*\* Generally, tell the model \_exactly\_ what to do.
\* \*\*Include examples and edge cases\*\*, especially to rectify any recurring failures. (\*\*Note:\*\* Adding examples may hurt performance for [reasoning models](/docs/guides/reasoning).)
2. \*\*Apply software engineering best practices.\*\*
\* \*\*Make the functions obvious and intuitive\*\*. ([principle of least surprise](https://en.wikipedia.org/wiki/Principle\_of\_least\_astonishment))
\* \*\*Use enums\*\* and object structure to make invalid states unrepresentable. (e.g. `toggle\_light(on: bool, off: bool)` allows for invalid calls)
\* \*\*Pass the intern test.\*\* Can an intern/human correctly use the function given nothing but what you gave the model? (If not, what questions do they ask you? Add the answers to the prompt.)
3. \*\*Offload the burden from the model and use code where possible.\*\*
\* \*\*Don't make the model fill arguments you already know.\*\* For example, if you already have an `order\_id` based on a previous menu, don't have an `order\_id` param – instead, have no params `submit\_refund()` and pass the `order\_id` with code.
\* \*\*Combine functions that are always called in sequence.\*\* For example, if you always call `mark\_location()` after `query\_location()`, just move the marking logic into the query function call.
4. \*\*Keep the number of functions small for higher accuracy.\*\*
\* \*\*Evaluate your performance\*\* with different numbers of functions.
\* \*\*Aim for fewer than 20 functions\*\* at any one time, though this is just a soft suggestion.
5. \*\*Leverage OpenAI resources.\*\*
\* \*\*Generate and iterate on function schemas\*\* in the [Playground](/playground).
\* \*\*Consider [fine-tuning](https://platform.openai.com/docs/guides/fine-tuning) to increase function calling accuracy\*\* for large numbers of functions or difficult tasks. ([cookbook](https://cookbook.openai.com/examples/fine\_tuning\_for\_function\_calling))

### Token Usage
Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If you run into token limits, we suggest limiting the number of functions or the length of the descriptions you provide for function parameters.
It is also possible to use [fine-tuning](/docs/guides/fine-tuning#fine-tuning-examples) to reduce the number of tokens used if you have many functions defined in your tools specification.
Handling function calls
-----------------------
When the model calls a function, you must execute it and return the result. Since model responses can include zero, one, or multiple calls, it is best practice to assume there are several.
The response `output` array contains an entry with the `type` having a value of `function\_call`. Each entry with a `call\_id` (used later to submit the function result), `name`, and JSON-encoded `arguments`.
Sample response with multiple function calls
```json
[
{
"id": "fc\_12345xyz",
"call\_id": "call\_12345xyz",
"type": "function\_call",
"name": "get\_weather",
"arguments": "{\"location\":\"Paris, France\"}"
},
{
"id": "fc\_67890abc",
"call\_id": "call\_67890abc",
"type": "function\_call",
"name": "get\_weather",
"arguments": "{\"location\":\"Bogotá, Colombia\"}"
},
{
"id": "fc\_99999def",
"call\_id": "call\_99999def",
"type": "function\_call",
"name": "send\_email",
"arguments": "{\"to\":\"bob@email.com\",\"body\":\"Hi bob\"}"
}
]
```
Execute function calls and append results
```python
for tool\_call in response.output:
if tool\_call.type != "function\_call":
continue
name = tool\_call.name
args = json.loads(tool\_call.arguments)
result = call\_function(name, args)
input\_messages.append({
"type": "function\_call\_output",
"call\_id": tool\_call.call\_id,
"output": str(result)
})
```
```javascript
for (const toolCall of response.output) {
if (toolCall.type !== "function\_call") {
continue;
}
const name = toolCall.name;
const args = JSON.parse(toolCall.arguments);
const result = callFunction(name, args);
input.push({
type: "function\_call\_output",
call\_id: toolCall.call\_id,
output: result.toString()
});
}
```
In the example above, we have a hypothetical `call\_function` to route each call. Here’s a possible implementation:
Execute function calls and append results
```python
def call\_function(name, args):
if name == "get\_weather":
return get\_weather(\*\*args)
if name == "send\_email":
return send\_email(\*\*args)
```
```javascript
const callFunction = async (name, args) => {
if (name === "get\_weather") {
return getWeather(args.latitude, args.longitude);
}
if (name === "send\_email") {
return sendEmail(args.to, args.body);
}
};
```

### Formatting results
A result must be a string, but the format is up to you (JSON, error codes, plain text, etc.). The model will interpret that string as needed.
If your function has no return value (e.g. `send\_email`), simply return a string to indicate success or failure. (e.g. `"success"`)

### Incorporating results into response
After appending the results to your `input`, you can send them back to the model to get a final response.
Send results back to model
```python
response = client.responses.create(
model="gpt-4.1",
input=input\_messages,
tools=tools,
)
```
```javascript
const response = await openai.responses.create({
model: "gpt-4.1",
input,
tools,
});
```
Final response
```json
"It's about 15°C in Paris, 18°C in Bogotá, and I've sent that email to Bob."
```
Additional configurations
-------------------------

### Tool choice
By default the model will determine when and how many tools to use. You can force specific behavior with the `tool\_choice` parameter.
1. \*\*Auto:\*\* (\_Default\_) Call zero, one, or multiple functions. `tool\_choice: "auto"`
2. \*\*Required:\*\* Call one or more functions. `tool\_choice: "required"`
3. \*\*Forced Function:\*\* Call exactly one specific function. `tool\_choice: {"type": "function", "name": "get\_weather"}`
![Function Calling Diagram Steps](https://cdn.openai.com/API/docs/images/function-calling-diagram-tool-choice.png)
You can also set `tool\_choice` to `"none"` to imitate the behavior of passing no functions.

### Parallel function calling
The model may choose to call multiple functions in a single turn. You can prevent this by setting `parallel\_tool\_calls` to `false`, which ensures exactly zero or one tool is called.
\*\*Note:\*\* Currently, if you are using a fine tuned model and the model calls multiple functions in one turn then [strict mode](#strict-mode) will be disabled for those calls.
\*\*Note for `gpt-4.1-nano-2025-04-14`:\*\* This snapshot of `gpt-4.1-nano` can sometimes include multiple tools calls for the same tool if parallel tool calls are enabled. It is recommended to disable this feature when using this nano snapshot.

### Strict mode
Setting `strict` to `true` will ensure function calls reliably adhere to the function schema, instead of being best effort. We recommend always enabling strict mode.
Under the hood, strict mode works by leveraging our [structured outputs](/docs/guides/structured-outputs) feature and therefore introduces a couple requirements:
1. `additionalProperties` must be set to `false` for each object in the `parameters`.
2. All fields in `properties` must be marked as `required`.
You can denote optional fields by adding `null` as a `type` option (see example below).
Strict mode enabled
```json
{
"type": "function",
"name": "get\_weather",
"description": "Retrieves current weather for the given location.",
"strict": true,
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City and country e.g. Bogotá, Colombia"
},
"units": {
"type": ["string", "null"],
"enum": ["celsius", "fahrenheit"],
"description": "Units the temperature will be returned in."
}
},
"required": ["location", "units"],
"additionalProperties": false
}
}
```
Strict mode disabled
```json
{
"type": "function",
"name": "get\_weather",
"description": "Retrieves current weather for the given location.",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City and country e.g. Bogotá, Colombia"
},
"units": {
"type": "string",
"enum": ["celsius", "fahrenheit"],
"description": "Units the temperature will be returned in."
}
},
"required": ["location"],
}
}
```
All schemas generated in the [playground](/playground) have strict mode enabled.
While we recommend you enable strict mode, it has a few limitations:
1. Some features of JSON schema are not supported. (See [supported schemas](/docs/guides/structured-outputs?context=with\_parse#supported-schemas).)
Specifically for fine tuned models:
1. Schemas undergo additional processing on the first request (and are then cached). If your schemas vary from request to request, this may result in higher latencies.
2. Schemas are cached for performance, and are not eligible for [zero data retention](/docs/models#how-we-use-your-data).
Streaming
---------
Streaming can be used to surface progress by showing which function is called as the model fills its arguments, and even displaying the arguments in real time.
Streaming function calls is very similar to streaming regular responses: you set `stream` to `true` and get different `event` objects.
Streaming function calls
```python
from openai import OpenAI
client = OpenAI()
tools = [{
"type": "function",
"name": "get\_weather",
"description": "Get current temperature for a given location.",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City and country e.g. Bogotá, Colombia"
}
},
"required": [
"location"
],
"additionalProperties": False
}
}]
stream = client.responses.create(
model="gpt-4.1",
input=[{"role": "user", "content": "What's the weather like in Paris today?"}],
tools=tools,
stream=True
)
for event in stream:
print(event)
```
```javascript
import { OpenAI } from "openai";
const openai = new OpenAI();
const tools = [{
type: "function",
name: "get\_weather",
description: "Get current temperature for provided coordinates in celsius.",
parameters: {
type: "object",
properties: {
latitude: { type: "number" },
longitude: { type: "number" }
},
required: ["latitude", "longitude"],
additionalProperties: false
},
strict: true
}];
const stream = await openai.responses.create({
model: "gpt-4.1",
input: [{ role: "user", content: "What's the weather like in Paris today?" }],
tools,
stream: true,
store: true,
});
for await (const event of stream) {
console.log(event)
}
```
Output events
```json
{"type":"response.output\_item.added","response\_id":"resp\_1234xyz","output\_index":0,"item":{"type":"function\_call","id":"fc\_1234xyz","call\_id":"call\_1234xyz","name":"get\_weather","arguments":""}}
{"type":"response.function\_call\_arguments.delta","response\_id":"resp\_1234xyz","item\_id":"fc\_1234xyz","output\_index":0,"delta":"{\""}
{"type":"response.function\_call\_arguments.delta","response\_id":"resp\_1234xyz","item\_id":"fc\_1234xyz","output\_index":0,"delta":"location"}
{"type":"response.function\_call\_arguments.delta","response\_id":"resp\_1234xyz","item\_id":"fc\_1234xyz","output\_index":0,"delta":"\":\""}
{"type":"response.function\_call\_arguments.delta","response\_id":"resp\_1234xyz","item\_id":"fc\_1234xyz","output\_index":0,"delta":"Paris"}
{"type":"response.function\_call\_arguments.delta","response\_id":"resp\_1234xyz","item\_id":"fc\_1234xyz","output\_index":0,"delta":","}
{"type":"response.function\_call\_arguments.delta","response\_id":"resp\_1234xyz","item\_id":"fc\_1234xyz","output\_index":0,"delta":" France"}
{"type":"response.function\_call\_arguments.delta","response\_id":"resp\_1234xyz","item\_id":"fc\_1234xyz","output\_index":0,"delta":"\"}"}
{"type":"response.function\_call\_arguments.done","response\_id":"resp\_1234xyz","item\_id":"fc\_1234xyz","output\_index":0,"arguments":"{\"location\":\"Paris, France\"}"}
{"type":"response.output\_item.done","response\_id":"resp\_1234xyz","output\_index":0,"item":{"type":"function\_call","id":"fc\_1234xyz","call\_id":"call\_2345abc","name":"get\_weather","arguments":"{\"location\":\"Paris, France\"}"}}
```
Instead of aggregating chunks into a single `content` string, however, you're aggregating chunks into an encoded `arguments` JSON object.
When the model calls one or more functions an event of type `response.output\_item.added` will be emitted for each function call that contains the following fields:
|Field|Description|
|---|---|
|response\_id|The id of the response that the function call belongs to|
|output\_index|The index of the output item in the response. This respresents the individual function calls in the response.|
|item|The in-progress function call item that includes a name, arguments and id field|
Afterwards you will receive a series of events of type `response.function\_call\_arguments.delta` which will contain the `delta` of the `arguments` field. These events contain the following fields:
|Field|Description|
|---|---|
|response\_id|The id of the response that the function call belongs to|
|item\_id|The id of the function call item that the delta belongs to|
|output\_index|The index of the output item in the response. This respresents the individual function calls in the response.|
|delta|The delta of the arguments field.|
Below is a code snippet demonstrating how to aggregate the `delta`s into a final `tool\_call` object.
Accumulating tool\\_call deltas
```python
final\_tool\_calls = {}
for event in stream:
if event.type === 'response.output\_item.added':
final\_tool\_calls[event.output\_index] = event.item;
elif event.type === 'response.function\_call\_arguments.delta':
index = event.output\_index
if final\_tool\_calls[index]:
final\_tool\_calls[index].arguments += event.delta
```
```javascript
const finalToolCalls = {};
for await (const event of stream) {
if (event.type === 'response.output\_item.added') {
finalToolCalls[event.output\_index] = event.item;
} else if (event.type === 'response.function\_call\_arguments.delta') {
const index = event.output\_index;
if (finalToolCalls[index]) {
finalToolCalls[index].arguments += event.delta;
}
}
}
```
Accumulated final\\_tool\\_calls\[0\]
```json
{
"type": "function\_call",
"id": "fc\_1234xyz",
"call\_id": "call\_2345abc",
"name": "get\_weather",
"arguments": "{\"location\":\"Paris, France\"}"
}
```
When the model has finished calling the functions an event of type `response.function\_call\_arguments.done` will be emitted. This event contains the entire function call including the following fields:
|Field|Description|
|---|---|
|response\_id|The id of the response that the function call belongs to|
|output\_index|The index of the output item in the response. This respresents the individual function calls in the response.|
|item|The function call item that includes a name, arguments and id field.|
Was this page useful?
