

## Imported snippet â€“ 2025-07-03 14:42:22

Production best practices
=========================
Transition AI projects to production with best practices.
This guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.
If you want to explore best practices for going into production further, please check out our Developer Day talk:
Setting up your organization
----------------------------
Once you [log in](/login) to your OpenAI account, you can find your organization name and ID in your [organization settings](/settings/organization/general). The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.
Users who belong to multiple organizations can [pass a header](/docs/api-reference/requesting-organization) to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the [default organization](/settings/organization/api-keys) will be billed. You can change your default organization in your [user settings](/settings/organization/api-keys).
You can invite new members to your organization from the [Team page](/settings/organization/team). Members can be \*\*readers\*\* or \*\*owners\*\*.
Readers:
\* Can make API requests.
\* Can view basic organization information.
\* Can create, update, and delete resources (like Assistants) in the organization, unless otherwise noted.
Owners:
\* Have all the permissions of readers.
\* Can modify billing information.
\* Can manage members within the organization.

### Managing billing limits
To begin using the OpenAI API, enter your [billing information](/settings/organization/billing/overview). If no billing information is entered, you will still have login access but will be unable to make API requests.
Once youâ€™ve entered your billing information, you will have an approved usage limit of $100 per month, which is set by OpenAI. Your quota limit will automatically increase as your usage on your platform increases and you move from one [usage tier](/docs/guides/rate-limits#usage-tiers) to another. You can review your current usage limit in the [limits](/settings/organization/limits) page in your account settings.
If youâ€™d like to be notified when your usage exceeds a certain dollar amount, you can set a notification threshold through the [usage limits](/settings/organization/limits) page. When the notification threshold is reached, the owners of the organization will receive an email notification. You can also set a monthly budget so that, once the monthly budget is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.

### API keys
The OpenAI API uses API keys for authentication. Visit your [API keys](/settings/organization/api-keys) page to retrieve the API key you'll use in your requests.
This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our [Best practices for API key safety](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).
API key usage can be monitored on the [Usage page](/usage) once tracking is enabled. If you are using an API key generated prior to Dec 20, 2023 tracking will not be enabled by default. You can enable tracking going forward on the [API key management dashboard](/api-keys). All API keys generated past Dec 20, 2023 have tracking enabled. Any previous untracked usage will be displayed as `Untracked` in the dashboard.

### Staging projects
As you scale, you may want to create separate projects for your staging and production environments. You can create these projects in the dashboard, allowing you to isolate your development and testing work, so you don't accidentally disrupt your live application. You can also limit user access to your production project, and set custom rate and spend limits per project.
Scaling your solution architecture
----------------------------------
When designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:
\* \*\*Horizontal scaling\*\*: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.
\* \*\*Vertical scaling\*\*: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.
\* \*\*Caching\*\*: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.
\* \*\*Load balancing\*\*: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.

### Managing rate limits
When using our API, it's important to understand and plan for [rate limits](/docs/guides/rate-limits).
Improving latencies
-------------------
Check out our most up-to-date guide on [latency optimization](/docs/guides/latency-optimization).
Latency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.
The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:
Network
End user to API latency
Server
Time to process prompt tokens
Server
Time to sample/generate tokens
Network
API to end user latency
The bulk of the latency typically arises from the token generation step.
> \*\*Intuition\*\*: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.

### Common factors affecting latency and possible mitigation techniques
Now that we have looked at the basics of latency, letâ€™s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.

#### Model
Our API offers different models with varying levels of complexity and generality. The most capable models, such as `gpt-4`, can generate more complex and diverse completions, but they also take longer to process your query. Models such as `gpt-4o-mini`, can generate faster and cheaper Chat Completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed, cost, and quality.

#### Number of completion tokens
Requesting a large amount of generated tokens completions can lead to increased latencies:
\* \*\*Lower max tokens\*\*: for requests with a similar token generation count, those that have a lower `max\_tokens` parameter incur less latency.
\* \*\*Include stop sequences\*\*: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using `11.` as a stop sequence, you can generate a list with only 10 items, since the completion will stop when `11.` is reached. [Read our help article on stop sequences](https://help.openai.com/en/articles/5072263-how-do-i-use-stop-sequences) for more context on how you can do this.
\* \*\*Generate fewer completions\*\*: lower the values of `n` and `best\_of` when possible where `n` refers to how many completions to generate for each prompt and `best\_of` is used to represent the result with the highest log probability per token.
If `n` and `best\_of` both equal 1 (which is the default), the number of generated tokens will be at most, equal to `max\_tokens`.
If `n` (the number of completions returned) or `best\_of` (the number of completions generated for consideration) are set to `> 1`, each request will create multiple outputs. Here, you can consider the number of generated tokens as `[ max\_tokens \* max (n, best\_of) ]`

#### Streaming
Setting `stream: true` in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so itâ€™s worth experimenting with streaming.

#### Infrastructure
Our servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.

#### Batching
Depending on your use case, batching \_may help\_. If you are sending multiple requests to the same endpoint, you can [batch the prompts](/docs/guides/rate-limits#batching-requests) to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.
Managing costs
--------------
To monitor your costs, you can set a [notification threshold](/settings/organization/limits) in your account to receive an email alert once you pass a certain usage threshold. You can also set a [monthly budget](/settings/organization/limits). Please be mindful of the potential for a monthly budget to cause disruptions to your application/users. Use the [usage tracking dashboard](/settings/organization/usage) to monitor your token usage during the current and past billing cycles.

### Text generation
One of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a [pay-as-you-go pricing model](https://openai.com/api/pricing/), with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.
\*\*One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token.\*\* There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, [fine-tuning](/docs/guides/model-optimization) models, or caching common user queries so that they don't need to be processed repeatedly.
You can experiment with our interactive [tokenizer tool](/tokenizer) to help you estimate costs. The API and playground also returns token counts as part of the response. Once youâ€™ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our [token usage help article](https://help.openai.com/en/articles/6614209-how-do-i-check-my-token-usage).
MLOps strategy
--------------
As you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These include
\* Data and model management: managing the data used to train or fine-tune your model and tracking versions and changes.
\* Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.
\* Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.
\* Model deployment: automating the process of deploying your model and related artifacts into production.
Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.
Security and compliance
-----------------------
As you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our [security practices](https://www.openai.com/security) and [trust and compliance portal](https://trust.openai.com/) provide our most comprehensive and up-to-date documentation. For reference, here is our [Privacy Policy](https://openai.com/privacy/) and [Terms of Use](https://openai.com/api/policies/terms/).
Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.

### Safety best practices
When creating your application with our API, consider our [safety best practices](/docs/guides/safety-best-practices) to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.
Business considerations
-----------------------
As projects using AI move from prototype to production, it is important to consider how to build a great product with AI and how that ties back to your core business. We certainly don't have all the answers but a great starting place is a talk from our Developer Day where we dive into this with some of our customers:
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:42:28

Safety best practices
=====================
Implement safety measures like moderation and human oversight.

### Use our free Moderation API
OpenAI's [Moderation API](/docs/guides/moderation) is free-to-use and can help reduce the frequency of unsafe content in your completions. Alternatively, you may wish to develop your own content filtration system tailored to your use case.

### Adversarial testing
We recommend â€œred-teamingâ€ your application to ensure it's robust to adversarial input. Test your product over a wide range of inputs and user behaviors, both a representative set and those reflective of someone trying to â€˜break' your application. Does it wander off topic? Can someone easily redirect the feature via prompt injections, e.g. â€œignore the previous instructions and do this insteadâ€?

### Human in the loop (HITL)
Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation. Humans should be aware of the limitations of the system, and have access to any information needed to verify the outputs (for example, if the application summarizes notes, a human should have easy access to the original notes to refer back).

### Prompt engineering
â€œPrompt engineeringâ€ can help constrain the topic and tone of output text. This reduces the chance of producing undesired content, even if a user tries to produce it. Providing additional context to the model (such as by giving a few high-quality examples of desired behavior prior to the new input) can make it easier to steer model outputs in desired directions.

### â€œKnow your customerâ€ (KYC)
Users should generally need to register and log-in to access your service. Linking this service to an existing account, such as a Gmail, LinkedIn, or Facebook log-in, may help, though may not be appropriate for all use-cases. Requiring a credit card or ID card reduces risk further.

### Constrain user input and limit output tokens
Limiting the amount of text a user can input into the prompt helps avoid prompt injection. Limiting the number of output tokens helps reduce the chance of misuse.
Narrowing the ranges of inputs or outputs, especially drawn from trusted sources, reduces the extent of misuse possible within an application.
Allowing user inputs through validated dropdown fields (e.g., a list of movies on Wikipedia) can be more secure than allowing open-ended text inputs.
Returning outputs from a validated set of materials on the backend, where possible, can be safer than returning novel generated content (for instance, routing a customer query to the best-matching existing customer support article, rather than attempting to answer the query from-scratch).

### Allow users to report issues
Users should generally have an easily-available method for reporting improper functionality or other concerns about application behavior (listed email address, ticket submission method, etc). This method should be monitored by a human and responded to as appropriate.

### Understand and communicate limitations
From hallucinating inaccurate information, to offensive outputs, to bias, and much more, language models may not be suitable for every use case without significant modifications. Consider whether the model is fit for your purpose, and evaluate the performance of the API on a wide range of potential inputs in order to identify cases where the API's performance might drop. Consider your customer base and the range of inputs that they will be using, and ensure their expectations are calibrated appropriately.
Safety and security are very important to us at OpenAI.
If in the course of your development you do notice any safety or security issues with the API or anything else related to OpenAI, please submit these through our [Coordinated Vulnerability Disclosure Program](https://openai.com/security/disclosure/).
End-user IDs
------------
Sending end-user IDs in your requests can be a useful tool to help OpenAI monitor and detect abuse. This allows OpenAI to provide your team with more actionable feedback in the event that we detect any policy violations in your application.
The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. If you offer a preview of your product to non-logged in users, you can send a session ID instead.
You can include end-user IDs in your API requests via the `user` parameter as follows:
Example: Providing a user identifier
```python
from openai import OpenAI
client = OpenAI()
response = client.chat.completions.create(
model="gpt-4o-mini",
messages=[
{"role": "user", "content": "This is a test"}
],
max\_tokens=5,
user="user\_123456"
)
```
```bash
curl https://api.openai.com/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4o-mini",
"messages": [
{"role": "user", "content": "This is a test"}
],
"max\_tokens": 5,
"user": "user123456"
}'
```
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:42:32

Prompt caching
==============
Reduce latency and cost with prompt caching.
Model prompts often contain repetitive content, like system prompts and common instructions. OpenAI routes API requests to servers that recently processed the same prompt, making it cheaper and faster than processing a prompt from scratch. This can reduce latency by up to 80% and cost by up to 75%. Prompt Caching works automatically on all your API requests (no code changes required) and has no additional fees associated with it. Prompt Caching is enabled for all recent [models](/docs/models), gpt-4o and newer.
This guide describes how prompt caching works in detail, so that you can optimize your prompts for lower latency and cost.
Structuring prompts
-------------------
Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.
![Prompt Caching visualization](https://openaidevs.retool.com/api/file/8593d9bb-4edb-4eb6-bed9-62bfb98db5ee)
How it works
------------
Caching is enabled automatically for prompts that are 1024 tokens or longer. When you make an API request, the following steps occur:
1. \*\*Cache Routing\*\*:
\* Requests are routed to a machine based on a hash of the initial prefix of the prompt. The hash typically uses the first 256 tokens, though the exact length varies depending on the model.
\* If you provide the [`user`](/docs/api-reference/responses/create#responses-create-user) parameter, it is combined with the prefix hash, allowing you to influence routing and improve cache hit rates. This is especially beneficial when many requests share long, common prefixes.
\* If requests for the same prefix and user combination exceed a certain rate (approximately 15 requests per minute), some may overflow and get routed to additional machines, reducing cache effectiveness.
2. \*\*Cache Lookup\*\*: The system checks if the initial portion (prefix) of your prompt exists in the cache on the selected machine.
3. \*\*Cache Hit\*\*: If a matching prefix is found, the system uses the cached result. This significantly decreases latency and reduces costs.
4. \*\*Cache Miss\*\*: If no matching prefix is found, the system processes your full prompt, caching the prefix afterward on that machine for future requests.
Cached prefixes generally remain active for 5 to 10 minutes of inactivity. However, during off-peak periods, caches may persist for up to one hour.
Requirements
------------
Caching is available for prompts containing 1024 tokens or more, with cache hits occurring in increments of 128 tokens. Therefore, the number of cached tokens in a request will always fall within the following sequence: 1024, 1152, 1280, 1408, and so on, depending on the prompt's length.
All requests, including those with fewer than 1024 tokens, will display a `cached\_tokens` field of the `usage.prompt\_tokens\_details` [Response object](/docs/api-reference/responses/object) or [Chat object](/docs/api-reference/chat/object) indicating how many of the prompt tokens were a cache hit. For requests under 1024 tokens, `cached\_tokens` will be zero.
```json
"usage": {
"prompt\_tokens": 2006,
"completion\_tokens": 300,
"total\_tokens": 2306,
"prompt\_tokens\_details": {
"cached\_tokens": 1920
},
"completion\_tokens\_details": {
"reasoning\_tokens": 0,
"accepted\_prediction\_tokens": 0,
"rejected\_prediction\_tokens": 0
}
}
```

### What can be cached
\* \*\*Messages:\*\* The complete messages array, encompassing system, user, and assistant interactions.
\* \*\*Images:\*\* Images included in user messages, either as links or as base64-encoded data, as well as multiple images can be sent. Ensure the detail parameter is set identically, as it impacts image tokenization.
\* \*\*Tool use:\*\* Both the messages array and the list of available `tools` can be cached, contributing to the minimum 1024 token requirement.
\* \*\*Structured outputs:\*\* The structured output schema serves as a prefix to the system message and can be cached.
Best practices
--------------
\* Structure prompts with \*\*static or repeated content at the beginning\*\* and dynamic, user-specific content at the end.
\* Use the \*\*[`user`](/docs/api-reference/responses/create#responses-create-user) parameter\*\* consistently across requests that share common prefixes. Select a `user` granularity that keeps each unique prefix-user combination below 15 requests per minute to avoid cache overflow.
\* \*\*Monitor your cache performance metrics\*\*, including cache hit rates, latency, and the proportion of tokens cached, to refine your strategy.
\* \*\*Maintain a steady stream of requests\*\* with identical prompt prefixes to minimize cache evictions and maximize caching benefits.
Frequently asked questions
--------------------------
1. \*\*How is data privacy maintained for caches?\*\*
Prompt caches are not shared between organizations. Only members of the same organization can access caches of identical prompts.
2. \*\*Does Prompt Caching affect output token generation or the final response of the API?\*\*
Prompt Caching does not influence the generation of output tokens or the final response provided by the API. Regardless of whether caching is used, the output generated will be identical. This is because only the prompt itself is cached, while the actual response is computed anew each time based on the cached prompt.
3. \*\*Is there a way to manually clear the cache?\*\*
Manual cache clearing is not currently available. Prompts that have not been encountered recently are automatically cleared from the cache. Typical cache evictions occur after 5-10 minutes of inactivity, though sometimes lasting up to a maximum of one hour during off-peak periods.
4. \*\*Will I be expected to pay extra for writing to Prompt Caching?\*\*
No. Caching happens automatically, with no explicit action needed or extra cost paid to use the caching feature.
5. \*\*Do cached prompts contribute to TPM rate limits?\*\*
Yes, as caching does not affect rate limits.
6. \*\*Is discounting for Prompt Caching available on Scale Tier and the Batch API?\*\*
Discounting for Prompt Caching is not available on the Batch API but is available on Scale Tier. With Scale Tier, any tokens that are spilled over to the shared API will also be eligible for caching.
7. \*\*Does Prompt Caching work on Zero Data Retention requests?\*\*
Yes, Prompt Caching is compliant with existing Zero Data Retention policies.
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:42:39

Predicted Outputs
=================
Reduce latency for model responses where much of the response is known ahead of time.
\*\*Predicted Outputs\*\* enable you to speed up API responses from [Chat Completions](/docs/api-reference/chat/create) when many of the output tokens are known ahead of time. This is most common when you are regenerating a text or code file with minor modifications. You can provide your prediction using the [`prediction` request parameter in Chat Completions](/docs/api-reference/chat/create#chat-create-prediction).
Predicted Outputs are available today using the latest `gpt-4o` and `gpt-4o-mini` models. Read on to learn how to use Predicted Outputs to reduce latency in your applications.
Code refactoring example
------------------------
Predicted Outputs are particularly useful for regenerating text documents and code files with small modifications. Let's say you want the [GPT-4o model](/docs/models#gpt-4o) to refactor a piece of TypeScript code, and convert the `username` property of the `User` class to be `email` instead:
```typescript
class User {
firstName: string = "";
lastName: string = "";
username: string = "";
}
export default User;
```
Most of the file will be unchanged, except for line 4 above. If you use the current text of the code file as your prediction, you can regenerate the entire file with lower latency. These time savings add up quickly for larger files.
Below is an example of using the `prediction` parameter in our SDKs to predict that the final output of the model will be very similar to our original code file, which we use as the prediction text.
Refactor a TypeScript class with a Predicted Output
```javascript
import OpenAI from "openai";
const code = `
class User {
firstName: string = "";
lastName: string = "";
username: string = "";
}
export default User;
`.trim();
const openai = new OpenAI();
const refactorPrompt = `
Replace the "username" property with an "email" property. Respond only
with code, and with no markdown formatting.
`;
const completion = await openai.chat.completions.create({
model: "gpt-4.1",
messages: [
{
role: "user",
content: refactorPrompt
},
{
role: "user",
content: code
}
],
store: true,
prediction: {
type: "content",
content: code
}
});
// Inspect returned data
console.log(completion);
console.log(completion.choices[0].message.content);
```
```python
from openai import OpenAI
code = """
class User {
firstName: string = "";
lastName: string = "";
username: string = "";
}
export default User;
"""
refactor\_prompt = """
Replace the "username" property with an "email" property. Respond only
with code, and with no markdown formatting.
"""
client = OpenAI()
completion = client.chat.completions.create(
model="gpt-4.1",
messages=[
{
"role": "user",
"content": refactor\_prompt
},
{
"role": "user",
"content": code
}
],
prediction={
"type": "content",
"content": code
}
)
print(completion)
print(completion.choices[0].message.content)
```
```bash
curl https://api.openai.com/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-d '{
"model": "gpt-4.1",
"messages": [
{
"role": "user",
"content": "Replace the username property with an email property. Respond only with code, and with no markdown formatting."
},
{
"role": "user",
"content": "$CODE\_CONTENT\_HERE"
}
],
"prediction": {
"type": "content",
"content": "$CODE\_CONTENT\_HERE"
}
}'
```
In addition to the refactored code, the model response will contain data that looks something like this:
```javascript
{
id: 'chatcmpl-xxx',
object: 'chat.completion',
created: 1730918466,
model: 'gpt-4o-2024-08-06',
choices: [ /\* ...actual text response here... \*/],
usage: {
prompt\_tokens: 81,
completion\_tokens: 39,
total\_tokens: 120,
prompt\_tokens\_details: { cached\_tokens: 0, audio\_tokens: 0 },
completion\_tokens\_details: {
reasoning\_tokens: 0,
audio\_tokens: 0,
accepted\_prediction\_tokens: 18,
rejected\_prediction\_tokens: 10
}
},
system\_fingerprint: 'fp\_159d8341cc'
}
```
Note both the `accepted\_prediction\_tokens` and `rejected\_prediction\_tokens` in the `usage` object. In this example, 18 tokens from the prediction were used to speed up the response, while 10 were rejected.
Note that any rejected tokens are still billed like other completion tokens generated by the API, so Predicted Outputs can introduce higher costs for your requests.
Streaming example
-----------------
The latency gains of Predicted Outputs are even greater when you use streaming for API responses. Here is an example of the same code refactoring use case, but using streaming in the OpenAI SDKs instead.
Predicted Outputs with streaming
```javascript
import OpenAI from "openai";
const code = `
class User {
firstName: string = "";
lastName: string = "";
username: string = "";
}
export default User;
`.trim();
const openai = new OpenAI();
const refactorPrompt = `
Replace the "username" property with an "email" property. Respond only
with code, and with no markdown formatting.
`;
const completion = await openai.chat.completions.create({
model: "gpt-4.1",
messages: [
{
role: "user",
content: refactorPrompt
},
{
role: "user",
content: code
}
],
store: true,
prediction: {
type: "content",
content: code
},
stream: true
});
// Inspect returned data
for await (const chunk of stream) {
process.stdout.write(chunk.choices[0]?.delta?.content || "");
}
```
```python
from openai import OpenAI
code = """
class User {
firstName: string = "";
lastName: string = "";
username: string = "";
}
export default User;
"""
refactor\_prompt = """
Replace the "username" property with an "email" property. Respond only
with code, and with no markdown formatting.
"""
client = OpenAI()
stream = client.chat.completions.create(
model="gpt-4.1",
messages=[
{
"role": "user",
"content": refactor\_prompt
},
{
"role": "user",
"content": code
}
],
prediction={
"type": "content",
"content": code
},
stream=True
)
for chunk in stream:
if chunk.choices[0].delta.content is not None:
print(chunk.choices[0].delta.content, end="")
```
Position of predicted text in response
--------------------------------------
When providing prediction text, your prediction can appear anywhere within the generated response, and still provide latency reduction for the response. Let's say your predicted text is the simple [Hono](https://hono.dev/) server shown below:
```typescript
import { serveStatic } from "@hono/node-server/serve-static";
import { serve } from "@hono/node-server";
import { Hono } from "hono";
const app = new Hono();
app.get("/api", (c) => {
return c.text("Hello Hono!");
});
// You will need to build the client code first `pnpm run ui:build`
app.use(
"/\*",
serveStatic({
rewriteRequestPath: (path) => `./dist${path}`,
})
);
const port = 3000;
console.log(`Server is running on port ${port}`);
serve({
fetch: app.fetch,
port,
});
```
You could prompt the model to regenerate the file with a prompt like:
```text
Add a get route to this application that responds with
the text "hello world". Generate the entire application
file again with this route added, and with no other
markdown formatting.
```
The response to the prompt might look something like this:
```typescript
import { serveStatic } from "@hono/node-server/serve-static";
import { serve } from "@hono/node-server";
import { Hono } from "hono";
const app = new Hono();
app.get("/api", (c) => {
return c.text("Hello Hono!");
});
app.get("/hello", (c) => {
return c.text("hello world");
});
// You will need to build the client code first `pnpm run ui:build`
app.use(
"/\*",
serveStatic({
rewriteRequestPath: (path) => `./dist${path}`,
})
);
const port = 3000;
console.log(`Server is running on port ${port}`);
serve({
fetch: app.fetch,
port,
});
```
You would still see accepted prediction tokens in the response, even though the prediction text appeared both before and after the new content added to the response:
```javascript
{
id: 'chatcmpl-xxx',
object: 'chat.completion',
created: 1731014771,
model: 'gpt-4o-2024-08-06',
choices: [ /\* completion here... \*/],
usage: {
prompt\_tokens: 203,
completion\_tokens: 159,
total\_tokens: 362,
prompt\_tokens\_details: { cached\_tokens: 0, audio\_tokens: 0 },
completion\_tokens\_details: {
reasoning\_tokens: 0,
audio\_tokens: 0,
accepted\_prediction\_tokens: 60,
rejected\_prediction\_tokens: 0
}
},
system\_fingerprint: 'fp\_9ee9e968ea'
}
```
This time, there were no rejected prediction tokens, because the entire content of the file we predicted was used in the final response. Nice! ðŸ”¥
Limitations
-----------
When using Predicted Outputs, you should consider the following factors and limitations.
\* Predicted Outputs are only supported with the GPT-4o and GPT-4o-mini series of models.
\* When providing a prediction, any tokens provided that are not part of the final completion are still charged at completion token rates. See the [`rejected\_prediction\_tokens` property of the `usage` object](/docs/api-reference/chat/object#chat/object-usage) to see how many tokens are not used in the final response.
\* The following [API parameters](/docs/api-reference/chat/create) are not supported when using Predicted Outputs:
\* `n`: values higher than 1 are not supported
\* `logprobs`: not supported
\* `presence\_penalty`: values greater than 0 are not supported
\* `frequency\_penalty`: values greater than 0 are not supported
\* `audio`: Predicted Outputs are not compatible with [audio inputs and outputs](/docs/guides/audio)
\* `modalities`: Only `text` modalities are supported
\* `max\_completion\_tokens`: not supported
\* `tools`: Function calling is not currently supported with Predicted Outputs
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:42:45

Reasoning best practices
========================
Learn when to use reasoning models and how they compare to GPT models.
OpenAI offers two types of models: [reasoning models](/docs/models#o4-mini) (o3 and o4-mini, for example) and [GPT models](/docs/models#gpt-4.1) (like GPT-4.1). These model families behave differently.
This guide covers:
1. The difference between our reasoning and non-reasoning GPT models
2. When to use our reasoning models
3. How to prompt reasoning models effectively
Read more about [reasoning models](/docs/guides/reasoning) and how they work.
Reasoning models vs. GPT models
-------------------------------
Compared to GPT models, our o-series models excel at different tasks and require different prompts. One model family isn't better than the otherâ€”they're just different.
We trained our o-series models (â€œthe plannersâ€) to think longer and harder about complex tasks, making them effective at strategizing, planning solutions to complex problems, and making decisions based on large volumes of ambiguous information. These models can also execute tasks with high accuracy and precision, making them ideal for domains that would otherwise require a human expertâ€”like math, science, engineering, financial services, and legal services.
On the other hand, our lower-latency, more cost-efficient GPT models (â€œthe workhorsesâ€) are designed for straightforward execution. An application might use o-series models to plan out the strategy to solve a problem, and use GPT models to execute specific tasks, particularly when speed and cost are more important than perfect accuracy.

### How to choose
What's most important for your use case?
\* \*\*Speed and cost\*\* â†’ GPT models are faster and tend to cost less
\* \*\*Executing well defined tasks\*\* â†’ GPT models handle explicitly defined tasks well
\* \*\*Accuracy and reliability\*\* â†’ o-series models are reliable decision makers
\* \*\*Complex problem-solving\*\* â†’ o-series models work through ambiguity and complexity
If speed and cost are the most important factors when completing your tasks \_and\_ your use case is made up of straightforward, well defined tasks, then our GPT models are the best fit for you. However, if accuracy and reliability are the most important factors \_and\_ you have a very complex, multistep problem to solve, our o-series models are likely right for you.
Most AI workflows will use a combination of both modelsâ€”o-series for agentic planning and decision-making, GPT series for task execution.
![GPT models pair well with o-series models](https://cdn.openai.com/API/docs/images/customer-service-example.png)
\_Our GPT-4o and GPT-4o mini models triage order details with customer information, identify the order issues and the return policy, and then feed all of these data points into o3-mini to make the final decision about the viability of the return based on policy.\_
When to use our reasoning models
--------------------------------
Here are a few patterns of successful usage that weâ€™ve observed from customers and internally at OpenAI. This isn't a comprehensive review of all possible use cases but, rather, some practical guidance for testing our o-series models.
[Ready to use a reasoning model? Skip to the quickstart â†’](/docs/guides/reasoning)

### 1\. Navigating ambiguous tasks
Reasoning models are particularly good at taking limited information or disparate pieces of information and with a simple prompt, understanding the userâ€™s intent and handling any gaps in the instructions. In fact, reasoning models will often ask clarifying questions before making uneducated guesses or attempting to fill information gaps.
> â€œo1â€™s reasoning capabilities enable our multi-agent platform Matrix to produce exhaustive, well-formatted, and detailed responses when processing complex documents. For example, o1 enabled Matrix to easily identify baskets available under the restricted payments capacity in a credit agreement, with a basic prompt. No former models are as performant. o1 yielded stronger results on 52% of complex prompts on dense Credit Agreements compared to other models.â€
>
> â€”[Hebbia](https://www.hebbia.com/), AI knowledge platform company for legal and finance

### 2\. Finding a needle in a haystack
When youâ€™re passing large amounts of unstructured information, reasoning models are great at understanding and pulling out only the most relevant information to answer a question.
> "To analyze a company's acquisition, o1 reviewed dozens of company documentsâ€”like contracts and leasesâ€”to find any tricky conditions that might affect the deal. The model was tasked with flagging key terms and in doing so, identified a crucial "change of control" provision in the footnotes: if the company was sold, it would have to pay off a $75 million loan immediately. o1's extreme attention to detail enables our AI agents to support finance professionals by identifying mission-critical information."
>
> â€”[Endex](https://endex.ai/), AI financial intelligence platform

### 3\. Finding relationships and nuance across a large dataset
Weâ€™ve found that reasoning models are particularly good at reasoning over complex documents that have hundreds of pages of dense, unstructured informationâ€”things like legal contracts, financial statements, and insurance claims. The models are particularly strong at drawing parallels between documents and making decisions based on unspoken truths represented in the data.
> â€œTax research requires synthesizing multiple documents to produce a final, cogent answer. We swapped GPT-4o for o1 and found that o1 was much better at reasoning over the interplay between documents to reach logical conclusions that were not evident in any one single document. As a result, we saw a 4x improvement in end-to-end performance by switching to o1â€”incredible.â€
>
> â€”[Blue J](https://www.bluej.com/), AI platform for tax research
Reasoning models are also skilled at reasoning over nuanced policies and rules, and applying them to the task at hand in order to reach a reasonable conclusion.
> "In financial analyses, analysts often tackle complex scenarios around shareholder equity and need to understand the relevant legal intricacies. We tested about 10 models from different providers with a challenging but common question: how does a fundraise affect existing shareholders, especially when they exercise their anti-dilution privileges? This required reasoning through pre- and post-money valuations and dealing with circular dilution loopsâ€”something top financial analysts would spend 20-30 minutes to figure out. We found that o1 and o3-mini can do this flawlessly! The models even produced a clear calculation table showing the impact on a $100k shareholder."
>
> â€“[BlueFlame AI](https://www.blueflame.ai/), AI platform for investment management

### 4\. Multistep agentic planning
Reasoning models are critical to agentic planning and strategy development. Weâ€™ve seen success when a reasoning model is used as â€œthe planner,â€ producing a detailed, multistep solution to a problem and then selecting and assigning the right GPT model (â€œthe doerâ€) for each step, based on whether high intelligence or low latency is most important.
> â€œWe use o1 as the planner in our agent infrastructure, letting it orchestrate other models in the workflow to complete a multistep task. We find o1 is really good at selecting data types and breaking down big questions into smaller chunks, enabling other models to focus on execution.â€
>
> â€”[Argon AI](https://argon-ai.com/), AI knowledge platform for the pharmaceutical industry
> â€œo1 powers many of our agentic workflows at Lindy, our AI assistant for work. The model uses function calling to pull information from your calendar or email and then can automatically help you schedule meetings, send emails, and manage other parts of your day-to-day tasks. We switched all of our agentic steps that used to cause issues to o1 and observing our agents becoming basically flawless overnight!â€
>
> â€”[Lindy.AI](http://Lindy.AI), AI assistant for work

### 5\. Visual reasoning
As of today, o1 is the only reasoning model that supports vision capabilities. What sets it apart from GPT-4o is that o1 can grasp even the most challenging visuals, like charts and tables with ambiguous structure or photos with poor image quality.
> â€œWe automate risk and compliance reviews for millions of products online, including luxury jewelry dupes, endangered species, and controlled substances. GPT-4o reached 50% accuracy on our hardest image classification tasks. o1 achieved an impressive 88% accuracy without any modifications to our pipeline.â€
>
> â€”[SafetyKit](https://www.safetykit.com/), AI-powered risk and compliance platform
From our own internal testing, weâ€™ve seen that o1 can identify fixtures and materials from highly detailed architectural drawings to generate a comprehensive bill of materials. One of the most surprising things we observed was that o1 can draw parallels across different images by taking a legend on one page of the architectural drawings and correctly applying it across another page without explicit instructions. Below you can see that, for the 4x4 PT wood posts, o1 recognized that "PT" stands for pressure treated based on the legend.
![o-series models correctly read architectural drawing details](https://cdn.openai.com/API/docs/images/architectural-drawing-example.png)

### 6\. Reviewing, debugging, and improving code quality
Reasoning models are particularly effective at reviewing and improving large amounts of code, often running code reviews in the background given the modelsâ€™ higher latency.
> â€œWe deliver automated AI Code Reviews on platforms like GitHub and GitLab. While code review process is not inherently latency-sensitive, it does require understanding the code diffs across multiple files. This is where o1 really shinesâ€”it's able to reliably detect minor changes to a codebase that could be missed by a human reviewer. We were able to increase product conversion rates by 3x after switching to o-series models.â€
>
> â€”[CodeRabbit](https://www.coderabbit.ai/), AI code review startup
While GPT-4o and GPT-4o mini may be better designed for writing code with their lower latency, weâ€™ve also seen o3-mini spike on code production for use cases that are slightly less latency-sensitive.
> â€œo3-mini consistently produces high-quality, conclusive code, and very frequently arrives at the correct solution when the problem is well-defined, even for very challenging coding tasks. While other models may only be useful for small-scale, quick code iterations, o3-mini excels at planning and executing complex software design systems.â€
>
> â€”[Windsurf](https://codeium.com/), collaborative agentic AI-powered IDE, built by Codeium

### 7\. Evaluation and benchmarking for other model responses
Weâ€™ve also seen reasoning models do well in benchmarking and evaluating other model responses. Data validation is important for ensuring dataset quality and reliability, especially in sensitive fields like healthcare. Traditional validation methods use predefined rules and patterns, but advanced models like o1 and o3-mini can understand context and reason about data for a more flexible and intelligent approach to validation.
> "Many customers use LLM-as-a-judge as part of their eval process in Braintrust. For example, a healthcare company might summarize patient questions using a workhorse model like gpt-4o, then assess the summary quality with o1. One Braintrust customer saw the F1 score of a judge go from 0.12 with 4o to 0.74 with o1! In these use cases, theyâ€™ve found o1â€™s reasoning to be a game-changer in finding nuanced differences in completions, for the hardest and most complex grading tasks."
>
> â€”[Braintrust](https://www.braintrust.dev/), AI evals platform
How to prompt reasoning models effectively
------------------------------------------
These models perform best with straightforward prompts. Some prompt engineering techniques, like instructing the model to "think step by step," may not enhance performance (and can sometimes hinder it). See best practices below, or [get started with prompt examples](/docs/guides/reasoning/advice-on-prompting#prompt-examples).
\* \*\*Developer messages are the new system messages\*\*: Starting with `o1-2024-12-17`, reasoning models support developer messages rather than system messages, to align with the chain of command behavior described in the [model spec](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command).
\* \*\*Keep prompts simple and direct\*\*: The models excel at understanding and responding to brief, clear instructions.
\* \*\*Avoid chain-of-thought prompts\*\*: Since these models perform reasoning internally, prompting them to "think step by step" or "explain your reasoning" is unnecessary.
\* \*\*Use delimiters for clarity\*\*: Use delimiters like markdown, XML tags, and section titles to clearly indicate distinct parts of the input, helping the model interpret different sections appropriately.
\* \*\*Try zero shot first, then few shot if needed\*\*: Reasoning models often don't need few-shot examples to produce good results, so try to write prompts without examples first. If you have more complex requirements for your desired output, it may help to include a few examples of inputs and desired outputs in your prompt. Just ensure that the examples align very closely with your prompt instructions, as discrepancies between the two may produce poor results.
\* \*\*Provide specific guidelines\*\*: If there are ways you explicitly want to constrain the model's response (like "propose a solution with a budget under $500"), explicitly outline those constraints in the prompt.
\* \*\*Be very specific about your end goal\*\*: In your instructions, try to give very specific parameters for a successful response, and encourage the model to keep reasoning and iterating until it matches your success criteria.
\* \*\*Markdown formatting\*\*: Starting with `o1-2024-12-17`, reasoning models in the API will avoid generating responses with markdown formatting. To signal to the model when you do want markdown formatting in the response, include the string `Formatting re-enabled` on the first line of your developer message.
How to keep costs low and accuracy high
---------------------------------------
With the introduction of `o3` and `o4-mini` models, persisted reasoning items in the Responses API are treated differently. Previously (for `o1`, `o3-mini`, `o1-mini` and `o1-preview`), reasoning items were always ignored in followâ€‘up API requests, even if they were included in the input items of the requests. With `o3` and `o4-mini`, some reasoning items adjacent to function calls are included in the modelâ€™s context to help improve model performance while using the least amount of reasoning tokens.
For the best results with this change, we recommend using the [Responses API](/docs/api-reference/responses) with the `store` parameter set to `true`, and passing in all reasoning items from previous requests (either using `previous\_response\_id`, or by taking all the output items from an older request and passing them in as input items for a new one). OpenAI will automatically include any relevant reasoning items in the model's context and ignore any irrelevant ones. In more advanced useâ€‘cases where youâ€™d like to manage what goes into the model's context more precisely, we recommend that you at least include all reasoning items between the latest function call and the previous user message. Doing this will ensure that the model doesnâ€™t have to restart its reasoning when you respond to a function call, resulting in better functionâ€‘calling performance and lower overall token usage.
If youâ€™re using the Chat Completions API, reasoning items are never included in the context of the model. This is because Chat Completions is a stateless API. This will result in slightly degraded model performance and greater reasoning token usage in complex agentic cases involving many function calls. In instances where complex multiple function calling is not involved, there should be no degradation in performance regardless of the API being used.
Other resources
---------------
For more inspiration, visit the [OpenAI Cookbook](https://cookbook.openai.com), which contains example code and links to third-party resources, or learn more about our models and reasoning capabilities:
\* [Meet the models](/docs/models)
\* [Reasoning guide](/docs/guides/reasoning)
\* [How to use reasoning for validation](https://cookbook.openai.com/examples/o1/using\_reasoning\_for\_data\_validation)
\* [Video course: Reasoning with o1](https://www.deeplearning.ai/short-courses/reasoning-with-o1/)
\* [Papers on advanced prompting to improve reasoning](https://cookbook.openai.com/related\_resources#papers-on-advanced-prompting-to-improve-reasoning)
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:42:49

Evals design best practices
===========================
Learn best practices for designing evals to test model performance in production environments.
Generative AI is variable. Models sometimes produce different output from the same input, which makes traditional software testing methods insufficient for AI architectures. Evaluations (\*\*evals\*\*) are a way to test your AI system despite this variability.
This guide provides high-level guidance on designing evals. To get started with the [Evals API](/docs/api-reference/evals), see [evaluating model performance](/docs/guides/evals).
What are evals?
---------------
Evals are structured tests for measuring a model's performance. They help ensure accuracy, performance, and reliability, despite the nondeterministic nature of AI systems. They're also one of the only ways to \_improve\_ performance of an LLM-based application (through [fine-tuning](/docs/guides/model-optimization)).

### Types of evals
When you see the word "evals," it could refer to a few things:
\* Industry benchmarks for comparing models in isolation, like [MMLU](https://github.com/openai/evals/blob/main/examples/mmlu.ipynb) and those listed on [HuggingFace's leaderboard](https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a)
\* Standard numerical scoresâ€”like [ROUGE](https://aclanthology.org/W04-1013/), [BERTScore](https://arxiv.org/abs/1904.09675)â€”that you can use as you design evals for your use case
\* Specific tests you implement to measure your LLM application's performance
This guide is about the third type: designing your own evals.

### How to read evals
You'll often see numerical eval scores between 0 and 1. There's more to evals than just scores. Combine metrics with human judgment to ensure you're answering the right questions.
\*\*Evals tips\*\*
\* Adopt eval-driven development: Evaluate early and often. Write scoped tests at every stage.
\* Design task-specific evals: Make tests reflect model capability in real-world distributions.
\* Log everything: Log as you develop so you can mine your logs for good eval cases.
\* Automate when possible: Structure evaluations to allow for automated scoring.
\* It's a journey, not a destination: Evaluation is a continuous process.
\* Maintain agreement: Use human feedback to calibrate automated scoring.
\*\*Anti-patterns\*\*
\* Overly generic metrics: Relying solely on academic metrics like perplexity or BLEU score.
\* Biased design: Creating eval datasets that don't faithfully reproduce production traffic patterns.
\* Vibe-based evals: Using "it seems like it's working" as an evaluation strategy, or waiting until you ship before implementing any evals.
\* Ignoring human feedback: Not calibrating your automated metrics against human evals.
Design your eval process
------------------------
There are a few important components of an eval workflow:
1. \*\*Define eval objective\*\*. What's the success criteria for the eval?
2. \*\*Collect dataset\*\*. Which data will help you evaluate against your objective? Consider synthetic eval data, domain-specific eval data, purchased eval data, human-curated eval data, production data, and historical data.
3. \*\*Define eval metrics\*\*. How will you check that the success criteria are met?
4. \*\*Run and compare evals\*\*. Iterate and improve model performance for your task or system.
5. \*\*Continuously evaluate\*\*. Set up continuous evaluation (CE) to run evals on every change, monitor your app to identify new cases of nondeterminism, and grow the eval set over time.
Let's run through a few examples.

### Example: Summarizing transcripts
To test your LLM-based application's ability to summarize transcripts, your eval design might be:
1. \*\*Define eval objective\*\*
The model should be able to compete with reference summaries for relevance and accuracy.
2. \*\*Collect dataset\*\*
Use a mix of production data (collected from user feedback on generated summaries) and datasets created by domain experts (writers) to determine a "good" summary.
3. \*\*Define eval metrics\*\*
On a held-out set of 1000 reference transcripts â†’ summaries, the implementation should achieve a ROUGE-L score of at least 0.40 and coherence score of at least 80% using G-Eval.
4. \*\*Run and compare evals\*\*
Use the [Evals API](/docs/guides/evals) to create and run evals in the OpenAI dashboard.
5. \*\*Continuously evaluate\*\*
Set up continuous evaluation (CE) to run evals on every change, monitor your app to identify new cases of nondeterminism, and grow the eval set over time.
LLMs are better at discriminating between options. Therefore, evaluations should focus on tasks like pairwise comparisons, classification, or scoring against specific criteria instead of open-ended generation. Aligning evaluation methods with LLMs' strengths in comparison leads to more reliable assessments of LLM outputs or model comparisons.

### Example: Q&A over docs
To test your LLM-based application's ability to do Q&A over docs, your eval design might be:
1. \*\*Define eval objective\*\*
The model should be able to provide precise answers, recall context as needed to reason through user prompts, and provide an answer that satisfies the user's need.
2. \*\*Collect dataset\*\*
Use a mix of production data (collected from users' satisfaction with answers provided to their questions), hard-coded correct answers to questions created by domain experts, and historical data from logs.
3. \*\*Define eval metrics\*\*
Context recall of at least 0.85, context precision of over 0.7, and 70+% positively rated answers.
4. \*\*Run and compare evals\*\*
Use the [Evals API](/docs/guides/evals) to create and run evals in the OpenAI dashboard.
5. \*\*Continuously evaluate\*\*
Set up continuous evaluation (CE) to run evals on every change, monitor your app to identify new cases of nondeterminism, and grow the eval set over time.
When creating an eval dataset, o3 and GPT-4.1 are useful for collecting eval examples and edge cases. Consider using o3 to help you generate a diverse set of test data across various scenarios. Ensure your test data includes typical cases, edge cases, and adversarial cases. Use human expert labellers.
Identify where you need evals
-----------------------------
Complexity increases as you move from simple to more complex architectures. Here are four common architecture patterns:
\* [Single-turn model interactions](#single-turn-model-interactions)
\* [Workflows](#workflow-architectures)
\* [Single-agent](#single-agent-architectures)
\* [Multi-agent](#multi-agent-architectures)
Read about each architecture below to identify where nondeterminism enters your system. That's where you'll want to implement evals.

### Single-turn model interactions
In this kind of architecture, the user provides input to the model, and the model processes these inputs (along with any developer prompts provided) to generate a corresponding output.

#### Example
As an example, consider an online retail scenario. Your system prompt instructs the model to \*\*categorize the customer's question\*\* into one of the following:
\* `order\_status`
\* `return\_policy`
\* `technical\_issue`
\* `cancel\_order`
\* `other`
To ensure a consistent, efficient user experience, the model should \*\*only return the label that matches user intent\*\*. Let's say the customer asks, "What's the status of my order?"
||
|Inputs provided by the developer and user|Instruction following: Does the model accurately understand and act according to the provided instructions?Instruction following: Does the model prioritize the system prompt over a conflicting user prompt?|Does the model stay focused on the triage task or get swayed by the user's question?|
|Outputs generated by the model|Functional correctness: Are the model's outputs accurate, relevant, and thorough enough to fulfill the intended task or objective?|Does the model's determination of intent correctly match the expected intent?|

### Workflow architectures
As you look to solve more complex problems, you'll likely transition from a single-turn model interaction to a multistep workflow that chains together several model calls. Workflows don't introduce any new elements of nondeterminism, but they involve multiple underlying model interactions, which you can evaluate in isolation.

#### Example
Take the same example as before, where the customer asks about their order status. A workflow architecture triages the customer request and routes it through a step-by-step process:
1. Extracting an Order ID
2. Looking up the order details
3. Providing the order details to a model for a final response
Each step in this workflow has its own system prompt that the model must follow, putting all fetched data into a friendly output.
||
|Inputs provided by the developer and user|Instruction following: Does the model accurately understand and act according to the provided instructions?Instruction following: Does the model prioritize the system prompt over a conflicting user prompt?|Does the model stay focused on the triage task or get swayed by the user's question?Does the model follow instructions to attempt to extract an Order ID?Does the final response include the order status, estimated arrival date, and tracking number?|
|Outputs generated by the model|Functional correctness: Are the model's outputs are accurate, relevant, and thorough enough to fulfill the intended task or objective?|Does the model's determination of intent correctly match the expected intent?Does the final response have the correct order status, estimated arrival date, and tracking number?|

### Single-agent architectures
Unlike workflows, agents solve unstructured problems that require flexible decision making. An agent has instructions and a set of tools and dynamically selects which tool to use. This introduces a new opportunity for nondeterminism.
Tools are developer defined chunks of code that the model can execute. This can range from small helper functions to API calls for existing services. For example, `check\_order\_status(order\_id)` could be a tool, where it takes the argument `order\_id` and calls an API to check the order status.

#### Example
Let's adapt our customer service example to use a single agent. The agent has access to three distinct tools:
\* Order lookup tool
\* Password reset tool
\* Product FAQ tool
When the customer asks about their order status, the agent dynamically decides to either invoke a tool or respond to the customer. For example, if the customer asks, "What is my order status?" the agent can now follow up by requesting the order ID from the customer. This helps create a more natural user experience.
||
|Inputs provided by the developer and user|Instruction following: Does the model accurately understand and act according to the provided instructions?Instruction following: Does the model prioritize the system prompt over a conflicting user prompt?|Does the model stay focused on the triage task or get swayed by the user's question?Does the model follow instructions to attempt to extract an Order ID?|
|Outputs generated by the model|Functional correctness: Are the model's outputs are accurate, relevant, and thorough enough to fulfill the intended task or objective?|Does the model's determination of intent correctly match the expected intent?|
|Tools chosen by the model|Tool selection: Evaluations that test whether the agent is able to select the correct tool to use.Data precision: Evaluations that verify the agent calls the tool with the correct arguments. Typically these arguments are extracted from the conversation history, so the goal is to validate this extraction was correct.|When the user asks about their order status, does the model correctly recommend invoking the order lookup tool?Does the model correctly extract the user-provided order ID to the lookup tool?|

### Multi-agent architectures
As you add tools and tasks to your single-agent architecture, the model may struggle to follow instructions or select the correct tool to call. Multi-agent architectures help by creating several distinct agents who specialize in different areas. This triaging and handoff among multiple agents introduces a new opportunity for nondeterminism.
The decision to use a multi-agent architecture should be driven by your evals. Starting with a multi-agent architecture adds unnecessary complexity that can slow down your time to production.

#### Example
Splitting the single-agent example into a multi-agent architecture, we'll have four distinct agents:
1. Triage agent
2. Order agent
3. Account management agent
4. Sales agent
When the customer asks about their order status, the triage agent may hand off the conversation to the order agent to look up the order. If the customer changes the topic to ask about a product, the order agent should hand the request back to the triage agent, who then hands off to the sales agent to fetch product information.
||
|Inputs provided by the developer and user|Instruction following: Does the model accurately understand and act according to the provided instructions?Instruction following: Does the model prioritize the system prompt over a conflicting user prompt?|Does the model stay focused on the triage task or get swayed by the user's question?Assuming the lookup\_order call returned, does the order agent return a tracking number and delivery date (doesn't have to be the correct one)?|
|Outputs generated by the model|Functional correctness: Are the model's outputs are accurate, relevant, and thorough enough to fulfill the intended task or objective?|Does the model's determination of intent correctly match the expected intent?Assuming the lookup\_order call returned, does the order agent provide the correct tracking number and delivery date in its response?Does the order agent follow system instructions to ask the customer their reason for requesting a return before processing the return?|
|Tools chosen by the model|Tool selection: Evaluations that test whether the agent is able to select the correct tool to use.Data precision: Evaluations that verify the agent calls the tool with the correct arguments. Typically these arguments are extracted from the conversation history, so the goal is to validate this extraction was correct.|Does the order agent correctly call the lookup order tool?Does the order agent correctly call the refund\_order tool?Does the order agent call the lookup order tool with the correct order ID?Does the account agent correctly call the reset\_password tool with the correct account ID?|
|Agent handoff|Agent handoff accuracy: Evaluations that test whether each agent can appropriately recognize the decision boundary for triaging to another agent|When a user asks about order status, does the triage agent correctly pass to the order agent?When the user changes the subject to talk about the latest product, does the order agent hand back control to the triage agent?|
Create and combine different types of evaluators
------------------------------------------------
As you design your own evals, there are several specific evaluator types to choose from. Another way to think about this is what role you want the evaluator to play.

### Metric-based evals
Quantitative evals provide a numerical score you can use to filter and rank results. They provide useful benchmarks for automated regression testing.
\* \*\*Examples\*\*: Exact match, string match, ROUGE/BLEU scoring, function call accuracy, executable evals (executed to assess functionality or behaviorâ€”e.g., text2sql)
\* \*\*Challenges\*\*: May not be tailored to specific use cases, may miss nuance

### Human evals
Human judgment evals provide the highest quality but are slow and expensive.
\* \*\*Examples\*\*: Skim over system outputs to get a sense of whether they look better or worse; create a randomized, blinded test in which employees, contractors, or outsourced labeling agencies judge the quality of system outputs (e.g., ranking a small set of possible outputs, or giving each a grade of 1-5)
\* \*\*Challenges\*\*: Disagreement among human experts, expensive, slow
\* \*\*Recommendations\*\*:
\* Conduct multiple rounds of detailed human review to refine the scorecard
\* Implement a "show rather than tell" policy by providing examples of different score levels (e.g., 1, 3, and 8 out of 10)
\* Include a pass/fail threshold in addition to the numerical score
\* A simple way to aggregate multiple reviewers is to take consensus votes

### LLM-as-a-judge and model graders
Using models to judge output is cheaper to run and more scalable than human evaluation. Strong LLM judges like GPT-4.1 can match both controlled and crowdsourced human preferences, achieving over 80% agreement (the same level of agreement between humans).
\* \*\*Examples\*\*:
\* Pairwise comparison: Present the judge model with two responses and ask it to determine which one is better based on specific criteria
\* Single answer grading: The judge model evaluates a single response in isolation, assigning a score or rating based on predefined quality metrics
\* Reference-guided grading: Provide the judge model with a reference or "gold standard" answer, which it uses as a benchmark to evaluate the given response
\* \*\*Challenges\*\*: Position bias (response order), verbosity bias (preferring longer responses)
\* \*\*Recommendations\*\*:
\* Use pairwise comparison or pass/fail for more reliability
\* Use the most capable model to grade if you can (e.g., o3)â€”o-series models excel at auto-grading from rubics or from a collection of reference expert answers
\* Control for response lengths as LLMs bias towards longer responses in general
\* Add reasoning and chain-of-thought as reasoning before scoring improves eval performance
\* Once the LLM judge reaches a point where it's faster, cheaper, and consistently agrees with human annotations, scale up
\* Structure questions to allow for automated grading while maintaining the integrity of the taskâ€”a common approach is to reformat questions into multiple choice formats
\* Ensure eval rubrics are clear and detailed
No strategy is perfect. The quality of LLM-as-Judge varies depending on problem context while using expert human annotators to provide ground-truth labels is expensive and time-consuming.
Handle edge cases
-----------------
While your evaluations should cover primary, happy-path scenarios for each architecture, real-world AI systems frequently encounter edge cases that challenge system performance. Evaluating these edge cases is important for ensuring reliability and a good user experience.
We see these edge cases fall into a few buckets:

### Input variability
Because users provide input to the model, our system must be flexible to handle the different ways our users may interact, like:
\* Non-English or multilingual inputs
\* Formats other than input text (e.g., XML, JSON, Markdown, CSV)
\* Input modalities (e.g., images)
Your evals for instruction following and functional correctness need to accommodate inputs that users might try.

### Contextual complexity
Many LLM-based applications fail due to poor understanding of the context of the request. This context could be from the user or noise in the past conversation history.
Examples include:
\* Multiple questions or intents in a single request
\* Typos and misspellings
\* Short requests with minimal context (e.g., if a user just says: "returns")
\* Long context or long-running conversations
\* Tool calls that return data with ambiguous property names (e.g., `"on: 123"`, where "on" is the order number)
\* Multiple tool calls, sometimes leading to incorrect arguments
\* Multiple agent handoffs, sometimes leading to circular handoffs

### Personalization and customization
While AI improves UX by adapting to user-specific requests, this flexibility introduces many edge cases. Clearly define evals for use cases you want to specifically support and block:
\* Jailbreak attempts to get the model to do something different
\* Formatting requests (e.g., format as JSON, or use bullet points)
\* Cases where user prompts conflict with your system prompts
Use evals to improve performance
--------------------------------
When your evals reach a level of maturity that consistently measures performance, shift to using your evals data to improve your application's performance.
Learn more about [reinforcement fine-tuning](/docs/guides/reinforcement-fine-tuning) to create a data flywheel.
Other resources
---------------
For more inspiration, visit the [OpenAI Cookbook](https://cookbook.openai.com), which contains example code and links to third-party resources, or learn more about our tools for evals:
\* [Evaluating model performance](/docs/guides/evals)
\* [How to evaluate a summarization task](https://cookbook.openai.com/examples/evaluation/how\_to\_eval\_abstractive\_summarization)
\* [Fine-tuning](/docs/guides/model-optimization)
\* [Graders](/docs/guides/graders)
\* [Evals API reference](/docs/api-reference/evals)
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:42:55

Fine-tuning best practices
==========================
Learn best practices to fine-tune OpenAI models and get better peformance, optimization, and task-specific model behavior.
If you're not getting strong results with a fine-tuned model, consider the following iterations on your process.

### Iterating on data quality
Below are a few ways to consider improving the quality of your training data set:
\* Collect examples to target remaining issues.
\* If the model still isn't good at certain aspects, add training examples that directly show the model how to do these aspects correctly.
\* Scrutinize existing examples for issues.
\* If your model has grammar, logic, or style issues, check if your data has any of the same issues. For instance, if the model now says "I will schedule this meeting for you" (when it shouldn't), see if existing examples teach the model to say it can do new things that it can't do
\* Consider the balance and diversity of data.
\* If 60% of the assistant responses in the data says "I cannot answer this", but at inference time only 5% of responses should say that, you will likely get an overabundance of refusals.
\* Make sure your training examples contain all of the information needed for the response.
\* If we want the model to compliment a user based on their personal traits and a training example includes assistant compliments for traits not found in the preceding conversation, the model may learn to hallucinate information.
\* Look at the agreement and consistency in the training examples.
\* If multiple people created the training data, it's likely that model performance will be limited by the level of agreement and consistency between people. For instance, in a text extraction task, if people only agreed on 70% of extracted snippets, the model would likely not be able to do better than this.
\* Make sure your all of your training examples are in the same format, as expected for inference.

### Iterating on data quantity
Once you're satisfied with the quality and distribution of the examples, you can consider scaling up the number of training examples. This tends to help the model learn the task better, especially around possible "edge cases". We expect a similar amount of improvement every time you double the number of training examples. You can loosely estimate the expected quality gain from increasing the training data size by:
\* Fine-tuning on your current dataset
\* Fine-tuning on half of your current dataset
\* Observing the quality gap between the two
In general, if you have to make a tradeoff, a smaller amount of high-quality data is generally more effective than a larger amount of low-quality data.

### Iterating on hyperparameters
Hyperparameters control how the model's weights are updated during the training process. A few common options are:
\* \*\*Epochs\*\*: An epoch is a single complete pass through your entire training dataset during model training. You will typically run multiple epochs so the model can iteratively refine its weights.
\* \*\*Learning rate multiplier\*\*: Adjusts the size of changes made to the model's learned parameters. A larger multiplier can speed up training, while a smaller one can lean to slower but more stable training.
\* \*\*Batch size\*\*: The number of examples the model processes in one forward and backward pass before updating its weights. Larger batches slow down training, but may produce more stable results.
We recommend initially training without specifying any of these, allowing us to pick a default for you based on dataset size, then adjusting if you observe the following:
\* If the model doesn't follow the training data as much as expected, increase the number of epochs by 1 or 2.
\* This is more common for tasks for which there is a single ideal completion (or a small set of ideal completions which are similar). Some examples include classification, entity extraction, or structured parsing. These are often tasks for which you can compute a final accuracy metric against a reference answer.
\* If the model becomes less diverse than expected, decrease the number of epochs by 1 or 2.
\* This is more common for tasks for which there are a wide range of possible good completions.
\* If the model doesn't appear to be converging, increase the learning rate multiplier.
You can set the hyperparameters as shown below:
Setting hyperparameters
```javascript
const fineTune = await openai.fineTuning.jobs.create({
training\_file: "file-abc123",
model: "gpt-4o-mini-2024-07-18",
method: {
type: "supervised",
supervised: {
hyperparameters: { n\_epochs: 2 },
},
},
});
```
```python
from openai import OpenAI
client = OpenAI()
client.fine\_tuning.jobs.create(
training\_file="file-abc123",
model="gpt-4o-mini-2024-07-18",
method={
"type": "supervised",
"supervised": {
"hyperparameters": {"n\_epochs": 2},
},
},
)
```
Adjust your dataset
-------------------
Another option if you're not seeing strong fine-tuning results is to go back and revise your training data. Here are a few best practices as you collect examples to use in your dataset.

### Training vs. testing datasets
After collecting your examples, split the dataset into training and test portions. The training set is for fine-tuning jobs, and the test set is for [evals](/docs/guides/evals).
When you submit a fine-tuning job with both training and test files, we'll provide statistics on both during the course of training. These statistics give you signal on how much the model's improving. Constructing a test set early on helps you [evaluate the model after training](/docs/guides/evals) by comparing with the test set benchmark.

### Crafting prompts for training data
Take the set of instructions and prompts that worked best for the model prior to fine-tuning, and include them in every training example. This should let you reach the best and most general results, especially if you have relatively few (under 100) training examples.
You may be tempted to shorten the instructions or prompts repeated in every example to save costs. Without repeated instructions, it may take more training examples to arrive at good results, as the model has to learn entirely through demonstration.

### Multi-turn chat in training data
To train the model on [multi-turn conversations](/docs/guides/conversation-state), include multiple `user` and `assistant` messages in the `messages` array for each line of your training data.
Use the optional `weight` key (value set to either 0 or 1) to disable fine-tuning on specific assistant messages. Here are some examples of controlling `weight` in a chat format:
```jsonl
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "Paris", "weight": 0}, {"role": "user", "content": "Can you be more sarcastic?"}, {"role": "assistant", "content": "Paris, as if everyone doesn't know that already.", "weight": 1}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "William Shakespeare", "weight": 0}, {"role": "user", "content": "Can you be more sarcastic?"}, {"role": "assistant", "content": "Oh, just some guy named William Shakespeare. Ever heard of him?", "weight": 1}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "384,400 kilometers", "weight": 0}, {"role": "user", "content": "Can you be more sarcastic?"}, {"role": "assistant", "content": "Around 384,400 kilometers. Give or take a few, like that really matters.", "weight": 1}]}
```

### Token limits
Token limits depend on model. Here's an overview of the maximum allowed context lengths:
|Model|Inference context length|Examples context length|
|---|---|---|
|gpt-4.1-2025-04-14|128,000 tokens|65,536 tokens|
|gpt-4.1-mini-2025-04-14|128,000 tokens|65,536 tokens|
|gpt-4.1-nano-2025-04-14|128,000 tokens|65,536 tokens|
|gpt-4o-2024-08-06|128,000 tokens|65,536 tokens|
|gpt-4o-mini-2024-07-18|128,000 tokens|65,536 tokens|
Examples longer than the default are truncated to the maximum context length, which removes tokens from the end of the training example. To make sure your entire training example fits in context, keep the total token counts in the message contents under the limit.
Compute token counts with [the tokenizer tool](/tokenizer) or by using code, as in this [cookbook example](https://cookbook.openai.com/examples/How\_to\_count\_tokens\_with\_tiktoken.ipynb).
Before uploading your data, you may want to check formatting and potential token costs - an example of how to do this can be found in the cookbook.
[
Fine-tuning data format validation
Learn about fine-tuning data formatting
](https://cookbook.openai.com/examples/chat\_finetuning\_data\_prep)
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:43:03

Reinforcement fine-tuning use cases
===================================
Learn use cases and best practices for reinforcement fine-tuning.
[Reinforcement fine-tuning](/docs/guides/reinforcement-fine-tuning) (RFT) provides a way to improve your model's performance at specific tasks. The task must be clear and have verifiable answers.
When to use reinforcement fine-tuning
-------------------------------------
Agentic workflows are designed to make decisions that are both correct and verifiable. RFT can help by providing explicit rubrics and using codeâ€‘based or LLMâ€‘based graders to measure functional success, factual accuracy, or policy compliance.
Across early users, three clear use cases have emerged:
1. \*\*Turn instructions into working code\*\*: Convert open-ended prompts into structured code, configs, or templates that must pass deterministic tests.
2. \*\*Pull facts into a clean format\*\*: Extract verifiable facts and summaries from messy, unstructured text and return JSON-structured or other schema-based outputs.
3. \*\*Apply complex rules correctly\*\*: Make fine-grained label or policy decisions when the information provided is nuanced, large in quantity, hierarchical, or high-stakes.
[Ready to use reinforcement fine-tuning? Skip to the guide â†’](/docs/guides/reinforcement-fine-tuning)

### 1\. Turn instructions into working code
In this use case, models reason over hidden domain constraints to produce structured outputs like code, queries, or infrastructure templates. Outputs must satisfy multiple correctness conditions, and success is usually deterministically graded: the artifact either compiles, passes tests, or meets an explicit schema.

#### Wiring verification IPs for semiconductor design
Use case
> \*\*Company\*\*: [ChipStack](https://www.chipstack.ai) is building the next-generation of AI-powered tools for chip design and verification, aimed at significantly reducing the time and cost of developing and validating complex semiconductor chips.
>
> \*\*Problem to solve\*\*: One task that's challenging and time-consuming for humans is binding design interfaces to verification IPs (pre-created verification components that, when properly applied, can signifcantly enhance quality and coverage of verification). There are many verification IPs, and each can contain dozens to hundreds of signals that may be mapped. Someone must understand this domain well in order to apply the verification IP correctly.
>
> \*\*Objective\*\*: To train OpenAI reasoning models to do this instead, ChipStack prepared a dataset consisting of less than 50 samples, then performed several RFT variations. For the final evaluation report, they ran this evaluation set three times against each model and variationâ€”o1-mini base and fine-tuned, o3-mini base and fine-tunedâ€”and averaged the results per-sample then overall.
Prompt
> Below is a piece of example data provided.
```text
[
{â€œnameâ€: â€œBLOCK\_SIZEâ€, â€œvalueâ€: â€œ8â€},
{â€œnameâ€: â€œADDR\_WIDTHâ€, â€œvalueâ€: â€œ4â€}
]
```
Grader code
> Below is a grader definition in Python of a string map, represented as a list of objects with `name` and `value` properties.
>
> Conceptually, this is meant to model a type like `Dict[str, str]`.
```python
{
"type": "python",
"name": "donors\_caas",
"image\_tag": "alpha",
"source": "from collections import Counter
def grade(sample: dict[str, str], item: dict[str, str]) -> float:

# multisets of (name, value) pairs
predicted = sample[\"output\_json\"][\"predicted\"]
expected = item[\"reference\_answer\"]
pred\_counts = Counter((d[\"name\"], d[\"value\"]) for d in predicted)
exp\_counts = Counter((d[\"name\"], d[\"value\"]) for d in expected)
true\_pos = sum(min(pred\_counts[p], exp\_counts[p]) for p in pred\_counts)
pred\_total = sum(pred\_counts.values())
exp\_total = sum(exp\_counts.values())
precision = true\_pos / pred\_total if pred\_total else 0.0
recall = true\_pos / exp\_total if exp\_total else 0.0
if precision + recall == 0.0:
return 0.0
return 2 \* precision \* recall / (precision + recall)"
}
```
Results
> For both o1-mini and o3-mini, performance improved by ~12 percentage points. The fine-tuned variants got much better about recognizing when not to apply wiring. Many commercial verification IPs can contain hundreds of optional signals, most of which are not meant to be applied.
>
> "Thanks to powerful base models and easy-to-use Reinforced Fine-Tuning APIs, we were able to significantly boost performance on our task with a small set of high-quality samples."
>
> â€”[ChipStack](https://www.chipstack.ai), next-generation of AI-powered tools for chip design and verification

#### Production-ready API snippets that compile and pass AST checks
Use case
> \*\*Company\*\*: [Runloop](https://www.runloop.ai) is a platform for AI-powered coding agents to be deployed into production and built with public and custom benchmarking capabilities to refine performance.
>
> \*\*Problem to solve\*\*: Runloop wanted to improve model performance at using third-party APIs, such as the Stripe API, which can be large and complex without a human in the loop. If they could train a model to use the Stripe API, Runloop could turn economically impactful business cases into working code.
>
> \*\*Objective\*\*: Their goal was teaching the model to master usage of the Stripe API, including writing complete code snippets for arbitrary user requests by either adapting information from existing integration guides, merging information from multiple guides, or inferring information not explicitly stated in the guides. They used RFT with two primary rewards:
>
> 1. Reward the model for outputting the answer in a Markdown format that aligns with expectation of how a "dynamic" integration guide should look.
> 2. Reward the model for producing "correct" code snippets by validating the outputted code via AST Grep. This allows them to confirm the model is making the correct Stripe SDK calls with the correct parameters and in some cases even in the correct order.
Grader code
```python

# Note this file gets uploaded to the OpenAI API as a grader
from ast\_grep\_py import SgRoot
from pydantic import BaseModel, Field # type: ignore
from typing import Any, List, Optional
import re
SUPPORTED\_LANGUAGES = ['typescript', 'javascript', 'ts', 'js']
class CodeBlock(BaseModel):
language: str = Field(
description="Programming language of the code block (e.g., 'python', 'javascript')",
examples=["python", "javascript", "typescript"]
)
path: str = Field(
description="Target file path where the code should be written",
examples=["main.py", "src/app.js", "index.html"]
)
code: str = Field(
description="Actual code content extracted from the code block"
)
class ASTGrepPattern(BaseModel):
file\_path\_mask: str = Field(..., description="The file path pattern to match against")
pattern: str = Field(..., description="The main AST grep pattern to search for")
additional\_greps: Optional[List[str]] = Field(
default=None,
description="Additional patterns that must also be present in the matched code"
)
def extract\_code\_blocks(llm\_output: str) -> List[CodeBlock]:

# Regular expression to match code blocks with optional language and path
try:
pattern = r"```(\w+\s+)?([\w./-]+)?\n([\s\S]\*?)\n```"
matches = list(re.finditer(pattern, llm\_output, re.DOTALL))
print(f"Found {len(matches)} code blocks in the LLM output")

# Check if any code blocks were found
if not matches:
raise Exception("No code blocks found in the LLM response")
code\_blocks: list[CodeBlock] = []
for match in matches:
language = match.group(1) or ""
path = match.group(2) or ""
code = match.group(3)

# Clean the path and language
path = path.strip()
language = language.strip()

# If path is relative (doesn't start with /), prefix with /home/user/testbed/
if path and not path.startswith("/"):
original\_path = path
path = f"/home/user/testbed/{path}"
print(
f"Converting relative path '{original\_path}' to absolute path '{path}'"
)
code\_blocks.append(
CodeBlock(language=language, path=path, code=code.strip())
)

# Check for missing language or path in code blocks
missing\_language = [
i for i, block in enumerate(code\_blocks) if not block.language
]
missing\_path = [i for i, block in enumerate(code\_blocks) if not block.path]
if missing\_language:
print(
f"WARNING: Code blocks at positions {missing\_language} are missing language identifiers"
)
raise Exception(
f"Code blocks at positions {missing\_language} are missing language identifiers"
)
if missing\_path:
print(
f"WARNING: Code blocks at positions {missing\_path} are missing file paths"
)
raise Exception(
f"Code blocks at positions {missing\_path} are missing file paths"
)
paths = [block.path for block in code\_blocks if block.path]
print(
f"Successfully extracted {len(code\_blocks)} code blocks with paths: {', '.join(paths)}"
)
except Exception as e:
print(f"Error extracting code blocks: {str(e)}")
raise
return code\_blocks
def calculate\_ast\_grep\_score(code\_blocks: List[CodeBlock], ast\_greps: Any) -> float:

# Convert ast\_greps to list if it's a dict
if isinstance(ast\_greps, dict):
ast\_greps = [ast\_greps]

# Parse each grep pattern into the Pydantic model
parsed\_patterns: List[ASTGrepPattern] = []
for grep in ast\_greps:
try:
pattern = ASTGrepPattern(\*\*grep)
parsed\_patterns.append(pattern)
except Exception as e:
print(f"Error parsing AST grep pattern: {e}")
return 0.0
if not parsed\_patterns:
return 0.0
total\_score = 0.0
pattern\_count = len(parsed\_patterns)

# Filter code blocks to only include TypeScript and JavaScript files
supported\_blocks = [
block for block in code\_blocks
if block.language.lower() in SUPPORTED\_LANGUAGES
]
if not supported\_blocks:
print("No TypeScript or JavaScript code blocks found to analyze")
return 0.0
for pattern in parsed\_patterns:

# Find matching code blocks based on path prefix
matching\_blocks = [
block for block in supported\_blocks
if block.path.startswith(pattern.file\_path\_mask)
]
if not matching\_blocks:
print(f"No matching code blocks found for path prefix: {pattern.file\_path\_mask}")
continue
pattern\_found = False
for block in matching\_blocks:
try:

# Create AST root for the code block
root = SgRoot(block.code, block.language)
node = root.root()

# Check main pattern
matches = node.find(pattern=pattern.pattern)
if not matches:
continue

# If we have additional greps, check them too
if pattern.additional\_greps:
all\_additional\_found = True
for additional\_grep in pattern.additional\_greps:
if additional\_grep not in block.code:
all\_additional\_found = False
break
if not all\_additional\_found:
continue

# If we get here, we found a match with all required patterns
pattern\_found = True
break
except Exception as e:
print(f"Error processing code block {block.path}: {e}")
continue
if pattern\_found:
total\_score += 1.0

# Return average score across all patterns
return total\_score / pattern\_count if pattern\_count > 0 else 0.0
def grade\_format(output\_text: str) -> float:

# Find  and  tags
plan\_start = output\_text.find('')
plan\_end = output\_text.find('')

# Find  `and`  tags
code\_start = output\_text.find('`')
code_end = output_text.find('`')
reward = 0.0
if plan\_start == -1 or plan\_end == -1 or code\_start == -1 or code\_end == -1:
print(f'missing plan or code tags. format reward: {reward}')
return reward
reward += 0.1 # total: 0.1
if not (plan\_start < plan\_end < code\_start < code\_end):
print(f'tags present but not in the correct order. format reward: {reward}')
return reward
reward += 0.1 # total: 0.2

# Check if there are any stray tags
plan\_tags = re.findall(r'', output\_text)
code\_tags = re.findall(r'', output\_text)
if len(plan\_tags) != 2 or len(code\_tags) != 2:
print(f'found stray plan or code tags. format reward: {reward}')
return reward
reward += 0.2 # total: 0.4

# Extract content after  tag
after\_tags = output\_text[code\_end + len(''):].strip()
if after\_tags:
print(f'found text after code tags. format reward: {reward}')
return reward
reward += 0.2 # total: 0.6

# Extract content inside  tags
plan\_content = output\_text[plan\_start + len(''):plan\_end].strip()
if not plan\_content:
print(f'no plan content found. format reward: {reward}')
return reward
reward += 0.1 # total: 0.7

# Extract content inside  `tags
code_content = output_text[code_start + len(''):code_end].strip()
if not code_content:
print(f'no code content found. format reward: {reward}')
return reward
reward += 0.1 # total: 0.8

# Extract content between`  and  `tags
between_tags = output_text[plan_end + len('`'):code\_start].strip()
if between\_tags:
print(f'found text between plan and code tags. format reward: {reward}')
return reward
reward += 0.2 # total: 1.0
if reward == 1.0:
print(f'global format reward: {reward}')
return reward
def grade(sample: Any, item: Any) -> float:
try:
output\_text = sample["output\_text"]
format\_reward = grade\_format(output\_text)
if format\_reward < 1.0:
return format\_reward

# Extract code content for grading
code\_start = output\_text.find('`')
code_end = output_text.find('`')
code\_to\_grade: str = output\_text[code\_start + len('`'):code_end].strip()
code_blocks: List[CodeBlock] = []
try:
code_blocks = extract_code_blocks(code_to_grade)
except Exception as e:
print(f'error extracting code blocks: {e}')
return 0.5
ast_greps = item["reference_answer"]["ast_greps"]
ast_grep_score = calculate_ast_grep_score(code_blocks, ast_greps)
return (format_reward + ast_grep_score) / 2.0
except Exception as e:
print(f"Error during grading: {str(e)}")
return 0.0
```
Results
> Looking at the total reward (format and AST Grep) together, Runloop has seen improvements of on average **12%** of the RFT model compared to the base o3-mini model on the benchmark.
>
> They implement two types of tests, one providing explicit content from the integration guides (assessing reasoning and instruction following) and one without (assessing knowledge recall). Both variants saw improvement of over **8%**.
>
> â€œOpenAIs RFT platform gives us access to the best generalized reasoning models in the world, with the toolset to supercharge that reasoning on problem domains important to our business.â€
>
> â€”[Runloop](https://www.runloop.ai/)

#### Correct handling of conflicts and dupes in a schedule manager
Use case
> **Company**: [Milo](https://www.joinmilo.com) helps busy parents manage chaotic family schedules by converting messy inputsâ€”like text convos with to-dos, school newsletter PDFs, weekly reminders, sports schedule emailsâ€”into reliable calendar and list actions.
>
> **Problem to solve**: Base GPT-4o prompting and SFT fell short of trust thresholds.
>
> **Objective**: Milo used RFT to properly create coding tasks like event vs. list classification, recurrence rule generation, accurate updates and deletes, conflict detection, and strict output formatting. They defined a grader that checked whether generated item objects were complete, categorized correctly, and were a duplicate or had a calendar conflict.
Results
> Results showed performance improvements across the board, with average correctness scores **increasing from 0.86 to 0.91**, while the most challenging scenarios improved from **0.46 to 0.71** (where a perfect score=1).
>
> "Accuracy isn't just a metricâ€”it's peace of mind for busy parents. These are still early days but with such important improvements in base performance, we're able to push more aggressively into complex reasoning needs."
>
> "Navigating and supporting family dynamics involves understanding nuanced implications of the data. Take conflictsâ€”knowing soccer for Ethan conflicts with Ella's recital because Dad has to drive both kids goes deeper than simple overlapping times."
>
> â€”[Milo](https://www.joinmilo.com), AI scheduling tool for families

### 2\. Pull facts into a clean format
These tasks typically involve subtle distinctions that demand clear classification guidelines. Successful framing requires explicit and hierarchical labeling schemes defined through consensus by domain experts. Without consistent agreement, grading signals become noisy, weakening RFT effectiveness.

#### Assigning ICD-10 medical codes
Use case
> **Company**: [Ambience](https://www.ambiencehealthcare.com) is an AI platform that eliminates administrative burden for clinicians and ensures accurate, compliant documentation across 100+ specialties, helping physicians focus on patient care while increasing documentation quality and reducing compliance risk for health systems.
>
> **Problem to solve**: ICD-10 coding is one of the most intricate administrative tasks in medicine. After every patient encounter, clinicians must map each diagnosis to one of ~70,000 codesâ€”navigating payor-specific rules on specificity, site-of-care, and mutually exclusive pairings. Errors can trigger audits and fines that stretch into nine figures.
>
> **Objective**: Using reinforcement fine-tuning on OpenAI frontier models, Ambience wanted to train a reasoning system that listens to the visit audio, pulls in relevant EHR context, and recommends ICD-10 codes with accuracy exceeding expert clinicians.
Results
> Ambience achieved model improvements that can lead human experts.
>
> On a gold-panel test set spanning hundreds of encounters, reinforcement fine-tuning moved the model from trailing humans to leading them by **12 pointsâ€”eliminating roughly one quarter of the coding errors trained physicians make**:
>
> * o3-mini (base): 0.39 (-6 pts)
> * Physician baseline: 0.45
> * RFT-tuned o3-mini: 0.57 (+12 pts)
>
> The result is a real-time, point-of-care coding support that can raise reimbursement integrity while reducing compliance risk.
>
> â€œAccurate ICD-10 selection is mission-critical for compliant documentation. RFT unlocked a new level of coding precision we hadnâ€™t seen from any foundation model and set a new bar for automated coding.â€
>
> â€”[Ambience Healthcare](https://www.ambiencehealthcare.com)

#### Extracting excerpts to support legal claims
Use case
> **Company**: [Harvey](https://www.harvey.ai) is building AI that legal teams trustâ€”and that trust hinges on retrieving precisely the right evidence from a sprawling corpora of contracts, statutes, and case law. Legal professionals arenâ€™t satisfied with models that merely generate plausible-sounding summaries or paraphrased answers. They demand verifiable citationsâ€”passages that can be traced directly back to source documents.
>
> **Problem to solve**: Harveyâ€™s clients use its models to triage litigation risk, construct legal arguments, and support due diligence for legal professionalsâ€”all tasks where a single missed or misquoted sentence can flip an outcome. Models must be able to parse long, dense legal documents and extract only the portions that matter. In practice, these inputs are often messy and inconsistent: some claims are vague, while others hinge on rare legal doctrines buried deep in boilerplate.
>
> **Objective**: The taskâ€™s requirements are to interpret nuanced legal claims, navigate long-form documents, and select on-point support with verbatim excerpts.
Prompt
```text

## Instructions
You will be provided with a question and a text excerpt. Identify any passages in the text that are directly relevant to answering the question.
- If there are no relevant passages, return an empty list.
- Passages must be copied **exactly** from the text. Do not paraphrase or summarize.

## Excerpt
"""{text_excerpt}"""
```
Grader
```python
from rapidfuzz import fuzz

# Similarity ratio helper
def fuzz_ratio(a: str, b: str) -> float:
"""Return a normalized similarity ratio using RapidFuzz.
"""
if len(a) == 0 and len(b) == 0:
return 1.0
return fuzz.ratio(a, b) / 100.0

# Main grading entrypoint (must be named `grade`)
def grade(sample: dict, item: dict) -> float:
"""Compute an F1â€‘style score for citation extraction answers using RapidFuzz.
"""
model_passages = (sample.get('output_json') or {}).get('passages', [])
ref_passages = (item.get('reference_answer') or {}).get('passages', [])

# If there are no reference passages, return 0.
if not ref_passages:
return 0.0

# Recall: average best match for each reference passage.
recall_scores = []
for ref in ref_passages:
best = 0.0
for out in model_passages:
score = fuzz_ratio(ref, out)
if score > best:
best = score
recall_scores.append(best)
recall = sum(recall_scores) / len(recall_scores)

# Precision: average best match for each model passage.
if not model_passages:
precision = 0.0
else:
precision_scores = []
for out in model_passages:
best = 0.0
for ref in ref_passages:
score = fuzz_ratio(ref, out)
if score > best:
best = score
precision_scores.append(best)
precision = sum(precision_scores) / len(precision_scores)
if precision + recall == 0:
return 0.0
return 2 * precision * recall / (precision + recall)
```
Results
> After reinforcement fine-tuning, Harvey saw a **20% increase** in the F1 score:
>
> * Baseline F1: 0.563
> * Post-RFT F1 - 0.6765
>
> Using RFT, Harvey significantly improved legal fact-extraction performance, surpassing GPT-4o efficiency and accuracy. Early trials showed RFT **winning or tying in 93% of comparisons** against GPT-4o.
>
> â€œThe RFT model demonstrated comparable or superior performance to GPT-4o, but with significantly faster inference, proving particularly beneficial for real-world legal use cases.
>
> â€”[Harvey](https://www.harvey.ai), AI for legal teams

### 3\. Apply complex rules correctly
This use case involves pulling verifiable facts or entities from unstructured inputs into clearly defined schemas (e.g., JSON objects, condition codes, medical codes, legal citations, or financial metrics).
Successful extraction tasks typically benefit from precise, continuous grading methodologiesâ€”like span-level F1 scores, fuzzy text-matching metrics, or numeric accuracy checksâ€”to evaluate how accurately the extracted information aligns with ground truth. Define explicit success criteria and detailed rubrics. Then, the model can achieve reliable, repeatable improvements.

#### Expert-level reasoning in tax analysis
Use case
> **Company**: [Accordance](https://www.accordance.com) is building a platform for tax, audit, and CPA teams.
>
> **Problem to solve**: Taxation is a highly complex domain, requiring deep reasoning across nuanced fact patterns and intricate regulations. It's also a field that continues changing.
>
> **Objective**: Accordance wanted a high-trust system for sophisticated tax scenarios while maintaining accuracy. Unlike traditional hardcoded software, it's important that their data extraction tool adapts as the tax landscape evolves.
Grader code
```text
[+0.05] For correctly identifying Alex (33.33%), Barbara (33.33% â†’ 20%), Chris (33.33%), and Dana (13.33%) ownership percentages
[+0.1] For correctly calculating Barbara's annual allocation as 26.67% and Dana's as 6.67% without closing of books
[+0.15] For properly allocating Alex ($300,000), Barbara ($240,030), Chris ($300,000), and Dana ($60,030) ordinary income
[+0.1] For calculating Alex's ending stock basis as $248,333 and debt basis as $75,000
[+0.05] For calculating Barbara's remaining basis after sale as $264,421
[+0.1] For calculating AAA before distributions as $1,215,000 and ending AAA as $315,000
[+0.1] For identifying all distributions as tax-free return of capital under AAA
[+0.1] For calculating Barbara's capital gain on stock sale as $223,720 ($400,000 - $176,280)
[+0.1] For explaining that closing of books would allocate based on actual half-year results
[+0.05] For identifying the ordering rules: AAA first, then E&P ($120,000), then remaining basis
[+0.05] For noting distributions exceeding $1,215,000 would be dividends up to $120,000 E&P
[+0.05] For correctly accounting for separately stated items in basis calculations (e.g., $50,000 Section 1231 gain)
```
Results
> By collaborating with OpenAI and their in-house tax experts, Accordance achieved:
>
> * Almost **40% improvement** in tax analysis tasks over base models
> * Superior performance compared to all other leading models on benchmarks like TaxBench
> * The RFT-trained models demonstrated an ability to handle advanced tax scenarios with high accuracyâ€”when evaluated by tax professionals, Accordanceâ€™s fine-tuned models showed expert-level reasoning, with the potential to save thousands of hours of manual work
>
> â€œWeâ€™ve achieved a 38.89% improvement in our tax analysis tasks over base models and significantly outperformed all other leading models on key tax benchmarks (including TaxBench). The RFT-trained modelsâ€™ abilities to handle sophisticated tax scenarios while maintaining accuracy demonstrates the readiness of reinforcement fine-tuningâ€”and AI more broadlyâ€”for professional applications. Most importantly, RFT provides a foundation for continuous adaptation as the tax landscape evolves, ensuring sustained value and relevance. When evaluated by tax experts, our fine-tuned models demonstrated expert-level reasoning capabilities that will save thousands of professional hoursâ€”this isnâ€™t just an incremental improvement, itâ€™s a paradigm shift in how tax work can be done.â€
>
> â€”[Accordance](https://www.accordance.com/), AI tax accounting company

#### Enforcement of nuanced content moderation policies
Use case
> **Company**: [SafetyKit](https://www.safetykit.com) is a risk and compliance platform that helps organizations make decisions across complex content moderation workflows.
>
> **Porblem to solve**: These systems must handle large volumes of content and apply intricate policy logic that requires multistep reasoning. Because of the volume of data and subtle distinctions in labelling, these types of tasks can be difficult for general purpose models.
>
> **Objective**: SafetyKit aimed to replace multiple nodes in their most complex workflows with a single reasoning agent using a reinforcement fine-tuned model. The goal is to reduce SafetyKitâ€™s time-to-market for novel policy enforcements even in challenging, nuanced domains.
Results
> SafetyKit is using their o3-mini RFT model to support advanced content moderation capabilities, ensuring user safety for one of the largest AI chatbot companies in the world. They have successfully improved F1-score **from 86% to 90%**, soon to replace dozens of 4o calls within their production pipeline.
>
> "SafetyKitâ€™s RFT-enabled moderation achieved substantial improvements in nuanced content moderation tasks, crucial for safeguarding users in dynamic, real-world scenarios."
>
> â€”[SafetyKit](https://www.safetykit.com)

#### Legal document reviews, comparisons, and summaries
Use case
> **Company**: [Thomson Reuters](https://www.thomsonreuters.com) is an AI and technology company empowering professionals with trusted content and workflow automation.
>
> **Problem to solve**: Legal professionals must read through large amounts of content before making any decisions. Thomson Reuter's CoCounsel product is designed to help these experts move faster by providing an AI assistant with content and industry knowledge. The models that power this tool must understand complex legal rules.
>
> **Objective**: Thomson Reuters aimed to create a reinforcement fine-tuned model excelling in legal AI skills. They conducted preliminary evaluations of RFT to see if they could achieve model performance improvements, using specialized datasets from three highly-used CoCounsel Legal AI skills for legal professionals:
>
> 1. Review documents: Generates detailed answers to questions asked against contracts, transcripts, and other legal documents
> 2. Compare documents: Highlights substantive differences between two or more different contracts or documents
> 3. Summarize: Summarizes the most important information within one or more documents to enable rapid legal review
Results
> ![Provide example data and create a fine-tuning job to optimize model performance for your use case](https://cdn.openai.com/API/docs/images/thomsonreuters-results.png)
>
> "LLM as a judge has been helpful in demonstrating the possibility of improving upon the reasoning models - in preliminary evaluations, the RFT model consistently performed better than the baseline o3-mini and o1 model"
>
> â€”[Thomson Reuters](https://www.thomsonreuters.com/), AI and technology company
Evals are the foundation
------------------------
**Before implementing RFT, we strongly recommended creating and running an eval for the task you intend to fine-tune on**. If the model you intend to fine-tune scores at either the absolute minimum or absolute maximum possible score, then RFT wonâ€™t be useful to you.
RFT works by reinforcing better answers to provided prompts. If we canâ€™t distinguish the quality of different answers (i.e., if they all receive the minimum or maximum possible score), then there's no training signal to learn from. However, if your eval scores somewhere in the range between the minimum and maximum possible scores, there's enough data to work with.
An effective eval reveals opportunities where human experts consistently agree but current frontier models struggle, presenting a valuable gap for RFT to close. [Get started with evals](/docs/guides/evals).
How to get better results from RFT
----------------------------------
To see improvements in your fine-tuned model, there are two main places to revisit and refine: making sure your task is well defined, and making your grading scheme more robust.

### Reframe or clarify your task
Good tasks give the model a fair chance to learn and let you quantify improvements.
* **Start with a task the model can already solve occasionally**. RFT works by sampling many answers, keeping what looks best, and nudging the model toward those answers. If the model never gets the answer correct today, it cannot improve.
* **Make sure each answer can be graded**. A grader must read an answer and produce a score without a person in the loop. We support multiple [grader types](/docs/guides/graders), including custom Python graders and LLM judges. If you can't write code to judge the answer with an available grader, RFT is not the right tool.
* **Remove doubt about the â€œrightâ€ answer**. If two careful people often disagree on the solution, the task is too fuzzy. Rewrite the prompt, add context, or split the task into clearer parts until domain experts agree.
* **Limit lucky guesses**. If the task is multiple choice with one obvious best pick, the model can win by chance. Add more classes, ask for short openâ€‘ended text, or tweak the format so guessing is costly.

### Strengthen your grader
Clear, robust grading schemes are essential for RFT.
* **Produce a smooth score, not a pass/fail stamp**. A score that shifts gradually as answers improve provides a better training signal.
* **Guard against reward hacking**. This happens when the model finds a shortcut that earns high scores without real skill.
* **Avoid skewed data**. Datasets in which one label shows up most of the time invite the model to guess that label. Balance the set or upâ€‘weight rare cases so the model must think.
* **Use an LLM judge when code falls short**. For rich, openâ€‘ended answers, have a [separate OpenAI model grade](/docs/guides/graders#model-graders) your fine-tuned model's answers. Make sure you:
* **Evaluate the judge**: Run multiple candidate responses and correct answers through your LLM judge to ensure the grade returned is stable and aligned with preference.
* **Provide few-shot examples**. Include great, fair, and poor answers in the prompt to improve the grader's effectiveness.
Learn more about [grader types](/docs/guides/graders).
Other resources
---------------
For more inspiration, visit the [OpenAI Cookbook](https://cookbook.openai.com), which contains example code and links to third-party resources, or learn more about our models and reasoning capabilities:
* [Meet the models](/docs/models)
* [Reinforcement fine-tuning guide](/docs/guides/reinforcement-fine-tuning)
* [Graders](/docs/guides/graders)
* [Model optimization overview](/docs/guides/model-optimization)
Was this page useful?`


## Imported snippet â€“ 2025-07-03 14:43:09

Model selection
===============
Choose the best model for performance and cost.
Choosing the right model, whether GPT-4o or a smaller option like GPT-4o-mini, requires balancing \*\*accuracy\*\*, \*\*latency\*\*, and \*\*cost\*\*. This guide explains key principles to help you make informed decisions, along with a practical example.
Core principles
---------------
The principles for model selection are simple:
\* \*\*Optimize for accuracy first:\*\* Optimize for accuracy until you hit your accuracy target.
\* \*\*Optimize for cost and latency second:\*\* Then aim to maintain accuracy with the cheapest, fastest model possible.

### 1\. Focus on accuracy first
Begin by setting a clear accuracy goal for your use case, where you're clear on the accuracy that would be "good enough" for this use case to go to production. You can accomplish this through:
\* \*\*Setting a clear accuracy target:\*\* Identify what your target accuracy statistic is going to be.
\* For example, 90% of customer service calls need to be triaged correctly at the first interaction.
\* \*\*Developing an evaluation dataset:\*\* Create a dataset that allows you to measure the model's performance against these goals.
\* To extend the example above, capture 100 interaction examples where we have what the user asked for, what the LLM triaged them to, what the correct triage should be, and whether this was correct or not.
\* \*\*Using the most powerful model to optimize:\*\* Start with the most capable model available to achieve your accuracy targets. Log all responses so we can use them for distillation of a smaller model.
\* Use retrieval-augmented generation to optimize for accuracy
\* Use fine-tuning to optimize for consistency and behavior
During this process, collect prompt and completion pairs for use in evaluations, few-shot learning, or fine-tuning. This practice, known as \*\*prompt baking\*\*, helps you produce high-quality examples for future use.
For more methods and tools here, see our [Accuracy Optimization Guide](https://platform.openai.com/docs/guides/optimizing-llm-accuracy).

#### Setting a realistic accuracy target
Calculate a realistic accuracy target by evaluating the financial impact of model decisions. For example, in a fake news classification scenario:
\* \*\*Correctly classified news:\*\* If the model classifies it correctly, it saves you the cost of a human reviewing it - let's assume \*\*$50\*\*.
\* \*\*Incorrectly classified news:\*\* If it falsely classifies a safe article or misses a fake news article, it may trigger a review process and possible complaint, which might cost us \*\*$300\*\*.
Our news classification example would need \*\*85.8%\*\* accuracy to cover costs, so targeting 90% or more ensures an overall return on investment. Use these calculations to set an effective accuracy target based on your specific cost structures.

### 2\. Optimize cost and latency
Cost and latency are considered secondary because if the model canâ€™t hit your accuracy target then these concerns are moot. However, once youâ€™ve got a model that works for your use case, you can take one of two approaches:
\* \*\*Compare with a smaller model zero- or few-shot:\*\* Swap out the model for a smaller, cheaper one and test whether it maintains accuracy at the lower cost and latency point.
\* \*\*Model distillation:\*\* Fine-tune a smaller model using the data gathered during accuracy optimization.
Cost and latency are typically interconnected; reducing tokens and requests generally leads to faster processing.
The main strategies to consider here are:
\* \*\*Reduce requests:\*\* Limit the number of necessary requests to complete tasks.
\* \*\*Minimize tokens:\*\* Lower the number of input tokens and optimize for shorter model outputs.
\* \*\*Select a smaller model:\*\* Use models that balance reduced costs and latency with maintained accuracy.
To dive deeper into these, please refer to our guide on [latency optimization](https://platform.openai.com/docs/guides/latency-optimization).

#### Exceptions to the rule
Clear exceptions exist for these principles. If your use case is extremely cost or latency sensitive, establish thresholds for these metrics before beginning your testing, then remove the models that exceed those from consideration. Once benchmarks are set, these guidelines will help you refine model accuracy within your constraints.
Practical example
-----------------
To demonstrate these principles, we'll develop a fake news classifier with the following target metrics:
\* \*\*Accuracy:\*\* Achieve 90% correct classification
\* \*\*Cost:\*\* Spend less than $5 per 1,000 articles
\* \*\*Latency:\*\* Maintain processing time under 2 seconds per article

### Experiments
We ran three experiments to reach our goal:
1. \*\*Zero-shot:\*\* Used `GPT-4o` with a basic prompt for 1,000 records, but missed the accuracy target.
2. \*\*Few-shot learning:\*\* Included 5 few-shot examples, meeting the accuracy target but exceeding cost due to more prompt tokens.
3. \*\*Fine-tuned model:\*\* Fine-tuned `GPT-4o-mini` with 1,000 labeled examples, meeting all targets with similar latency and accuracy but significantly lower costs.
|ID|Method|Accuracy|Accuracy target|Cost|Cost target|Avg. latency|Latency target|
|---|---|---|---|---|---|---|---|
|1|gpt-4o zero-shot|84.5%||$1.72||< 1s||
|2|gpt-4o few-shot (n=5)|91.5%|âœ“|$11.92||< 1s|âœ“|
|3|gpt-4o-mini fine-tuned w/ 1000 examples|91.5%|âœ“|$0.21|âœ“|< 1s|âœ“|
Conclusion
----------
By switching from `gpt-4o` to `gpt-4o-mini` with fine-tuning, we achieved \*\*equivalent performance for less than 2%\*\* of the cost, using only 1,000 labeled examples.
This process is important - you often canâ€™t jump right to fine-tuning because you donâ€™t know whether fine-tuning is the right tool for the optimization you need, or you donâ€™t have enough labeled examples. Use `gpt-4o` to achieve your accuracy targets, and curate a good training set - then go for a smaller, more efficient model with fine-tuning.
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:43:15

Latency optimization
====================
Improve latency across a wide variety of LLM-related use cases.
This guide covers the core set of principles you can apply to improve latency across a wide variety of LLM-related use cases. These techniques come from working with a wide range of customers and developers on production applications, so they should apply regardless of what you're building â€“ from a granular workflow to an end-to-end chatbot.
While there's many individual techniques, we'll be grouping them into \*\*seven principles\*\* meant to represent a high-level taxonomy of approaches for improving latency.
At the end, we'll walk through an [example](#example) to see how they can be applied.

### Seven principles
1. [Process tokens faster.](#process-tokens-faster)
2. [Generate fewer tokens.](#generate-fewer-tokens)
3. [Use fewer input tokens.](#use-fewer-input-tokens)
4. [Make fewer requests.](#make-fewer-requests)
5. [Parallelize.](#parallelize)
6. [Make your users wait less.](#make-your-users-wait-less)
7. [Don't default to an LLM.](#don-t-default-to-an-llm)
Process tokens faster
---------------------
\*\*Inference speed\*\* is probably the first thing that comes to mind when addressing latency (but as you'll see soon, it's far from the only one). This refers to the actual \*\*rate at which the LLM processes tokens\*\*, and is often measured in TPM (tokens per minute) or TPS (tokens per second).
The main factor that influences inference speed is \*\*model size\*\* â€“ smaller models usually run faster (and cheaper), and when used correctly can even outperform larger models. To maintain high quality performance with smaller models you can explore:
\* using a longer, [more detailed prompt](/docs/guides/prompt-engineering#tactic-specify-the-steps-required-to-complete-a-task),
\* adding (more) [few-shot examples](/docs/guides/prompt-engineering#tactic-provide-examples), or
\* [fine-tuning](/docs/guides/model-optimization) / distillation.
You can also employ inference optimizations like our [\*\*Predicted outputs\*\*](/docs/guides/predicted-outputs) feature. Predicted outputs let you significantly reduce latency of a generation when you know most of the output ahead of time, such as code editing tasks. By giving the model a prediction, the LLM can focus more on the actual changes, and less on the content that will remain the same.
Deep dive
Compute capacity & additional inference optimizations
Generate fewer tokens
---------------------
Generating tokens is almost always the highest latency step when using an LLM: as a general heuristic, \*\*cutting 50% of your output tokens may cut ~50% your latency\*\*. The way you reduce your output size will depend on output type:
If you're generating \*\*natural language\*\*, simply \*\*asking the model to be more concise\*\* ("under 20 words" or "be very brief") may help. You can also use few shot examples and/or fine-tuning to teach the model shorter responses.
If you're generating \*\*structured output\*\*, try to \*\*minimize your output syntax\*\* where possible: shorten function names, omit named arguments, coalesce parameters, etc.
Finally, while not common, you can also use `max\_tokens` or `stop\_tokens` to end your generation early.
Always remember: an output token cut is a (milli)second earned!
Use fewer input tokens
----------------------
While reducing the number of input tokens does result in lower latency, this is not usually a significant factor â€“ \*\*cutting 50% of your prompt may only result in a 1-5% latency improvement\*\*. Unless you're working with truly massive context sizes (documents, images), you may want to spend your efforts elsewhere.
That being said, if you \_are\_ working with massive contexts (or you're set on squeezing every last bit of performance \_and\_ you've exhausted all other options) you can use the following techniques to reduce your input tokens:
\* \*\*Fine-tuning the model\*\*, to replace the need for lengthy instructions / examples.
\* \*\*Filtering context input\*\*, like pruning RAG results, cleaning HTML, etc.
\* \*\*Maximize shared prompt prefix\*\*, by putting dynamic portions (e.g. RAG results, history, etc) later in the prompt. This makes your request more [KV cache](https://medium.com/@joaolages/kv-caching-explained-276520203249)\-friendly (which most LLM providers use) and means fewer input tokens are processed on each request.
Check out our docs to learn more about how [prompt caching](/docs/guides/prompt-engineering#prompt-caching) works.
Make fewer requests
-------------------
Each time you make a request you incur some round-trip latency â€“ this can start to add up.
If you have sequential steps for the LLM to perform, instead of firing off one request per step consider \*\*putting them in a single prompt and getting them all in a single response\*\*. You'll avoid the additional round-trip latency, and potentially also reduce complexity of processing multiple responses.
An approach to doing this is by collecting your steps in an enumerated list in the combined prompt, and then requesting the model to return the results in named fields in a JSON. This way you can easily parse out and reference each result!
Parallelize
-----------
Parallelization can be very powerful when performing multiple steps with an LLM.
If the steps \*\*are \_not\_ strictly sequential\*\*, you can \*\*split them out into parallel calls\*\*. Two shirts take just as long to dry as one.
If the steps \*\*\_are\_ strictly sequential\*\*, however, you might still be able to \*\*leverage speculative execution\*\*. This is particularly effective for classification steps where one outcome is more likely than the others (e.g. moderation).
1. Start step 1 & step 2 simultaneously (e.g. input moderation & story generation)
2. Verify the result of step 1
3. If result was not the expected, cancel step 2 (and retry if necessary)
If your guess for step 1 is right, then you essentially got to run it with zero added latency!
Make your users wait less
-------------------------
There's a huge difference between \*\*waiting\*\* and \*\*watching progress happen\*\* â€“ make sure your users experience the latter. Here are a few techniques:
\* \*\*Streaming\*\*: The single most effective approach, as it cuts the \_waiting\_ time to a second or less. (ChatGPT would feel pretty different if you saw nothing until each response was done.)
\* \*\*Chunking\*\*: If your output needs further processing before being shown to the user (moderation, translation) consider \*\*processing it in chunks\*\* instead of all at once. Do this by streaming to your backend, then sending processed chunks to your frontend.
\* \*\*Show your steps\*\*: If you're taking multiple steps or using tools, surface this to the user. The more real progress you can show, the better.
\* \*\*Loading states\*\*: Spinners and progress bars go a long way.
Note that while \*\*showing your steps & having loading states\*\* have a mostly psychological effect, \*\*streaming & chunking\*\* genuinely do reduce overall latency once you consider the app + user system: the user will finish reading a response sooner.
Don't default to an LLM
-----------------------
LLMs are extremely powerful and versatile, and are therefore sometimes used in cases where a \*\*faster classical method\*\* would be more appropriate. Identifying such cases may allow you to cut your latency significantly. Consider the following examples:
\* \*\*Hard-coding:\*\* If your \*\*output\*\* is highly constrained, you may not need an LLM to generate it. Action confirmations, refusal messages, and requests for standard input are all great candidates to be hard-coded. (You can even use the age-old method of coming up with a few variations for each.)
\* \*\*Pre-computing:\*\* If your \*\*input\*\* is constrained (e.g. category selection) you can generate multiple responses in advance, and just make sure you never show the same one to a user twice.
\* \*\*Leveraging UI:\*\* Summarized metrics, reports, or search results are sometimes better conveyed with classical, bespoke UI components rather than LLM-generated text.
\* \*\*Traditional optimization techniques:\*\* An LLM application is still an application; binary search, caching, hash maps, and runtime complexity are all \_still\_ useful in a world of LLMs.
Example
-------
Let's now look at a sample application, identify potential latency optimizations, and propose some solutions!
We'll be analyzing the architecture and prompts of a hypothetical customer service bot inspired by real production applications. The [architecture and prompts](#architecture-and-prompts) section sets the stage, and the [analysis and optimizations](#analysis-and-optimizations) section will walk through the latency optimization process.
You'll notice this example doesn't cover every single principle, much like real-world use cases don't require applying every technique.

### Architecture and prompts
The following is the \*\*initial architecture\*\* for a hypothetical \*\*customer service bot\*\*. This is what we'll be making changes to.
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-0.png)
At a high level, the diagram flow describes the following process:
1. A user sends a message as part of an ongoing conversation.
2. The last message is turned into a \*\*self-contained query\*\* (see examples in prompt).
3. We determine whether or not \*\*additional (retrieved) information is required\*\* to respond to that query.
4. \*\*Retrieval\*\* is performed, producing search results.
5. The assistant \*\*reasons\*\* about the user's query and search results, and \*\*produces a response\*\*.
6. The response is sent back to the user.
Below are the prompts used in each part of the diagram. While they are still only hypothetical and simplified, they are written with the same structure and wording that you would find in a production application.
Places where you see placeholders like "\*\*\[user input here\]\*\*" represent dynamic portions, that would be replaced by actual data at runtime.
Query contextualization prompt
Re-writes user query to be a self-contained search query.
SYSTEM
Given the previous conversation, re-write the last user query so it contains all necessary context. # Example History: \[{user: "What is your return policy?"},{assistant: "..."}\] User Query: "How long does it cover?" Response: "How long does the return policy cover?" # Conversation \[last 3 messages of conversation\] # User Query \[last user query\]
USER
\[JSON-formatted input conversation here\]
Retrieval check prompt
Determines whether a query requires performing retrieval to respond.
SYSTEM
Given a user query, determine whether it requires doing a realtime lookup to respond to. # Examples User Query: "How can I return this item after 30 days?" Response: "true" User Query: "Thank you!" Response: "false"
USER
\[input user query here\]
Assistant prompt
Fills the fields of a JSON to reason through a pre-defined set of steps to produce a final response given a user conversation and relevant retrieved information.
SYSTEM
You are a helpful customer service bot. Use the result JSON to reason about each user query - use the retrieved context. # Example User: "My computer screen is cracked! I want it fixed now!!!" Assistant Response: { "message\\_is\\_conversation\\_continuation": "True", "number\\_of\\_messages\\_in\\_conversation\\_so\\_far": "1", "user\\_sentiment": "Aggravated", "query\\_type": "Hardware Issue", "response\\_tone": "Validating and solution-oriented", "response\\_requirements": "Propose options for repair or replacement.", "user\\_requesting\\_to\\_talk\\_to\\_human": "False", "enough\\_information\\_in\\_context": "True", "response": "..." }
USER
\# Relevant Information \` \` \` \[retrieved context\] \` \` \`
USER
\[input user query here\]

### Analysis and optimizations

#### Part 1: Looking at retrieval prompts
Looking at the architecture, the first thing that stands out is the \*\*consecutive GPT-4 calls\*\* - these hint at a potential inefficiency, and can often be replaced by a single call or parallel calls.
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-2.png)
In this case, since the check for retrieval requires the contextualized query, let's \*\*combine them into a single prompt\*\* to [make fewer requests](#make-fewer-requests).
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-3.png)
Combined query contextualization and retrieval check prompt
\*\*What changed?\*\* Before, we had one prompt to re-write the query and one to determine whether this requires doing a retrieval lookup. Now, this combined prompt does both. Specifically, notice the updated instruction in the first line of the prompt, and the updated output JSON:
```jsx
{
query:"[contextualized query]",
retrieval:"[true/false - whether retrieval is required]"
}
```
SYSTEM
Given the previous conversation, re-write the last user query so it contains all necessary context. Then, determine whether the full request requires doing a realtime lookup to respond to. Respond in the following form: { query:"\[contextualized query\]", retrieval:"\[true/false - whether retrieval is required\]" } # Examples History: \[{user: "What is your return policy?"},{assistant: "..."}\] User Query: "How long does it cover?" Response: {query: "How long does the return policy cover?", retrieval: "true"} History: \[{user: "How can I return this item after 30 days?"},{assistant: "..."}\] User Query: "Thank you!" Response: {query: "Thank you!", retrieval: "false"} # Conversation \[last 3 messages of conversation\] # User Query \[last user query\]
USER
\[JSON-formatted input conversation here\]
Actually, adding context and determining whether to retrieve are very straightforward and well defined tasks, so we can likely use a \*\*smaller, fine-tuned model\*\* instead. Switching to GPT-3.5 will let us [process tokens faster](#process-tokens-faster).
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-4.png)

#### Part 2: Analyzing the assistant prompt
Let's now direct our attention to the Assistant prompt. There seem to be many distinct steps happening as it fills the JSON fields â€“ this could indicate an opportunity to [parallelize](#parallelize).
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-5.png)
However, let's pretend we have run some tests and discovered that splitting the reasoning steps in the JSON produces worse responses, so we need to explore different solutions.
\*\*Could we use a fine-tuned GPT-3.5 instead of GPT-4?\*\* Maybe â€“ but in general, open-ended responses from assistants are best left to GPT-4 so it can better handle a greater range of cases. That being said, looking at the reasoning steps themselves, they may not all require GPT-4 level reasoning to produce. The well defined, limited scope nature makes them and \*\*good potential candidates for fine-tuning\*\*.
```jsx
{
"message\_is\_conversation\_continuation": "True", // <-
"number\_of\_messages\_in\_conversation\_so\_far": "1", // <-
"user\_sentiment": "Aggravated", // <-
"query\_type": "Hardware Issue", // <-
"response\_tone": "Validating and solution-oriented", // <-
"response\_requirements": "Propose options for repair or replacement.", // <-
"user\_requesting\_to\_talk\_to\_human": "False", // <-
"enough\_information\_in\_context": "True", // <-
"response": "..." // X -- benefits from GPT-4
}
```
This opens up the possibility of a trade-off. Do we keep this as a \*\*single request entirely generated by GPT-4\*\*, or \*\*split it into two sequential requests\*\* and use GPT-3.5 for all but the final response? We have a case of conflicting principles: the first option lets us [make fewer requests](#make-fewer-requests), but the second may let us [process tokens faster](#1-process-tokens-faster).
As with many optimization tradeoffs, the answer will depend on the details. For example:
\* The proportion of tokens in the `response` vs the other fields.
\* The average latency decrease from processing most fields faster.
\* The average latency \_increase\_ from doing two requests instead of one.
The conclusion will vary by case, and the best way to make the determiation is by testing this with production examples. In this case let's pretend the tests indicated it's favorable to split the prompt in two to [process tokens faster](#process-tokens-faster).
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-6.png)
\*\*Note:\*\* We'll be grouping `response` and `enough\_information\_in\_context` together in the second prompt to avoid passing the retrieved context to both new prompts.
Assistants prompt - reasoning
This prompt will be passed to GPT-3.5 and can be fine-tuned on curated examples.
\*\*What changed?\*\* The "enough\\_information\\_in\\_context" and "response" fields were removed, and the retrieval results are no longer loaded into this prompt.
SYSTEM
You are a helpful customer service bot. Based on the previous conversation, respond in a JSON to determine the required fields. # Example User: "My freaking computer screen is cracked!" Assistant Response: { "message\\_is\\_conversation\\_continuation": "True", "number\\_of\\_messages\\_in\\_conversation\\_so\\_far": "1", "user\\_sentiment": "Aggravated", "query\\_type": "Hardware Issue", "response\\_tone": "Validating and solution-oriented", "response\\_requirements": "Propose options for repair or replacement.", "user\\_requesting\\_to\\_talk\\_to\\_human": "False", }
Assistants prompt - response
This prompt will be processed by GPT-4 and will receive the reasoning steps determined in the prior prompt, as well as the results from retrieval.
\*\*What changed?\*\* All steps were removed except for "enough\\_information\\_in\\_context" and "response". Additionally, the JSON we were previously filling in as output will be passed in to this prompt.
SYSTEM
You are a helpful customer service bot. Use the retrieved context, as well as these pre-classified fields, to respond to the user's query. # Reasoning Fields \` \` \` \[reasoning json determined in previous GPT-3.5 call\] \` \` \` # Example User: "My freaking computer screen is cracked!" Assistant Response: { "enough\\_information\\_in\\_context": "True", "response": "..." }
USER
\# Relevant Information \` \` \` \[retrieved context\] \` \` \`
In fact, now that the reasoning prompt does not depend on the retrieved context we can [parallelize](#parallelize) and fire it off at the same time as the retrieval prompts.
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-6b.png)

#### Part 3: Optimizing the structured output
Let's take another look at the reasoning prompt.
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-7b.png)
Taking a closer look at the reasoning JSON you may notice the field names themselves are quite long.
```jsx
{
"message\_is\_conversation\_continuation": "True", // <-
"number\_of\_messages\_in\_conversation\_so\_far": "1", // <-
"user\_sentiment": "Aggravated", // <-
"query\_type": "Hardware Issue", // <-
"response\_tone": "Validating and solution-oriented", // <-
"response\_requirements": "Propose options for repair or replacement.", // <-
"user\_requesting\_to\_talk\_to\_human": "False", // <-
}
```
By making them shorter and moving explanations to the comments we can [generate fewer tokens](#generate-fewer-tokens).
```jsx
{
"cont": "True", // whether last message is a continuation
"n\_msg": "1", // number of messages in the continued conversation
"tone\_in": "Aggravated", // sentiment of user query
"type": "Hardware Issue", // type of the user query
"tone\_out": "Validating and solution-oriented", // desired tone for response
"reqs": "Propose options for repair or replacement.", // response requirements
"human": "False", // whether user is expressing want to talk to human
}
```
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-8b.png)
This small change removed 19 output tokens. While with GPT-3.5 this may only result in a few millisecond improvement, with GPT-4 this could shave off up to a second.
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/token-counts-latency-customer-service-large.png)
You might imagine, however, how this can have quite a significant impact for larger model outputs.
We could go further and use single characters for the JSON fields, or put everything in an array, but this may start to hurt our response quality. The best way to know, once again, is through testing.

#### Example wrap-up
Let's review the optimizations we implemented for the customer service bot example:
![Assistants object architecture diagram](https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-11b.png)
1. \*\*Combined\*\* query contextualization and retrieval check steps to [make fewer requests](#make-fewer-requests).
2. For the new prompt, \*\*switched to a smaller, fine-tuned GPT-3.5\*\* to [process tokens faster](process-tokens-faster).
3. Split the assistant prompt in two, \*\*switching to a smaller, fine-tuned GPT-3.5\*\* for the reasoning, again to [process tokens faster](#process-tokens-faster).
4. [Parallelized](#parallelize) the retrieval checks and the reasoning steps.
5. \*\*Shortened reasoning field names\*\* and moved comments into the prompt, to [generate fewer tokens](#generate-fewer-tokens).
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:43:19

Optimizing LLM Accuracy
=======================
Maximize correctness and consistent behavior when working with LLMs.

### How to maximize correctness and consistent behavior when working with LLMs
Optimizing LLMs is hard.
We've worked with many developers across both start-ups and enterprises, and the reason optimization is hard consistently boils down to these reasons:
\* Knowing \*\*how to start\*\* optimizing accuracy
\* \*\*When to use what\*\* optimization method
\* What level of accuracy is \*\*good enough\*\* for production
This paper gives a mental model for how to optimize LLMs for accuracy and behavior. Weâ€™ll explore methods like prompt engineering, retrieval-augmented generation (RAG) and fine-tuning. Weâ€™ll also highlight how and when to use each technique, and share a few pitfalls.
As you read through, it's important to mentally relate these principles to what accuracy means for your specific use case. This may seem obvious, but there is a difference between producing a bad copy that a human needs to fix vs. refunding a customer $1000 rather than $100. You should enter any discussion on LLM accuracy with a rough picture of how much a failure by the LLM costs you, and how much a success saves or earns you - this will be revisited at the end, where we cover how much accuracy is â€œgood enoughâ€ for production.
LLM optimization context
------------------------
Many â€œhow-toâ€ guides on optimization paint it as a simple linear flow - you start with prompt engineering, then you move on to retrieval-augmented generation, then fine-tuning. However, this is often not the case - these are all levers that solve different things, and to optimize in the right direction you need to pull the right lever.
It is useful to frame LLM optimization as more of a matrix:
![Accuracy mental model diagram](https://cdn.openai.com/API/docs/images/diagram-optimizing-accuracy-01.png)
The typical LLM task will start in the bottom left corner with prompt engineering, where we test, learn, and evaluate to get a baseline. Once weâ€™ve reviewed those baseline examples and assessed why they are incorrect, we can pull one of our levers:
\* \*\*Context optimization:\*\* You need to optimize for context when 1) the model lacks contextual knowledge because it wasnâ€™t in its training set, 2) its knowledge is out of date, or 3) it requires knowledge of proprietary information. This axis maximizes \*\*response accuracy\*\*.
\* \*\*LLM optimization:\*\* You need to optimize the LLM when 1) the model is producing inconsistent results with incorrect formatting, 2) the tone or style of speech is not correct, or 3) the reasoning is not being followed consistently. This axis maximizes \*\*consistency of behavior\*\*.
In reality this turns into a series of optimization steps, where we evaluate, make a hypothesis on how to optimize, apply it, evaluate, and re-assess for the next step. Hereâ€™s an example of a fairly typical optimization flow:
![Accuracy mental model journey diagram](https://cdn.openai.com/API/docs/images/diagram-optimizing-accuracy-02.png)
In this example, we do the following:
\* Begin with a prompt, then evaluate its performance
\* Add static few-shot examples, which should improve consistency of results
\* Add a retrieval step so the few-shot examples are brought in dynamically based on the question - this boosts performance by ensuring relevant context for each input
\* Prepare a dataset of 50+ examples and fine-tune a model to increase consistency
\* Tune the retrieval and add a fact-checking step to find hallucinations to achieve higher accuracy
\* Re-train the fine-tuned model on the new training examples which include our enhanced RAG inputs
This is a fairly typical optimization pipeline for a tough business problem - it helps us decide whether we need more relevant context or if we need more consistent behavior from the model. Once we make that decision, we know which lever to pull as our first step toward optimization.
Now that we have a mental model, letâ€™s dive into the methods for taking action on all of these areas. Weâ€™ll start in the bottom-left corner with Prompt Engineering.

### Prompt engineering
Prompt engineering is typically the best place to start\\*\\*. It is often the only method needed for use cases like summarization, translation, and code generation where a zero-shot approach can reach production levels of accuracy and consistency.
This is because it forces you to define what accuracy means for your use case - you start at the most basic level by providing an input, so you need to be able to judge whether or not the output matches your expectations. If it is not what you want, then the reasons \*\*why\*\* will show you what to use to drive further optimizations.
To achieve this, you should always start with a simple prompt and an expected output in mind, and then optimize the prompt by adding \*\*context\*\*, \*\*instructions\*\*, or \*\*examples\*\* until it gives you what you want.

#### Optimization
To optimize your prompts, Iâ€™ll mostly lean on strategies from the [Prompt Engineering guide](https://platform.openai.com/docs/guides/prompt-engineering) in the OpenAI API documentation. Each strategy helps you tune Context, the LLM, or both:
|Strategy|Context optimization|LLM optimization|
|---|---|---|
|Write clear instructions||X|
|Split complex tasks into simpler subtasks|X|X|
|Give GPTs time to "think"||X|
|Test changes systematically|X|X|
|Provide reference text|X||
|Use external tools|X||
These can be a little difficult to visualize, so weâ€™ll run through an example where we test these out with a practical example. Letâ€™s use gpt-4-turbo to correct Icelandic sentences to see how this can work.
Prompt engineering for language corrections
The [Icelandic Errors Corpus](https://repository.clarin.is/repository/xmlui/handle/20.500.12537/105) contains combinations of an Icelandic sentence with errors, and the corrected version of that sentence. Weâ€™ll use the baseline GPT-4 model to try to solve this task, and then apply different optimization techniques to see how we can improve the modelâ€™s performance.
Given an Icelandic sentence, we want the model to return a corrected version of the sentence. Weâ€™ll use Bleu score to measure the relative quality of the translation.
|system|user|ground\_truth|assistant|BLEU|
|---|---|---|---|---|
|The following sentences contain Icelandic sentences which may include errors. Please correct these errors using as few word changes as possible.|SÃ¶rvistÃ¶lur eru nÃ¦r hÃ¡lsi og skartgripir kvenna Ã¡ brjÃ³tsti.|SÃ¶rvistÃ¶lur eru nÃ¦r hÃ¡lsi og skartgripir kvenna Ã¡ brjÃ³sti.|SÃ¶rvistÃ¶lur eru nÃ¦r hÃ¡lsi og skartgripir kvenna Ã¡ brjÃ³sti.|1.0|
We perform a first attempt with GPT-4 with no examples, and it performs decently, getting a BLEU score of 62. Weâ€™ll now add some few-shot examples and see whether we can teach the model the style weâ€™re looking for by showing rather than telling. An example looks like this:
SYSTEM
The following sentences contain Icelandic sentences which may include errors. Please correct these errors using as few word changes as possible. # Examples
USER
"Stofnendurnir sÃ©u margir og eru fulltrÃºar hennar frÃ¡ Englandi, Grikklandi, RÃºsslandi, SvÃ­Ã¾jÃ³Ã° og fleiri lÃ¶ndum EvrÃ³pu."
ASSISTANT
"Hann segir aÃ° stofnendur leynireglunnar sÃ©u margir og aÃ° fulltrÃºar hennar sÃ©u frÃ¡ Englandi, Grikklandi, RÃºsslandi, SvÃ­Ã¾jÃ³Ã° og fleiri lÃ¶ndum EvrÃ³pu."
USER
"Helsta fÃ¦Ã°a bjÃºgorma eru hÃ¦gfara lÃ­fverur sem eru Ã¡ sama bÃºsvÃ¦Ã°i og bjÃºgormarnir, oft smÃ¦rri ormar eins og burstormar (frÃ¦Ã°iheiti: Polychatete)."
ASSISTANT
"Helsta fÃ¦Ã°a bjÃºgorma eru hÃ¦gfara lÃ­fverur sem eru Ã¡ sama bÃºsvÃ¦Ã°i og bjÃºgormarnir, oft smÃ¦rri ormar eins og burstaormar (frÃ¦Ã°iheiti: Polychatete)."
USER
"SÃ¶rvistÃ¶lur eru nÃ¦r hÃ¡lsi og skartgripir kvenna Ã¡ brjÃ³tsti."
ASSISTANT
"SÃ¶rvistÃ¶lur eru nÃ¦r hÃ¡lsi og skartgripir kvenna Ã¡ brjÃ³sti."
USER
\[input user query here\]
The overall translation quality is better, showing an improvement to a Bleu score of \*\*70 (+8%)\*\*. This is pretty good, and shows us that giving the model examples of the task is helping it to learn.
This tells us that it is the \*\*behavior\*\* of the model that we need to optimize - it already has the knowledge that it needs to solve the problem, so providing many more examples may be the optimization we need.
Weâ€™ll revisit this later in the paper to test how our more advanced optimization methods play with this use case.
Weâ€™ve seen that prompt engineering is a great place to start, and that with the right tuning methods we can push the performance pretty far.
However, the biggest issue with prompt engineering is that it often doesnâ€™t scale - we either need dynamic context to be fed to allow the model to deal with a wider range of problems than we can deal with through adding content to the context, or we need more consistent behavior than we can achieve with few-shot examples.
Deep dive
Using long context to scale prompt engineering
So how far can you really take prompt engineering? The answer is that it depends, and the way you make your decision is through evaluations.

### Evaluation
This is why \*\*a good prompt with an evaluation set of questions and ground truth answers\*\* is the best output from this stage. If we have a set of 20+ questions and answers, and we have looked into the details of the failures and have a hypothesis of why theyâ€™re occurring, then weâ€™ve got the right baseline to take on more advanced optimization methods.
Before you move on to more sophisticated optimization methods, it's also worth considering how to automate this evaluation to speed up your iterations. Some common practices weâ€™ve seen be effective here are:
\* Using approaches like [ROUGE](https://aclanthology.org/W04-1013/) or [BERTScore](https://arxiv.org/abs/1904.09675) to provide a finger-in-the-air judgment. This doesnâ€™t correlate that closely with human reviewers, but can give a quick and effective measure of how much an iteration changed your model outputs.
\* Using [GPT-4](https://arxiv.org/pdf/2303.16634.pdf) as an evaluator as outlined in the G-Eval paper, where you provide the LLM a scorecard to assess the output as objectively as possible.
If you want to dive deeper on these, check out [this cookbook](https://cookbook.openai.com/examples/evaluation/how\_to\_eval\_abstractive\_summarization) which takes you through all of them in practice.
Understanding the tools
-----------------------
So youâ€™ve done prompt engineering, youâ€™ve got an eval set, and your model is still not doing what you need it to do. The most important next step is to diagnose where it is failing, and what tool works best to improve it.
Here is a basic framework for doing so:
![Classifying memory problem diagram](https://cdn.openai.com/API/docs/images/diagram-optimizing-accuracy-03.png)
You can think of framing each failed evaluation question as an \*\*in-context\*\* or \*\*learned\*\* memory problem. As an analogy, imagine writing an exam. There are two ways you can ensure you get the right answer:
\* You attend class for the last 6 months, where you see many repeated examples of how a particular concept works. This is \*\*learned\*\* memory - you solve this with LLMs by showing examples of the prompt and the response you expect, and the model learning from those.
\* You have the textbook with you, and can look up the right information to answer the question with. This is \*\*in-context\*\* memory - we solve this in LLMs by stuffing relevant information into the context window, either in a static way using prompt engineering, or in an industrial way using RAG.
These two optimization methods are \*\*additive, not exclusive\*\* - they stack, and some use cases will require you to use them together to use optimal performance.
Letâ€™s assume that weâ€™re facing a short-term memory problem - for this weâ€™ll use RAG to solve it.

### Retrieval-augmented generation (RAG)
RAG is the process of \*\*R\*\*etrieving content to \*\*A\*\*ugment your LLMâ€™s prompt before \*\*G\*\*enerating an answer. It is used to give the model \*\*access to domain-specific context\*\* to solve a task.
RAG is an incredibly valuable tool for increasing the accuracy and consistency of an LLM - many of our largest customer deployments at OpenAI were done using only prompt engineering and RAG.
![RAG diagram](https://cdn.openai.com/API/docs/images/diagram-optimizing-accuracy-04.png)
In this example we have embedded a knowledge base of statistics. When our user asks a question, we embed that question and retrieve the most relevant content from our knowledge base. This is presented to the model, which answers the question.
RAG applications introduce a new axis we need to optimize against, which is retrieval. For our RAG to work, we need to give the right context to the model, and then assess whether the model is answering correctly. Iâ€™ll frame these in a grid here to show a simple way to think about evaluation with RAG:
![RAG evaluation diagram](https://cdn.openai.com/API/docs/images/diagram-optimizing-accuracy-05.png)
You have two areas your RAG application can break down:
|Area|Problem|Resolution|
|---|---|---|
|Retrieval|You can supply the wrong context, so the model canâ€™t possibly answer, or you can supply too much irrelevant context, which drowns out the real information and causes hallucinations.|Optimizing your retrieval, which can include:- Tuning the search to return the right results.- Tuning the search to include less noise.- Providing more information in each retrieved resultThese are just examples, as tuning RAG performance is an industry into itself, with libraries like LlamaIndex and LangChain giving many approaches to tuning here.|
|LLM|The model can also get the right context and do the wrong thing with it.|Prompt engineering by improving the instructions and method the model uses, and, if showing it examples increases accuracy, adding in fine-tuning|
The key thing to take away here is that the principle remains the same from our mental model at the beginning - you evaluate to find out what has gone wrong, and take an optimization step to fix it. The only difference with RAG is you now have the retrieval axis to consider.
While useful, RAG only solves our in-context learning issues - for many use cases, the issue will be ensuring the LLM can learn a task so it can perform it consistently and reliably. For this problem we turn to fine-tuning.

### Fine-tuning
To solve a learned memory problem, many developers will continue the training process of the LLM on a smaller, domain-specific dataset to optimize it for the specific task. This process is known as \*\*fine-tuning\*\*.
Fine-tuning is typically performed for one of two reasons:
\* \*\*To improve model accuracy on a specific task:\*\* Training the model on task-specific data to solve a learned memory problem by showing it many examples of that task being performed correctly.
\* \*\*To improve model efficiency:\*\* Achieve the same accuracy for less tokens or by using a smaller model.
The fine-tuning process begins by preparing a dataset of training examples - this is the most critical step, as your fine-tuning examples must exactly represent what the model will see in the real world.
Many customers use a process known as \*\*prompt baking\*\*, where you extensively log your prompt inputs and outputs during a pilot. These logs can be pruned into an effective training set with realistic examples.
![Fine-tuning process diagram](https://cdn.openai.com/API/docs/images/diagram-optimizing-accuracy-06.png)
Once you have this clean set, you can train a fine-tuned model by performing a \*\*training\*\* run - depending on the platform or framework youâ€™re using for training you may have hyperparameters you can tune here, similar to any other machine learning model. We always recommend maintaining a hold-out set to use for \*\*evaluation\*\* following training to detect overfitting. For tips on how to construct a good training set you can check out the [guidance](/docs/guides/fine-tuning#analyzing-your-fine-tuned-model) in our Fine-tuning documentation. Once training is completed, the new, fine-tuned model is available for inference.
For optimizing fine-tuning weâ€™ll focus on best practices we observe with OpenAIâ€™s model customization offerings, but these principles should hold true with other providers and OSS offerings. The key practices to observe here are:
\* \*\*Start with prompt-engineering:\*\* Have a solid evaluation set from prompt engineering which you can use as a baseline. This allows a low-investment approach until youâ€™re confident in your base prompt.
\* \*\*Start small, focus on quality:\*\* Quality of training data is more important than quantity when fine-tuning on top of a foundation model. Start with 50+ examples, evaluate, and then dial your training set size up if you havenâ€™t yet hit your accuracy needs, and if the issues causing incorrect answers are due to consistency/behavior and not context.
\* \*\*Ensure your examples are representative:\*\* One of the most common pitfalls we see is non-representative training data, where the examples used for fine-tuning differ subtly in formatting or form from what the LLM sees in production. For example, if you have a RAG application, fine-tune the model with RAG examples in it so it isnâ€™t learning how to use the context zero-shot.

### All of the above
These techniques stack on top of each other - if your early evals show issues with both context and behavior, then it's likely you may end up with fine-tuning + RAG in your production solution. This is ok - these stack to balance the weaknesses of both approaches. Some of the main benefits are:
\* Using fine-tuning to \*\*minimize the tokens\*\* used for prompt engineering, as you replace instructions and few-shot examples with many training examples to ingrain consistent behaviour in the model.
\* \*\*Teaching complex behavior\*\* using extensive fine-tuning
\* Using RAG to \*\*inject context\*\*, more recent content or any other specialized context required for your use cases
Using these tools to improve language translation
Weâ€™ll continue building on the Icelandic correction example we used above. Weâ€™ll test out the following approaches:
\* Our original hypothesis was that this was a behavior optimization problem, so our first step will be to fine-tune a model. Weâ€™ll try both gpt-3.5-turbo and gpt-4 here.
\* Weâ€™ll also try RAG - in this instance our hypothesis is that relevant examples might give additional context which could help the model solve the problem, but this is a lower confidence optimization.

#### Fine-tuning
To fine-tune for our use-case weâ€™ll use a dataset of 1000 examples similar to our few-shot examples above:
SYSTEM
The following sentences contain Icelandic sentences which may include errors. Please correct these errors using as few word changes as possible.
USER
"HiÃ° sameinaÃ°a fyrirtÃ¦ki verÃ°ur einn af stÃ¦rstu bÃ­laframleiÃ°endum Ã­ heiminum."
ASSISTANT
"HiÃ° sameinaÃ°a fyrirtÃ¦ki verÃ°ur einn af stÃ¦rstu bÃ­laframleiÃ°endum heims."
We use these 1000 examples to train both gpt-3.5-turbo and gpt-4 fine-tuned models, and rerun our evaluation on our validation set. This confirmed our hypothesis - we got a meaningful bump in performance with both, with even the 3.5 model outperforming few-shot gpt-4 by 8 points:
|Run|Method|Bleu Score|
|---|---|---|
|1|gpt-4 with zero-shot|62|
|2|gpt-4 with 3 few-shot examples|70|
|3|gpt-3.5-turbo fine-tuned with 1000 examples|78|
|4|gpt-4 fine-tuned with 1000 examples|87|
Great, this is starting to look like production level accuracy for our use case. However, let's test whether we can squeeze a little more performance out of our pipeline by adding some relevant RAG examples to the prompt for in-context learning.

#### RAG + Fine-tuning
Our final optimization adds 1000 examples from outside of the training and validation sets which are embedded and placed in a vector database. We then run a further test with our gpt-4 fine-tuned model, with some perhaps surprising results:
![Icelandic case study diagram](https://cdn.openai.com/API/docs/images/diagram-optimizing-accuracy-07.png) \_Bleu Score per tuning method (out of 100)\_
RAG actually \*\*decreased\*\* accuracy, dropping four points from our GPT-4 fine-tuned model to 83.
This illustrates the point that you use the right optimization tool for the right job - each offers benefits and risks that we manage with evaluations and iterative changes. The behavior we witnessed in our evals and from what we know about this question told us that this is a behavior optimization problem where additional context will not necessarily help the model. This was borne out in practice - RAG actually confounded the model by giving it extra noise when it had already learned the task effectively through fine-tuning.
We now have a model that should be close to production-ready, and if we want to optimize further we can consider a wider diversity and quantity of training examples.
Now you should have an appreciation for RAG and fine-tuning, and when each is appropriate. The last thing you should appreciate with these tools is that once you introduce them there is a trade-off here in our speed to iterate:
\* For RAG you need to tune the retrieval as well as LLM behavior
\* With fine-tuning you need to rerun the fine-tuning process and manage your training and validation sets when you do additional tuning.
Both of these can be time-consuming and complex processes, which can introduce regression issues as your LLM application becomes more complex. If you take away one thing from this paper, let it be to squeeze as much accuracy out of basic methods as you can before reaching for more complex RAG or fine-tuning - let your accuracy target be the objective, not jumping for RAG + FT because they are perceived as the most sophisticated.
How much accuracy is â€œgood enoughâ€ for production
-------------------------------------------------
Tuning for accuracy can be a never-ending battle with LLMs - they are unlikely to get to 99.999% accuracy using off-the-shelf methods. This section is all about deciding when is enough for accuracy - how do you get comfortable putting an LLM in production, and how do you manage the risk of the solution you put out there.
I find it helpful to think of this in both a \*\*business\*\* and \*\*technical\*\* context. Iâ€™m going to describe the high level approaches to managing both, and use a customer service help-desk use case to illustrate how we manage our risk in both cases.

### Business
For the business it can be hard to trust LLMs after the comparative certainties of rules-based or traditional machine learning systems, or indeed humans! A system where failures are open-ended and unpredictable is a difficult circle to square.
An approach Iâ€™ve seen be successful here was for a customer service use case - for this, we did the following:
First we identify the primary success and failure cases, and assign an estimated cost to them. This gives us a clear articulation of what the solution is likely to save or cost based on pilot performance.
\* For example, a case getting solved by an AI where it was previously solved by a human may save \*\*$20\*\*.
\* Someone getting escalated to a human when they shouldnâ€™t might cost \*\*$40\*\*
\* In the worst case scenario, a customer gets so frustrated with the AI they churn, costing us \*\*$1000\*\*. We assume this happens in 5% of cases.
|Event|Value|Number of cases|Total value|
|---|---|---|---|
|AI success|+20|815|$16,300|
|AI failure (escalation)|-40|175.75|$7,030|
|AI failure (churn)|-1000|9.25|$9,250|
|Result|||+20|
|Break-even accuracy|||81.5%|
The other thing we did is to measure the empirical stats around the process which will help us measure the macro impact of the solution. Again using customer service, these could be:
\* The CSAT score for purely human interactions vs. AI ones
\* The decision accuracy for retrospectively reviewed cases for human vs. AI
\* The time to resolution for human vs. AI
In the customer service example, this helped us make two key decisions following a few pilots to get clear data:
1. Even if our LLM solution escalated to humans more than we wanted, it still made an enormous operational cost saving over the existing solution. This meant that an accuracy of even 85% could be ok, if those 15% were primarily early escalations.
2. Where the cost of failure was very high, such as a fraud case being incorrectly resolved, we decided the human would drive and the AI would function as an assistant. In this case, the decision accuracy stat helped us make the call that we werenâ€™t comfortable with full autonomy.

### Technical
On the technical side it is more clear - now that the business is clear on the value they expect and the cost of what can go wrong, your role is to build a solution that handles failures gracefully in a way that doesnâ€™t disrupt the user experience.
Letâ€™s use the customer service example one more time to illustrate this, and weâ€™ll assume weâ€™ve got a model that is 85% accurate in determining intent. As a technical team, here are a few ways we can minimize the impact of the incorrect 15%:
\* We can prompt engineer the model to prompt the customer for more information if it isnâ€™t confident, so our first-time accuracy may drop but we may be more accurate given 2 shots to determine intent.
\* We can give the second-line assistant the option to pass back to the intent determination stage, again giving the UX a way of self-healing at the cost of some additional user latency.
\* We can prompt engineer the model to hand off to a human if the intent is unclear, which costs us some operational savings in the short-term but may offset customer churn risk in the long term.
Those decisions then feed into our UX, which gets slower at the cost of higher accuracy, or more human interventions, which feed into the cost model covered in the business section above.
You now have an approach to breaking down the business and technical decisions involved in setting an accuracy target that is grounded in business reality.
Taking this forward
-------------------
This is a high level mental model for thinking about maximizing accuracy for LLMs, the tools you can use to achieve it, and the approach for deciding where enough is enough for production. You have the framework and tools you need to get to production consistently, and if you want to be inspired by what others have achieved with these methods then look no further than our customer stories, where use cases like [Morgan Stanley](https://openai.com/customer-stories/morgan-stanley) and [Klarna](https://openai.com/customer-stories/klarna) show what you can achieve by leveraging these techniques.
Best of luck, and weâ€™re excited to see what you build with this!
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:43:24

Advanced usage
==============
Use advanced techniques for reproducibility and parameter tuning.
OpenAI's text generation models (often called generative pre-trained transformers or large language models) have been trained to understand natural language, code, and images. The models provide text outputs in response to their inputs. The text inputs to these models are also referred to as "prompts". Designing a prompt is essentially how you â€œprogramâ€ a large language model model, usually by providing instructions or some examples of how to successfully complete a task.
Reproducible outputs
--------------------
Chat Completions are non-deterministic by default (which means model outputs may differ from request to request). That being said, we offer some control towards deterministic outputs by giving you access to the [seed](/docs/api-reference/chat/create#chat-create-seed) parameter and the [system\\_fingerprint](/docs/api-reference/completions/object#completions/object-system\_fingerprint) response field.
To receive (mostly) deterministic outputs across API calls, you can:
\* Set the [seed](/docs/api-reference/chat/create#chat-create-seed) parameter to any integer of your choice and use the same value across requests you'd like deterministic outputs for.
\* Ensure all other parameters (like `prompt` or `temperature`) are the exact same across requests.
Sometimes, determinism may be impacted due to necessary changes OpenAI makes to model configurations on our end. To help you keep track of these changes, we expose the [system\\_fingerprint](/docs/api-reference/chat/object#chat/object-system\_fingerprint) field. If this value is different, you may see different outputs due to changes we've made on our systems.
[
Deterministic outputs
Explore the new seed parameter in the OpenAI cookbook
](https://cookbook.openai.com/examples/reproducible\_outputs\_with\_the\_seed\_parameter)
Managing tokens
---------------
Language models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., `a` or `apple`), and in some languages tokens can be even shorter than one character or even longer than one word.
As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.
Check out our [Tokenizer tool](https://platform.openai.com/tokenizer) to test specific strings and see how they are translated into tokens.
For example, the string `"ChatGPT is great!"` is encoded into six tokens: `["Chat", "G", "PT", " is", " great", "!"]`.
The total number of tokens in an API call affects:
\* How much your API call costs, as you pay per token
\* How long your API call takes, as writing more tokens takes more time
\* Whether your API call works at all, as total tokens must be below the model's maximum limit (4097 tokens for `gpt-3.5-turbo`)
Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the [pricing](https://openai.com/api/pricing) page for more information).
To see how many tokens are used by an API call, check the `usage` field in the API response (e.g., `response['usage']['total\_tokens']`).
Chat models like `gpt-3.5-turbo` and `gpt-4-turbo-preview` use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.
Deep dive
Counting tokens for chat API calls
To see how many tokens are in a text string without making an API call, use OpenAIâ€™s [tiktoken](https://github.com/openai/tiktoken) Python library. Example code can be found in the OpenAI Cookbookâ€™s guide on [how to count tokens with tiktoken](https://cookbook.openai.com/examples/how\_to\_count\_tokens\_with\_tiktoken).
Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.
If a conversation has too many tokens to fit within a modelâ€™s maximum limit (e.g., more than 4097 tokens for `gpt-3.5-turbo` or more than 128k tokens for `gpt-4o`), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.
Note that very long conversations are more likely to receive incomplete replies. For example, a `gpt-3.5-turbo` conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.
Parameter details
-----------------

### Frequency and presence penalties
The frequency and presence penalties found in the [Chat Completions API](/docs/api-reference/chat/create) and [Legacy Completions API](/docs/api-reference/completions) can be used to reduce the likelihood of sampling repetitive sequences of tokens.
Deep dive
Penalties behind the scenes
Reasonable values for the penalty coefficients are around 0.1 to 1 if the aim is to just reduce repetitive samples somewhat. If the aim is to strongly suppress repetition, then one can increase the coefficients up to 2, but this can noticeably degrade the quality of samples. Negative values can be used to increase the likelihood of repetition.

### Token log probabilities
The [logprobs](/docs/api-reference/chat/create#chat-create-logprobs) parameter found in the [Chat Completions API](/docs/api-reference/chat/create) and [Legacy Completions API](/docs/api-reference/completions), when requested, provides the log probabilities of each output token, and a limited number of the most likely tokens at each token position alongside their log probabilities. This can be useful in some cases to assess the confidence of the model in its output, or to examine alternative responses the model might have given.

### Other parameters
See the full [API reference documentation](https://platform.openai.com/docs/api-reference/chat) to learn more.
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:43:29

Responses vs. Chat Completions
==============================
Compare the Responses API and Chat Completions API.
The [Responses API](https://platform.openai.com/docs/api-reference/responses) and [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) are two different ways to interact with OpenAI's models. This guide explains the key differences between the two APIs.
Why the Responses API?
----------------------
The Responses API is our newest core API and an agentic API primitive, combining the simplicity of Chat Completions with the ability to do more agentic tasks. As model capabilities evolve, the Responses API is a flexible foundation for building action-oriented applications, with built-in tools:
\* [Web search](/docs/guides/tools-web-search)
\* [File search](/docs/guides/tools-file-search)
\* [Computer use](/docs/guides/tools-computer-use)
If you're a new user, we recommend using the Responses API.
|Capabilities|Chat Completions API|Responses API|
|---|---|---|
|Text generation|||
|Audio||Coming soon|
|Vision|||
|Structured Outputs|||
|Function calling|||
|Web search|||
|File search|||
|Computer use|||
|Code interpreter||Coming soon|

### The Chat Completions API is not going away
The Chat Completions API is an industry standard for building AI applications, and we intend to continue supporting this API indefinitely. We're introducing the Responses API to simplify workflows involving tool use, code execution, and state management. We believe this new API primitive will allow us to more effectively enhance the OpenAI platform into the future.

### A stateful API and semantic events
Events are simpler with the Responses API. It has a predictable, event-driven architecture, whereas the Chat Completions API continuously appends to the content field as tokens are generatedâ€”requiring you to manually track differences between each state. Multi-step conversational logic and reasoning are easier to implement with the Responses API.
The Responses API clearly emits semantic events detailing precisely what changed (e.g., specific text additions), so you can write integrations targeted at specific emitted events (e.g., text changes), simplifying integration and improving type safety.

### Model availability in each API
Whenever possible, all new models will be added to both the Chat Completions API and Responses API. Some models may only be available through Responses API if they use built-in tools (e.g. our computer use models), or trigger multiple model generation turns behind the scenes (e.g. o1-pro) . The detail pages for each [model](/docs/models) will indicate if they support Chat Completions, Responses, or both.
Compare the code
----------------
The following examples show how to make a basic API call to the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) and the [Responses API](https://platform.openai.com/docs/api-reference/responses).

### Text generation example
Both APIs make it easy to generate output from our models. A completion requires a `messages` array, but a response requires an `input` (string or array, as shown below).
Chat Completions API
```python
from openai import OpenAI
client = OpenAI()
completion = client.chat.completions.create(
model="gpt-4.1",
messages=[
{
"role": "user",
"content": "Write a one-sentence bedtime story about a unicorn."
}
]
)
print(completion.choices[0].message.content)
```
Responses API
```python
from openai import OpenAI
client = OpenAI()
response = client.responses.create(
model="gpt-4.1",
input=[
{
"role": "user",
"content": "Write a one-sentence bedtime story about a unicorn."
}
]
)
print(response.output\_text)
```
When you get a response back from the Responses API, the fields differ slightly. Instead of a `message`, you receive a typed `response` object with its own `id`. Responses are stored by default. Chat completions are stored by default for new accounts. To disable storage when using either API, set `store: false`.
Chat Completions API
```json
[
{
"index": 0,
"message": {
"role": "assistant",
"content": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
"refusal": null
},
"logprobs": null,
"finish\_reason": "stop"
}
]
```
Responses API
```json
[
{
"id": "msg\_67b73f697ba4819183a15cc17d011509",
"type": "message",
"role": "assistant",
"content": [
{
"type": "output\_text",
"text": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
"annotations": []
}
]
}
]
```

### Other noteworthy differences
\* The Responses API returns `output`, while the Chat Completions API returns a `choices` array.
\* Structured Outputs API shape is different. Instead of `response\_format`, use `text.format` in Responses. Learn more in the [Structured Outputs](/docs/guides/structured-outputs) guide.
\* Function calling API shape is differentâ€”both for the function config on the request and function calls sent back in the response. See the full difference in the [function calling guide](/docs/guides/function-calling).
\* Reasoning is different. Instead of `reasoning\_effort` in Chat Completions, use `reasoning.effort` with the Responses API. Read more details in the [reasoning](/docs/guides/reasoning) guide.
\* The Responses SDK has an `output\_text` helper, which the Chat Completions SDK does not have.
\* Conversation state: You have to manage conversation state yourself in Chat Completions, while Responses has `previous\_response\_id` to help you with long-running conversations.
\* Responses are stored by default. Chat completions are stored by default for new accounts. To disable storage, set `store: false`.
What this means for existing APIs
---------------------------------

### Chat Completions
The Chat Completions API remains our most widely used API. We'll continue supporting it with new models and capabilities. If you don't need built-in tools for your application, you can confidently continue using Chat Completions.
We'll keep releasing new models to Chat Completions whenever their capabilities don't depend on built-in tools or multiple model calls. When you're ready for advanced capabilities designed specifically for agent workflows, we recommend the Responses API.
Assistants
----------
Based on developer feedback from the [Assistants API](/docs/api-reference/assistants) beta, we've incorporated key improvements into the Responses API to make it more flexible, faster, and easier to use. The Responses API represents the future direction for building agents on OpenAI.
We're working to achieve full feature parity between the Assistants and the Responses API, including support for Assistant-like and Thread-like objects and the Code Interpreter tool. When complete, we plan to formally announce the deprecation of the Assistants API with a target sunset date in the first half of 2026.
Upon deprecation, we will provide a clear migration guide from the Assistants API to the Responses API that allows developers to preserve all their data and migrate their applications. Until we formally announce the deprecation, we'll continue delivering new models to the Assistants API.
Was this page useful?


## Imported snippet â€“ 2025-07-03 14:43:40

Flex processing
Beta
=======================
Optimize costs with flex processing.
Flex processing provides significantly lower costs for [Chat Completions](/docs/api-reference/chat) or [Responses](/docs/api-reference/responses) requests in exchange for slower response times and occasional resource unavailability. It is ideal for non-production or lower-priority tasks such as model evaluations, data enrichment, or asynchronous workloads.
Token inputs and outputs are [priced](/docs/pricing) at [Batch API rates](/docs/guides/batch), with additional discounts from [prompt caching](/docs/guides/prompt-caching).
Flex processing is in beta, and currently only available for [o3](/docs/models/o3) and [o4-mini](/docs/models/o4-mini) models.
API usage
---------
Set the `service\_tier` parameter to `flex` in your API request ([Chat](/docs/api-reference/chat) or [Responses](/docs/api-reference/responses)) to take advantage of Flex processing.
Flex processing example
```javascript
import OpenAI from "openai";
const client = new OpenAI({
timeout: 15 \* 1000 \* 60, // Increase default timeout to 15 minutes
});
const response = await client.responses.create({
model: "o3",
instructions: "List and describe all the metaphors used in this book.",
input: "",
service\_tier: "flex",
}, { timeout: 15 \* 1000 \* 60 });
console.log(response.output\_text);
```
```python
from openai import OpenAI
client = OpenAI(

# increase default timeout to 15 minutes (from 10 minutes)
timeout=900.0
)

# you can override the max timeout per request as well
response = client.with\_options(timeout=900.0).responses.create(
model="o3",
instructions="List and describe all the metaphors used in this book.",
input="",
service\_tier="flex",
)
print(response.output\_text)
```
```bash
curl https://api.openai.com/v1/responses \
-H "Authorization: Bearer $OPENAI\_API\_KEY" \
-H "Content-Type: application/json" \
-d '{
"model": "o3",
"instructions": "List and describe all the metaphors used in this book.",
"input": "",
"service\_tier": "flex"
}'
```

#### API request timeouts
Due to slower processing speeds with Flex processing, request timeouts are more likely. Here are some considerations for handling timeouts:
\* \*\*Default timeout\*\*: The default timeout is \*\*10 minutes\*\* when making API requests with an official OpenAI SDK. You may need to increase this timeout for lengthy prompts or complex tasks.
\* \*\*Configuring timeouts\*\*: Each SDK will provide a parameter to increase this timeout. In the Python and JavaScript SDKs, this is `timeout` as shown in the code samples above.
\* \*\*Automatic retries\*\*: The OpenAI SDKs automatically retry requests that result in a `408 Request Timeout` error code twice before throwing an exception.
Resource unavailable errors
---------------------------
Flex processing may sometimes lack sufficient resources to handle your requests, resulting in a `429 Resource Unavailable` error code. \*\*You will not be charged when this occurs.\*\*
When encountering Resource Unavailable errors, consider these strategies:
\* \*\*Retry requests with exponential backoff\*\*: This approach is suitable for workloads that can tolerate delays and aims to minimize costs. For implementation details, see [this cookbook](https://cookbook.openai.com/examples/how\_to\_handle\_rate\_limits?utm\_source=chatgpt.com#retrying-with-exponential-backoff).
\* \*\*Fallback to standard request\*\*: Switching to the default tier is recommended if timely completion is important and occasional higher costs are acceptable. Set `service\_tier` to `auto` in your request to do this, or remove the `service\_tier` parameter to use the default tier.
Was this page useful?
